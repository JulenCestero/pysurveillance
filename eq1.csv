Authors,Author(s) ID,Title,Year,Source title,Volume,Issue,Art. No.,Page start,Page end,Page count,Cited by,DOI,Link,Affiliations,Authors with affiliations,Author Keywords,Index Keywords,Correspondence Address,Document Type,Publication Stage,Access Type,Source,EID
"Shou Z., Di X., Ye J., Zhu H., Zhang H., Hampshire R.","57203962496;36987391400;57194611807;57212490055;57202297163;57203505770;","Optimal passenger-seeking policies on E-hailing platforms using Markov decision process and imitation learning",2020,"Transportation Research Part C: Emerging Technologies","111",,,"91","113",,,"10.1016/j.trc.2019.12.005","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076866879&doi=10.1016%2fj.trc.2019.12.005&partnerID=40&md5=de4ff1de56dcbb64192f6b9cb83c54ba","Department of Civil Engineering and Engineering Mechanics, Columbia University, United States; Data Science Institute, Columbia University, United States; Didi Chuxing Inc., Beijing, China; National Maglev Transportation Engineering R&D Center, Tongji University, Shanghai, China; University of Michigan Transportation Research Institute, University of Michigan, Ann Arbor, United States; Ford School of Public Policy, University of Michigan, Ann Arbor, United States","Shou, Z., Department of Civil Engineering and Engineering Mechanics, Columbia University, United States; Di, X., Department of Civil Engineering and Engineering Mechanics, Columbia University, United States, Data Science Institute, Columbia University, United States; Ye, J., Didi Chuxing Inc., Beijing, China; Zhu, H., Didi Chuxing Inc., Beijing, China; Zhang, H., National Maglev Transportation Engineering R&D Center, Tongji University, Shanghai, China; Hampshire, R., University of Michigan Transportation Research Institute, University of Michigan, Ann Arbor, United States, Ford School of Public Policy, University of Michigan, Ann Arbor, United States","E-hailing; Imitation learning; Markov Decision Process (MDP)","Behavioral research; Driver training; Earnings; Intelligent systems; Markov processes; Monte Carlo methods; Optimization; Roads and streets; Taxicabs; Traffic congestion; Dynamic adjustment; Imitation learning; Inverse reinforcement learning; Markov Decision Processes; Optimal sequential; Supply demand ratio; Transportation network; Vehicle-miles traveled; Reinforcement learning; competition (economics); computer simulation; decision analysis; learning; Markov chain; Monte Carlo analysis; numerical model; taxi transport; transportation policy","Di, X.; Department of Civil Engineering and Engineering Mechanics, Columbia UniversityUnited States; email: sharon.di@columbia.edu",Article,"Final",,Scopus,2-s2.0-85076866879
"Hitomi K., Matsui K., Rivas A., Corchado J.M.","57209749083;56239039500;57195304326;7006360842;","Development of a dangerous driving suppression system using inverse reinforcement learning and blockchain",2020,"Advances in Intelligent Systems and Computing","1003",,,"3","9",,,"10.1007/978-3-030-23887-2_1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85068624697&doi=10.1007%2f978-3-030-23887-2_1&partnerID=40&md5=782181af9063b264dab444a5c630e966","Faculty of Robotics and Design, Osaka Institute of Technology, Osaka, Japan; BISITE Digital Innovation Hub, University of Salamanca, Salamanca, Spain","Hitomi, K., Faculty of Robotics and Design, Osaka Institute of Technology, Osaka, Japan; Matsui, K., Faculty of Robotics and Design, Osaka Institute of Technology, Osaka, Japan; Rivas, A., BISITE Digital Innovation Hub, University of Salamanca, Salamanca, Spain; Corchado, J.M., BISITE Digital Innovation Hub, University of Salamanca, Salamanca, Spain","Blockchain; Inverse reinforcement learning; Safe driving","Accidents; Blockchain; Cameras; Distributed computer systems; Machine learning; Competitive spirit; Dangerous drivings; Inverse reinforcement learning; Reckless driving; Safe driving; Significance levels; Suppression systems; System development; Reinforcement learning","Matsui, K.; Faculty of Robotics and Design, Osaka Institute of TechnologyJapan; email: kenji.matsui@oit.ac.jp",Conference Paper,"Final",,Scopus,2-s2.0-85068624697
"Hidaka K., Hayakawa K., Nishi T., Usui T., Yamamoto T.","57195234027;15047971700;57205550400;57211070410;55703202800;","Generating pedestrian walking behavior considering detour and pause in the path under space-time constraints",2019,"Transportation Research Part C: Emerging Technologies","108",,,"115","129",,,"10.1016/j.trc.2019.09.005","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072558066&doi=10.1016%2fj.trc.2019.09.005&partnerID=40&md5=7fc436530a474ca4c79d672095d38f88","Toyota Central R&D Labs., Inc., 41-1, Yokomichi, Nagakute, Aichi, Japan; Toyota Central R&D Labs., Inc., Koraku Mori Building 10F, 1-4-14 Koraku, Bunkyo-ku, Tokyo, Japan; University of Human Environments, 6-2, Kamisanbonmatsu, Motojuku-cho, Okazaki, Japan; Nagoya University Institute of Materials and Systems for Sustainability, Furo-cho, Chikusa-ku, Nagoya, Japan","Hidaka, K., Toyota Central R&D Labs., Inc., 41-1, Yokomichi, Nagakute, Aichi, Japan; Hayakawa, K., Toyota Central R&D Labs., Inc., Koraku Mori Building 10F, 1-4-14 Koraku, Bunkyo-ku, Tokyo, Japan; Nishi, T., Toyota Central R&D Labs., Inc., Koraku Mori Building 10F, 1-4-14 Koraku, Bunkyo-ku, Tokyo, Japan; Usui, T., University of Human Environments, 6-2, Kamisanbonmatsu, Motojuku-cho, Okazaki, Japan; Yamamoto, T., Nagoya University Institute of Materials and Systems for Sustainability, Furo-cho, Chikusa-ku, Nagoya, Japan","Inverse reinforcement learning; Pedestrian behavior modeling; Space-time constraints; Trajectory generation","Inverse problems; Machine learning; Stochastic systems; Trajectories; Goal-oriented behavior; Inverse reinforcement learning; Location-aware technology; Pedestrian behavior model; Pedestrian trajectories; Points of Interest(POI); Space time; Trajectory generation; Reinforcement learning; inverse analysis; learning; pedestrian; public space; spatiotemporal analysis; urban transport; walking","Hidaka, K.; Toyota Central R&D Labs., Inc., 41-1, Yokomichi, Japan; email: hidaka@mosk.tytlabs.co.jp",Article,"Final",,Scopus,2-s2.0-85072558066
"Rivera-Villicana J., Zambetta F., Harland J., Berry M.","56415008400;6507963260;7005796007;13006225500;","Exploring apprenticeship learning for player modelling in interactive narratives",2019,"CHI PLAY 2019 - Extended Abstracts of the Annual Symposium on Computer-Human Interaction in Play",,,,"645","652",,,"10.1145/3341215.3356314","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074810571&doi=10.1145%2f3341215.3356314&partnerID=40&md5=b59d41d47bde855b478470d8d907dab4","Applied Artificial Intelligence Institute (A2I2, Deakin University, Burwood, VIC, Australia; School of Science, RMIT University, Melbourne, VIC, Australia; School of Media and Communication, RMIT University, Melbourne, VIC, Australia","Rivera-Villicana, J., Applied Artificial Intelligence Institute (A2I2, Deakin University, Burwood, VIC, Australia, School of Science, RMIT University, Melbourne, VIC, Australia; Zambetta, F., School of Science, RMIT University, Melbourne, VIC, Australia; Harland, J., School of Science, RMIT University, Melbourne, VIC, Australia; Berry, M., School of Media and Communication, RMIT University, Melbourne, VIC, Australia","Anchorhead; Apprenticeship Learning; Interactive Narratives; Inverse Reinforcement Learning; Player Modelling","Apprentices; Human computer interaction; Interactive computer systems; Action sequences; Anchorhead; Apprenticeship learning; Interactive fiction; Interactive narrative; Inverse reinforcement learning; Receding horizon; Reward function; Reinforcement learning",,Conference Paper,"Final",Open Access,Scopus,2-s2.0-85074810571
"Lin X., Adams S.C., Beling P.A.","57211939661;56954203500;6603732790;","Multi-agent inverse reinforcement learning for certain general-sum stochastic games",2019,"Journal of Artificial Intelligence Research","66",,,"473","502",,,"10.1613/jair.1.11541","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075421977&doi=10.1613%2fjair.1.11541&partnerID=40&md5=827863ba9b393daba2a36fc860008d66","Data Science, MassMutual Financial Group, 470 Atlantic Ave., Boston, MA  02210, United States; University of Virginia, Charlottesville, VA  22904, United States","Lin, X., Data Science, MassMutual Financial Group, 470 Atlantic Ave., Boston, MA  02210, United States, University of Virginia, Charlottesville, VA  22904, United States; Adams, S.C., University of Virginia, Charlottesville, VA  22904, United States; Beling, P.A., University of Virginia, Charlottesville, VA  22904, United States",,"Computation theory; Convex optimization; Game theory; Linear programming; Machine learning; Multi agent systems; Reinforcement learning; Stochastic systems; Characteristic set; Convex optimization problems; Cooperative strategy; Correlated equilibria; General-sum stochastic games; Inverse reinforcement learning; Linear programming problem; Solution uniqueness; Inverse problems",,Article,"Final",Open Access,Scopus,2-s2.0-85075421977
"Sun R., Hu S., Zhao H., Moze M., Aioun F., Guillemard F.","57212482840;57212480046;35723299400;15925903800;57190282434;11138805900;","Human-like Highway Trajectory Modeling based on Inverse Reinforcement Learning",2019,"2019 IEEE Intelligent Transportation Systems Conference, ITSC 2019",,, 8916970,"1482","1489",,,"10.1109/ITSC.2019.8916970","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076817726&doi=10.1109%2fITSC.2019.8916970&partnerID=40&md5=0b1c3910668462c02e960091c577bb64","Peking University, Key Lab of Machine Perception, Beijing, China; Groupe PSA, Velizy, France","Sun, R., Peking University, Key Lab of Machine Perception, Beijing, China; Hu, S., Peking University, Key Lab of Machine Perception, Beijing, China; Zhao, H., Peking University, Key Lab of Machine Perception, Beijing, China; Moze, M., Groupe PSA, Velizy, France; Aioun, F., Groupe PSA, Velizy, France; Guillemard, F., Groupe PSA, Velizy, France",,"Autonomous vehicles; Behavioral research; Deep learning; Intelligent systems; Intelligent vehicle highway systems; Machine learning; Mapping; Reinforcement learning; Trajectories; Autonomous driving; Behavior patterns; Cutting edge technology; Experimental validations; Human-like trajectory; Inverse reinforcement learning; State-of-the-art performance; Trajectory modeling; Inverse problems",,Conference Paper,"Final",,Scopus,2-s2.0-85076817726
"Xin L., Li S.E., Wang P., Cao W., Nie B., Chan C.-Y., Cheng B.","55343655300;15065803700;56193114700;57212480181;56896875900;7404814589;36611317200;","Accelerated Inverse Reinforcement Learning with Randomly Pre-sampled Policies for Autonomous Driving Reward Design",2019,"2019 IEEE Intelligent Transportation Systems Conference, ITSC 2019",,, 8916952,"2757","2764",,,"10.1109/ITSC.2019.8916952","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076813251&doi=10.1109%2fITSC.2019.8916952&partnerID=40&md5=cdaf83297d647c8db8c194bdb46f7ed6","Tsinghua University, State Key Laboratory of Automotive Safety and Energy, School of Vehicle and Mobility, Beijing, 100084, China; California PATH, University of California, Berkeley, Richmond, CA  94804, United States","Xin, L., Tsinghua University, State Key Laboratory of Automotive Safety and Energy, School of Vehicle and Mobility, Beijing, 100084, China; Li, S.E., Tsinghua University, State Key Laboratory of Automotive Safety and Energy, School of Vehicle and Mobility, Beijing, 100084, China; Wang, P., California PATH, University of California, Berkeley, Richmond, CA  94804, United States; Cao, W., Tsinghua University, State Key Laboratory of Automotive Safety and Energy, School of Vehicle and Mobility, Beijing, 100084, China; Nie, B., Tsinghua University, State Key Laboratory of Automotive Safety and Energy, School of Vehicle and Mobility, Beijing, 100084, China; Chan, C.-Y., California PATH, University of California, Berkeley, Richmond, CA  94804, United States; Cheng, B., Tsinghua University, State Key Laboratory of Automotive Safety and Energy, School of Vehicle and Mobility, Beijing, 100084, China","Autonomous driving; Inverse reinforcement learning; Maximum entropy; Pre-sampled policy; Reward design","Autonomous vehicles; Efficiency; Intelligent systems; Intelligent vehicle highway systems; Inverse problems; Machine learning; Maximum entropy methods; Trajectories; Autonomous driving; Candidate trajectories; Improving learning; Inverse reinforcement learning; Optimal policies; Optimal trajectories; Reward function; Simulated driving; Reinforcement learning","Li, S.E.; Tsinghua University, State Key Laboratory of Automotive Safety and Energy, School of Vehicle and MobilityChina; email: cwh19@mails.tsinghua.edu.cn",Conference Paper,"Final",,Scopus,2-s2.0-85076813251
"Xu K., Zeng Y., Zhang Q., Yin Q., Sun L., Xiao K.","56828524300;57209278546;56520586200;23096547400;57209284133;56419405500;","Online probabilistic goal recognition and its application in dynamic shortest-path local network interdiction",2019,"Engineering Applications of Artificial Intelligence","85",,,"57","71",,,"10.1016/j.engappai.2019.05.009","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85067169848&doi=10.1016%2fj.engappai.2019.05.009&partnerID=40&md5=88544050dbd7a3f858f31b6957bab2a3","College of Systems Engineering, National University of Defense Technology, Changsha, China; Joint Service College, National Defense University, Beijing, China; Science and Technology on Information Systems Engineering Laboratory, College of System Engineering, National University of Defense Technology, Changsha, China","Xu, K., College of Systems Engineering, National University of Defense Technology, Changsha, China; Zeng, Y., College of Systems Engineering, National University of Defense Technology, Changsha, China; Zhang, Q., College of Systems Engineering, National University of Defense Technology, Changsha, China; Yin, Q., College of Systems Engineering, National University of Defense Technology, Changsha, China; Sun, L., Joint Service College, National Defense University, Beijing, China; Xiao, K., Science and Technology on Information Systems Engineering Laboratory, College of System Engineering, National University of Defense Technology, Changsha, China","Behavior modeling; Goal recognition; Network interdiction","Graph theory; Inverse problems; Probability distributions; Reinforcement learning; Behavior model; Dynamic shortest path; Effectiveness of knowledge; Goal recognition; Inverse reinforcement learning; Network interdiction; Network interdiction problems; Probabilistic distribution; Heuristic methods","Yin, Q.; College of Systems Engineering, National University of Defense TechnologyChina; email: yinquanjun@nudt.edu.cn",Article,"Final",,Scopus,2-s2.0-85067169848
"Self R., Harlan M., Kamalapurkar R.","57209273510;57211021537;55210138200;","Online inverse reinforcement learning for nonlinear systems",2019,"CCTA 2019 - 3rd IEEE Conference on Control Technology and Applications",,, 8920458,"296","301",,,"10.1109/CCTA.2019.8920458","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077788456&doi=10.1109%2fCCTA.2019.8920458&partnerID=40&md5=01d76d0016785e22e56072bea9e4fe04","Oklahoma State University, School of Mechanical and Aerospace Engineering, Still-Water, OK, United States","Self, R., Oklahoma State University, School of Mechanical and Aerospace Engineering, Still-Water, OK, United States; Harlan, M., Oklahoma State University, School of Mechanical and Aerospace Engineering, Still-Water, OK, United States; Kamalapurkar, R., Oklahoma State University, School of Mechanical and Aerospace Engineering, Still-Water, OK, United States",,"Machine learning; Nonlinear systems; Online systems; Reinforcement learning; Input trajectory; Inverse reinforcement learning; Reward function; Theoretical guarantees; Unknown values; E-learning",,Conference Paper,"Final",,Scopus,2-s2.0-85077788456
"Ahlberg S., Dimarogonas D.V.","57211204477;6506281602;","Human-in-the-loop control synthesis for multi-agent systems under hard and soft metric interval temporal logic specifications∗",2019,"IEEE International Conference on Automation Science and Engineering","2019-August",, 8842954,"788","793",,,"10.1109/COASE.2019.8842954","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072955571&doi=10.1109%2fCOASE.2019.8842954&partnerID=40&md5=897b7dc4962c14d4c76b9233b734410b","Division of Decision and Control Systems, KTH Royal Institute of Technology, Sweden","Ahlberg, S., Division of Decision and Control Systems, KTH Royal Institute of Technology, Sweden; Dimarogonas, D.V., Division of Decision and Control Systems, KTH Royal Institute of Technology, Sweden",,"Control system synthesis; Reinforcement learning; Temporal logic; Control synthesis; Hard and soft constraints; Human-in-the-loop control; Interval temporal logic; Inverse reinforcement learning; Mixed initiative; Optimal paths; Soft constraint; Multi agent systems",,Conference Paper,"Final",,Scopus,2-s2.0-85072955571
"Pauna A., Bica I., Pop F., Castiglione A.","55360454800;36343237100;22836395600;16027997000;","On the rewards of self-adaptive IoT honeypots",2019,"Annales des Telecommunications/Annals of Telecommunications","74","7-8",,"501","515",,,"10.1007/s12243-018-0695-7","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059537479&doi=10.1007%2fs12243-018-0695-7&partnerID=40&md5=1dfaadfd843d037fe93c6cd53acf0945","Faculty of Military Electronic and Information Systems, Military Technical Academy, Bucharest, Romania; Faculty of Automatic Control and Computers, “Politehnica” University of Bucharest, Bucharest, Romania; National Institute for Research and Development in Informatics (ICI), Bucharest, Romania; Department of Computer Science, University of Salerno, Fisciano, SA, Italy","Pauna, A., Faculty of Military Electronic and Information Systems, Military Technical Academy, Bucharest, Romania; Bica, I., Faculty of Military Electronic and Information Systems, Military Technical Academy, Bucharest, Romania; Pop, F., Faculty of Automatic Control and Computers, “Politehnica” University of Bucharest, Bucharest, Romania, National Institute for Research and Development in Informatics (ICI), Bucharest, Romania; Castiglione, A., Department of Computer Science, University of Salerno, Fisciano, SA, Italy","Honeypot systems; Internet of things; Inverse reinforcement learning; Neural network; Reinforcement learning; Self-adaptive honeypot systems","Learning algorithms; Network security; Neural networks; Reinforcement learning; Risk assessment; Apprenticeship learning; Honeypots; Inverse reinforcement learning; Iot devices; Key Issues; Optimal reward; Reward function; Self-Adaptive; Internet of things","Pop, F.; Faculty of Automatic Control and Computers, “Politehnica” University of BucharestRomania; email: florin.pop@cs.pub.ro",Article,"Final",,Scopus,2-s2.0-85059537479
"Ashida K., Kato T., Hotta K., Oka K.","57204919943;57208750954;7202730545;7201489853;","Multiple tracking and machine learning reveal dopamine modulation for area-restricted foraging behaviors via velocity change in Caenorhabditis elegans",2019,"Neuroscience Letters","706",,,"68","74",,1,"10.1016/j.neulet.2019.05.011","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065714065&doi=10.1016%2fj.neulet.2019.05.011&partnerID=40&md5=9ad5c6c0707c68bcea841a485614818a","Department of Bioscience and Informatics, Faculty of Science and Technology, Keio University, Yokohama, 223-8522, Japan; Graduate Institute of Medicine, College of Medicine, Kaohsiung Medical University, Kaohsiung City, 80708, Taiwan; Waseda Research Institute for Science and Engineering, Waseda University, 2-2 Wakamatsucho, Shinjuku, Tokyo  162-8480, Japan","Ashida, K., Department of Bioscience and Informatics, Faculty of Science and Technology, Keio University, Yokohama, 223-8522, Japan; Kato, T., Department of Bioscience and Informatics, Faculty of Science and Technology, Keio University, Yokohama, 223-8522, Japan; Hotta, K., Department of Bioscience and Informatics, Faculty of Science and Technology, Keio University, Yokohama, 223-8522, Japan; Oka, K., Department of Bioscience and Informatics, Faculty of Science and Technology, Keio University, Yokohama, 223-8522, Japan, Graduate Institute of Medicine, College of Medicine, Kaohsiung Medical University, Kaohsiung City, 80708, Taiwan, Waseda Research Institute for Science and Engineering, Waseda University, 2-2 Wakamatsucho, Shinjuku, Tokyo  162-8480, Japan","Area-restricted search; Behavioral assay; Dopamine; Foraging; Inverse reinforcement learning; Machine learning","dopamine; dopamine; Article; Caenorhabditis elegans; controlled study; dopamine metabolism; foraging behavior; machine learning; nonhuman; priority journal; reinforcement; velocity; animal; appetitive behavior; Caenorhabditis elegans; drug effect; transgenic animal; Animals; Animals, Genetically Modified; Appetitive Behavior; Caenorhabditis elegans; Dopamine; Machine Learning; Reinforcement, Psychology","Oka, K.; Department of Bioscience and Informatics, Faculty of Science and Technology, Keio UniversityJapan; email: oka@bio.keio.ac.jp",Article,"Final",,Scopus,2-s2.0-85065714065
"Chenreddy A.R., Pakiman P., Nadarajah S., Chandrasekaran R., Abens R.","57210646582;57210638739;56999957300;7006808991;57189268770;","Smoile: A shopper marketing optimization and inverse learning engine",2019,"Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining",,,,"2934","2942",,,"10.1145/3292500.3330788","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071146621&doi=10.1145%2f3292500.3330788&partnerID=40&md5=0a6bc41426faf6957a2db3592219b48b","University of Illinois at Chicago, Chicago, IL, United States; Foresight ROI, Inc, Chicago, IL, United States","Chenreddy, A.R., University of Illinois at Chicago, Chicago, IL, United States; Pakiman, P., University of Illinois at Chicago, Chicago, IL, United States; Nadarajah, S., University of Illinois at Chicago, Chicago, IL, United States; Chandrasekaran, R., University of Illinois at Chicago, Chicago, IL, United States; Abens, R., Foresight ROI, Inc, Chicago, IL, United States","Data-driven optimization; Inverse reinforcement learning; Shopper marketing","Commerce; Data mining; Engines; Machine learning; Sales; Data-driven optimization; Industry benchmarks; Inverse reinforcement learning; Manual intervention; Optimization approach; Predictive modeling; Regression techniques; Sequential process; Reinforcement learning","Nadarajah, S.; University of Illinois at ChicagoUnited States; email: selvan@uic.edu",Conference Paper,"Final",,Scopus,2-s2.0-85071146621
"Oh M.-H., Iyengar G.","56939849000;7005191853;","Sequential anomaly detection using inverse reinforcement learning",2019,"Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining",,,,"1480","1490",,,"10.1145/3292500.3330932","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071146603&doi=10.1145%2f3292500.3330932&partnerID=40&md5=528a1e6194f6c36b5b0d7bad8675655f","Columbia University, New York, NY, United States","Oh, M.-H., Columbia University, New York, NY, United States; Iyengar, G., Columbia University, New York, NY, United States","Anomaly detection; Bootstrap; Inverse reinforcement learning; Neural networks; Outlier detection; Spatial-temporal data","Bayesian networks; Data mining; Decision making; Inverse problems; Machine learning; Neural networks; Reinforcement learning; Safety engineering; Anomaly detection methods; Application scenario; Automatic detection systems; Bootstrap; Critical environment; Decision making agents; Inverse reinforcement learning; Spatial-temporal data; Anomaly detection",,Conference Paper,"Final",,Scopus,2-s2.0-85071146603
"Piao S., Huang Y., Liu H.","57211741412;57211277745;35272617500;","Online multi-modal imitation learning via lifelong intention encoding",2019,"2019 4th IEEE International Conference on Advanced Robotics and Mechatronics, ICARM 2019",,, 8833960,"786","792",,,"10.1109/ICARM.2019.8833960","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073232592&doi=10.1109%2fICARM.2019.8833960&partnerID=40&md5=a6d7d90c008261e5828d799e4d65519a","School of Computer Science and Technology, Harbin Institute of Technology, Harbin, China; Department of Computer Science and Technology, Tsinghua University, Beijing, China","Piao, S., School of Computer Science and Technology, Harbin Institute of Technology, Harbin, China; Huang, Y., School of Computer Science and Technology, Harbin Institute of Technology, Harbin, China; Liu, H., Department of Computer Science and Technology, Tsinghua University, Beijing, China",,"E-learning; Encoding (symbols); Inverse problems; Machine learning; Robotics; Signal encoding; Imitation learning; Inverse reinforcement learning; Multi-modal; Reward function; Robotic manipulation; Sparse coding; Reinforcement learning","Liu, H.; Department of Computer Science and Technology, Tsinghua UniversityChina; email: hpliu@tsinghua.edu.cn",Conference Paper,"Final",,Scopus,2-s2.0-85073232592
"Imani M., Braga-Neto U.M.","57189031055;6603390822;","Control of gene regulatory networks using bayesian inverse reinforcement learning",2019,"IEEE/ACM Transactions on Computational Biology and Bioinformatics","16","4", 3370654,"1250","1261",,5,"10.1109/TCBB.2018.2830357","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046001548&doi=10.1109%2fTCBB.2018.2830357&partnerID=40&md5=6da59589d88f5f109056eebaf9f54ed5","Department of Electrical and Computer Engineering, Texas A and M University, College Station, TX  77843, United States","Imani, M., Department of Electrical and Computer Engineering, Texas A and M University, College Station, TX  77843, United States; Braga-Neto, U.M., Department of Electrical and Computer Engineering, Texas A and M University, College Station, TX  77843, United States","Bayesian inverse reinforcement learning; Boolean Kalman smoother; Gene regulatory networks; Melanoma; P53-MDM2; Partially-observed Boolean dynamical system; Q-learning","Bayesian networks; Boolean functions; Cost functions; Costs; Dermatology; Dynamical systems; Gene expression; Inverse problems; Learning algorithms; Oncology; State feedback; Boolean dynamical systems; Gene regulatory networks; Inverse reinforcement learning; Kalman smoother; Melanoma; p53-MDM2; Q-learning; Reinforcement learning","Imani, M.; Department of Electrical and Computer Engineering, Texas A and M UniversityUnited States; email: m.imani88@tamu.edu",Article,"Final",,Scopus,2-s2.0-85046001548
"Zhuansun S., Yang J.-A., Liu H.","57202022207;9737323800;56125442700;","Apprenticeship learning in cognitive jamming",2019,"Optimal Control Applications and Methods","40","4",,"647","658",,,"10.1002/oca.2502","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063507637&doi=10.1002%2foca.2502&partnerID=40&md5=4df1fe33eb13cf892375d0e3a61546d0","Electronic Countermeasure Institute, National University of Defense Technology, Hefei, China; Key Laboratory of Electronic Restriction, Hefei, China","Zhuansun, S., Electronic Countermeasure Institute, National University of Defense Technology, Hefei, China, Key Laboratory of Electronic Restriction, Hefei, China; Yang, J.-A., Electronic Countermeasure Institute, National University of Defense Technology, Hefei, China, Key Laboratory of Electronic Restriction, Hefei, China; Liu, H., Electronic Countermeasure Institute, National University of Defense Technology, Hefei, China, Key Laboratory of Electronic Restriction, Hefei, China","cognitive jamming; inverse reinforcement learning; jamming strategy; Markov decision process; reinforcement learning","Apprentices; Electronic warfare; Inverse problems; Jamming; Learning algorithms; Machine learning; Markov processes; Transmitters; Apprenticeship learning; Changing environment; Cognitive jamming; Complete information; Inverse reinforcement learning; Jamming strategies; Markov Decision Processes; Number of iterations; Reinforcement learning","Zhuansun, S.; Electronic Countermeasure Institute, National University of Defense TechnologyChina; email: zhuansunss@sina.com",Article,"Final",,Scopus,2-s2.0-85063507637
"Kim K., Kim S., Lee C., Ko S.","57209973294;57209981341;57209978509;37049022800;","Poster: Modeling exploration/exploitation decisions through mobile sensing for understanding mechanisms of addiction",2019,"MobiSys 2019 - Proceedings of the 17th Annual International Conference on Mobile Systems, Applications, and Services",,,,"512","513",,,"10.1145/3307334.3328599","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069165274&doi=10.1145%2f3307334.3328599&partnerID=40&md5=85c7467a0ff678ab1bea14f74921e41b","School of Electrical and Computer Engineering, UNIST Ulsan, South Korea","Kim, K., School of Electrical and Computer Engineering, UNIST Ulsan, South Korea; Kim, S., School of Electrical and Computer Engineering, UNIST Ulsan, South Korea; Lee, C., School of Electrical and Computer Engineering, UNIST Ulsan, South Korea; Ko, S., School of Electrical and Computer Engineering, UNIST Ulsan, South Korea",,"Disease control; Game theory; Neurology; Reinforcement learning; Brain disease; Cost-efficient; Exploration/exploitation; Higher learning; Inverse reinforcement learning; Literature models; Loss of control; Mobile sensing; Inverse problems",,Conference Paper,"Final",,Scopus,2-s2.0-85069165274
"Sun L., Zhan W., Chan C.-Y., Tomizuka M.","56007948700;57193013312;7404814589;35567325500;","Behavior planning of autonomous cars with social perception",2019,"IEEE Intelligent Vehicles Symposium, Proceedings","2019-June",, 8814223,"207","213",,1,"10.1109/IVS.2019.8814223","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072284431&doi=10.1109%2fIVS.2019.8814223&partnerID=40&md5=4216b2808dab454d1154403749e92e4f","Department of Mechanical Engineering, University of California, Berkeley, CA  94720, United States; California PATH, University of California, Berkeley, CA  94720, United States","Sun, L., Department of Mechanical Engineering, University of California, Berkeley, CA  94720, United States; Zhan, W., Department of Mechanical Engineering, University of California, Berkeley, CA  94720, United States; Chan, C.-Y., California PATH, University of California, Berkeley, CA  94720, United States; Tomizuka, M., Department of Mechanical Engineering, University of California, Berkeley, CA  94720, United States",,"Cost functions; Decision making; Intelligent vehicle highway systems; Model predictive control; Predictive control systems; Reinforcement learning; Roads and streets; Sensor networks; Behavior planning; Distributed sensor; Dynamic environments; Individual behavior; Inverse reinforcement learning; Probabilistic planning; Probabilistic prediction; Sensor limitations; Autonomous vehicles",,Conference Paper,"Final",,Scopus,2-s2.0-85072284431
"You C., Lu J., Filev D., Tsiotras P.","57191623509;7601561346;7004005522;7006580129;","Advanced planning for autonomous vehicles using reinforcement learning and deep inverse reinforcement learning",2019,"Robotics and Autonomous Systems","114",,,"1","18",,6,"10.1016/j.robot.2019.01.003","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060531902&doi=10.1016%2fj.robot.2019.01.003&partnerID=40&md5=49dcd8d86ea9d1835065e136374f100a","School of Aerospace Engineering, Georgia Institute of Technology, Atlanta, GA  30332-0150, United States; Research & Advanced Engineering, Ford Motor Company, Dearborn, MI  48121, United States; School of Aerospace Engineering and the Institute for Robotics & Intelligent Machines, Georgia Institute of Technology, Atlanta, GA  30332-0150, United States","You, C., School of Aerospace Engineering, Georgia Institute of Technology, Atlanta, GA  30332-0150, United States; Lu, J., Research & Advanced Engineering, Ford Motor Company, Dearborn, MI  48121, United States; Filev, D., Research & Advanced Engineering, Ford Motor Company, Dearborn, MI  48121, United States; Tsiotras, P., School of Aerospace Engineering and the Institute for Robotics & Intelligent Machines, Georgia Institute of Technology, Atlanta, GA  30332-0150, United States","Autonomous vehicle; Deep neural-network; Inverse reinforcement learning; Maximum entropy; Path planning; Reinforcement learning","Autonomous vehicles; Behavioral research; Deep neural networks; Entropy; Highway traffic control; Intelligent systems; Intelligent vehicle highway systems; Inverse problems; Learning algorithms; Machine learning; Markov processes; Maximum entropy methods; Maximum principle; Motion planning; Stochastic models; Stochastic systems; Traffic congestion; Driving strategy; Intelligent transportation systems; Inverse reinforcement learning; Markov Decision Processes; Maximum entropy principle; Planning problem; Reinforcement learning techniques; Simulated results; Reinforcement learning","Tsiotras, P.; School of Aerospace Engineering and the Institute for Robotics & Intelligent Machines, Georgia Institute of TechnologyUnited States; email: tsiotras@gatech.edu",Article,"Final",Open Access,Scopus,2-s2.0-85060531902
"Brooks C., Szafir D.","57188741380;50362161600;","Balanced Information Gathering and Goal-Oriented Actions in Shared Autonomy",2019,"ACM/IEEE International Conference on Human-Robot Interaction","2019-March",, 8673192,"85","94",,,"10.1109/HRI.2019.8673192","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064003646&doi=10.1109%2fHRI.2019.8673192&partnerID=40&md5=424a322c45313ab54b0ea0c382b4b911","Department of Computer Science, University of Colorado Boulder, Boulder, United States; Department of Computer Science, ATLAS Institute, University of Colorado Boulder, Boulder, United States","Brooks, C., Department of Computer Science, University of Colorado Boulder, Boulder, United States; Szafir, D., Department of Computer Science, ATLAS Institute, University of Colorado Boulder, Boulder, United States","active learning; assistive teleoperation; human-robot teaming; inverse reinforcement learning; Shared autonomy","Artificial intelligence; Control theory; Degrees of freedom (mechanics); Economic and social effects; Man machine systems; Manipulators; Reinforcement learning; Remote control; Active Learning; Assistive; Human robots; Inverse reinforcement learning; Shared autonomy; Human robot interaction",,Conference Paper,"Final",,Scopus,2-s2.0-85064003646
"Krishnan S., Garg A., Liaw R., Thananjeyan B., Miller L., Pokorny F.T., Goldberg K.","56272628500;55561356700;57195417297;57195425489;57192208784;55314672100;35453491100;","SWIRL: A sequential windowed inverse reinforcement learning algorithm for robot tasks with delayed rewards",2019,"International Journal of Robotics Research","38","2-3",,"126","145",,1,"10.1177/0278364918784350","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052190280&doi=10.1177%2f0278364918784350&partnerID=40&md5=34ff0bf996ea99be0cac565416bdc71c","AUTOLAB, University of California, Berkeley, United States; Stanford University, United States; RPL/CSC, KTH Royal Institute of Technology, Sweden","Krishnan, S., AUTOLAB, University of California, Berkeley, United States; Garg, A., AUTOLAB, University of California, Berkeley, United States, Stanford University, United States; Liaw, R., AUTOLAB, University of California, Berkeley, United States; Thananjeyan, B., AUTOLAB, University of California, Berkeley, United States; Miller, L., AUTOLAB, University of California, Berkeley, United States; Pokorny, F.T., RPL/CSC, KTH Royal Institute of Technology, Sweden; Goldberg, K., AUTOLAB, University of California, Berkeley, United States","inverse reinforcement learning; learning from demonstrations; medical robots and systems; Reinforcement learning","Cloning; Deformation; Demonstrations; Learning algorithms; Robotic surgery; Robots; Surgery; Autonomous exploration; Behavioral cloning; Da vinci surgical robots; Inverse reinforcement learning; Learning from demonstration; Medical robots and systems; Physical experiments; Transition conditions; Reinforcement learning","Krishnan, S.; AUTOLAB, University of CaliforniaUnited States; email: sanjay@eecs.berkeley.edu",Article,"Final",,Scopus,2-s2.0-85052190280
"Huang S.H., Held D., Abbeel P., Dragan A.D.","56422734600;35955893400;8269962600;55193779100;","Enabling robots to communicate their objectives",2019,"Autonomous Robots","43","2",,"309","326",,5,"10.1007/s10514-018-9771-0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048686549&doi=10.1007%2fs10514-018-9771-0&partnerID=40&md5=98b721ed244e1992157cfc865a3f201c","University of California, Berkeley, Berkeley, CA, United States; Carnegie Mellon University, Pittsburgh, PA, United States","Huang, S.H., University of California, Berkeley, Berkeley, CA, United States; Held, D., Carnegie Mellon University, Pittsburgh, PA, United States; Abbeel, P., University of California, Berkeley, Berkeley, CA, United States; Dragan, A.D., University of California, Berkeley, Berkeley, CA, United States","Explainable artificial intelligence; Human-robot interaction; Inverse reinforcement learning; Transparency","Cognitive systems; Human computer interaction; Intelligent robots; Reinforcement learning; Transparency; Appropriate models; Candidate models; Human learning; Inverse reinforcement learning; Mental model; Objective functions; Robot behavior; Robot model; Human robot interaction","Huang, S.H.; University of California, BerkeleyUnited States; email: shhuang@cs.berkeley.edu",Article,"Final",,Scopus,2-s2.0-85048686549
"Pan W., Qu R., Hwang K.-S., Lin H.-S.","57198514328;57207731019;7402426737;57208084484;","An Ensemble Fuzzy Approach for Inverse Reinforcement Learning",2019,"International Journal of Fuzzy Systems","21","1",,"95","103",,2,"10.1007/s40815-018-0535-y","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063725863&doi=10.1007%2fs40815-018-0535-y&partnerID=40&md5=ec720eef2adc7b168db34d223539130c","School of Computer Science, Northwestern Polytechnical University, Xi’an, China; Department of Electrical Engineering, National Sun Yat-Sen University, Kaohsiung, Taiwan","Pan, W., School of Computer Science, Northwestern Polytechnical University, Xi’an, China; Qu, R., School of Computer Science, Northwestern Polytechnical University, Xi’an, China; Hwang, K.-S., Department of Electrical Engineering, National Sun Yat-Sen University, Kaohsiung, Taiwan; Lin, H.-S., Department of Electrical Engineering, National Sun Yat-Sen University, Kaohsiung, Taiwan","Fuzzy; Inverse reinforcement learning; Reward feature","Demonstrations; Information theory; Inverse problems; Machine learning; Maximum likelihood; Apprenticeship learning; Fuzzy; Information-theoretic methods; Inverse reinforcement learning; Linear combinations; Reward feature; Sampling efficiency; Simulation demonstrate; Reinforcement learning","Hwang, K.-S.; Department of Electrical Engineering, National Sun Yat-Sen UniversityTaiwan; email: hwang@ccu.edu.tw",Article,"Final",,Scopus,2-s2.0-85063725863
"Zhang K., Yu Y.","57208822390;57210071874;","Methodologies for Imitation Learning via Inverse Reinforcement Learning: A Review [基于逆强化学习的示教学习方法综述]",2019,"Jisuanji Yanjiu yu Fazhan/Computer Research and Development","56","2",,"254","261",,,"10.7544/issn1000-1239.2019.20170578","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065870040&doi=10.7544%2fissn1000-1239.2019.20170578&partnerID=40&md5=3635cf1183521045d24b1351d4cec616","State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, 210023, China","Zhang, K., State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, 210023, China; Yu, Y., State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, 210023, China","Imitation learning; Inverse reinforcement learning; Markov decision process; Multi-step decision problem; Reinforcement learning","Behavioral research; Decision making; Inverse problems; Learning algorithms; Machine learning; Markov processes; Autonomous robotic systems; Decision problems; Imitation learning; Inverse reinforcement learning; Markov Decision Processes; Reinforcement learning approach; Reinforcement learning method; Sequential decision making; Reinforcement learning","Yu, Y.; State Key Laboratory for Novel Software Technology, Nanjing UniversityChina; email: yuy@nju.edu.cn",Review,"Final",,Scopus,2-s2.0-85065870040
"Peysakhovich A.","55258497600;","Reinforcement learning and inverse reinforcement learning with system 1 and system 2",2019,"AIES 2019 - Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society",,,,"409","415",,,"10.1145/3306618.3314259","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85070585523&doi=10.1145%2f3306618.3314259&partnerID=40&md5=1378a3a0e54b12f4d90637873a59b823","Facebook AI Research","Peysakhovich, A., Facebook AI Research","Behavioral economics; Dual-system models; Inverse reinforcement learning; Reinforcement learning","Decision making; Economics; Inverse problems; Machine learning; Optimization; Philosophical aspects; Rational functions; Applications of AI; Behavioral economics; Competing models; Decision makers; Dual system; Inverse reinforcement learning; Markov decision problem; Reward function; Reinforcement learning","Peysakhovich, A.; Facebook AI Researchemail: alexpeys@fb.com",Conference Paper,"Final",,Scopus,2-s2.0-85070585523
"Scobee D.R.R., Rubies Royo V., Tomlin C.J., Sastry S.S.","55355309200;57207112216;56751013300;56111601700;","Haptic Assistance via Inverse Reinforcement Learning",2019,"Proceedings - 2018 IEEE International Conference on Systems, Man, and Cybernetics, SMC 2018",,, 8616258,"1510","1517",,,"10.1109/SMC.2018.00262","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062208365&doi=10.1109%2fSMC.2018.00262&partnerID=40&md5=22d49c40e7cdfed28db0dcdde5206feb","Department of Electrical Engineering and Computer Science, University of California, Berkeley, United States","Scobee, D.R.R., Department of Electrical Engineering and Computer Science, University of California, Berkeley, United States; Rubies Royo, V., Department of Electrical Engineering and Computer Science, University of California, Berkeley, United States; Tomlin, C.J., Department of Electrical Engineering and Computer Science, University of California, Berkeley, United States; Sastry, S.S., Department of Electrical Engineering and Computer Science, University of California, Berkeley, United States",,"Autonomous agents; Feedback; Haptic interfaces; Inverse problems; Machine learning; Remote control; Assistive; Control interfaces; Controlled system; Haptic assistance; Haptic feedbacks; Human users; Inverse reinforcement learning; User study; Reinforcement learning",,Conference Paper,"Final",,Scopus,2-s2.0-85062208365
"Inga J., Eitel M., Flad M., Hohmann S.","57188985251;57207116884;6603246805;56027574000;","Evaluating Human Behavior in Manual and Shared Control via Inverse Optimization",2019,"Proceedings - 2018 IEEE International Conference on Systems, Man, and Cybernetics, SMC 2018",,, 8616457,"2699","2704",,2,"10.1109/SMC.2018.00461","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062238522&doi=10.1109%2fSMC.2018.00461&partnerID=40&md5=92327a6f4f3e3d4226005c4789426a93","Institute of Control Systems (IRS), Karlsruhe Institute of Technology (KIT), Karlsruhe, Germany","Inga, J., Institute of Control Systems (IRS), Karlsruhe Institute of Technology (KIT), Karlsruhe, Germany; Eitel, M., Institute of Control Systems (IRS), Karlsruhe Institute of Technology (KIT), Karlsruhe, Germany; Flad, M., Institute of Control Systems (IRS), Karlsruhe Institute of Technology (KIT), Karlsruhe, Germany; Hohmann, S., Institute of Control Systems (IRS), Karlsruhe Institute of Technology (KIT), Karlsruhe, Germany","Human Behavior Identification; Inverse Optimal Control; Inverse Reinforcement Learning; Shared Control","Cost functions; Cybernetics; Function evaluation; Inverse problems; Manual control; Reinforcement learning; Human behavior modeling; Human behaviors; Human machine interaction; Inverse reinforcement learning; Inverse-optimal control; Optimal control theory; Shared control; Shared-control systems; Behavioral research",,Conference Paper,"Final",,Scopus,2-s2.0-85062238522
"Sama K., Morales Y., Akai N., Takeuchi E., Takeda K.","57202967306;24450513700;55671618700;15726701000;7404334995;","Learning How to Drive in Blind Intersections from Human Data",2019,"Proceedings - 2018 IEEE International Conference on Systems, Man, and Cybernetics, SMC 2018",,, 8616059,"317","324",,,"10.1109/SMC.2018.00064","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062214828&doi=10.1109%2fSMC.2018.00064&partnerID=40&md5=e4a38d9d84d37d02a1145412bfd00900","Institute of Innovation for Future Society, Nagoya University, Nagoya, 464-8603, Japan; Graduate School of Information Science, Nagoya University, Nagoya, 463-8603, Japan; Takeda Lab, Graduate School of Information Science, Nagoya University, Nagoya, 463-8603, Japan","Sama, K., Institute of Innovation for Future Society, Nagoya University, Nagoya, 464-8603, Japan; Morales, Y., Institute of Innovation for Future Society, Nagoya University, Nagoya, 464-8603, Japan; Akai, N., Institute of Innovation for Future Society, Nagoya University, Nagoya, 464-8603, Japan; Takeuchi, E., Graduate School of Information Science, Nagoya University, Nagoya, 463-8603, Japan; Takeda, K., Takeda Lab, Graduate School of Information Science, Nagoya University, Nagoya, 463-8603, Japan","autonomous driving; clustering; learning","Autonomous vehicles; Cybernetics; Digital storage; Drive-in facilities; Driver training; Reinforcement learning; Autonomous driving; Blind intersection; clustering; Different class; Inverse reinforcement learning; learning; Residential areas; Safe driving; Traffic control",,Conference Paper,"Final",,Scopus,2-s2.0-85062214828
"Qureshi A.H., Yip M.C., Boots B.","55849495300;35868173800;10239214300;","Adversarial imitation via variational inverse reinforcement learning",2019,"7th International Conference on Learning Representations, ICLR 2019",,,,"","",,2,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071163274&partnerID=40&md5=ab8d39b5cc1125692a5b71a496439ebb","Department of Electrical and Computer Engineering, University of California San Diego, San Diego, CA  92093, United States; College of Computing, Georgia Institute of Technology, Atlanta, GA  30332, United States","Qureshi, A.H., Department of Electrical and Computer Engineering, University of California San Diego, San Diego, CA  92093, United States; Yip, M.C., Department of Electrical and Computer Engineering, University of California San Diego, San Diego, CA  92093, United States; Boots, B., College of Computing, Georgia Institute of Technology, Atlanta, GA  30332, United States",,"Inverse problems; Learning algorithms; Machine learning; Maximum entropy methods; Adversarial learning; Adversarial networks; High-dimensional; Information maximization; Inverse reinforcement learning; State of the art; Training and testing; Transfer learning; Reinforcement learning",,Conference Paper,"Final",,Scopus,2-s2.0-85071163274
"Gaurav S., Ziebart B.","57195415451;13608719300;","Discriminatively learning inverse optimal control models for predicting human intentions",2019,"Proceedings of the International Joint Conference on Autonomous Agents and Multiagent Systems, AAMAS","3",,,"1368","1376",,1,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076919757&partnerID=40&md5=33dc7d96d62868d9d3af8a3ae368b146","University of Illinois at Chicago, Chicago, IL, United States","Gaurav, S., University of Illinois at Chicago, Chicago, IL, United States; Ziebart, B., University of Illinois at Chicago, Chicago, IL, United States","Goal prediction; Intent prediction; Inverse reinforcement learning; Maximum likelihood estimation","Autonomous agents; Computer aided design; Forecasting; Inverse problems; Machine learning; Multi agent systems; Reinforcement learning; Robots; Bayesian reasoning; Human intentions; Human robots; Inverse reinforcement learning; Inverse-optimal control; Partial sequences; Pointing tasks; Maximum likelihood estimation",,Conference Paper,"Final",,Scopus,2-s2.0-85076919757
"Chen X.-L., Cao L., Xu Z.-X., Lai J., Li C.-X.","57192470874;57198528477;57194948633;57208789168;56103739800;","A Study of Continuous Maximum Entropy Deep Inverse Reinforcement Learning",2019,"Mathematical Problems in Engineering","2019",, 4834516,"","",,,"10.1155/2019/4834516","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065661758&doi=10.1155%2f2019%2f4834516&partnerID=40&md5=ec9b791f267d629157e927d83f2e0afb","Command and Control Engineering College, Army Engineering University, No. 1, Hai Fu Road, Guang Hua Road, Qin Huai District, Nanjing City, Jiangsu Province, 210007, China","Chen, X.-L., Command and Control Engineering College, Army Engineering University, No. 1, Hai Fu Road, Guang Hua Road, Qin Huai District, Nanjing City, Jiangsu Province, 210007, China; Cao, L., Command and Control Engineering College, Army Engineering University, No. 1, Hai Fu Road, Guang Hua Road, Qin Huai District, Nanjing City, Jiangsu Province, 210007, China; Xu, Z.-X., Command and Control Engineering College, Army Engineering University, No. 1, Hai Fu Road, Guang Hua Road, Qin Huai District, Nanjing City, Jiangsu Province, 210007, China; Lai, J., Command and Control Engineering College, Army Engineering University, No. 1, Hai Fu Road, Guang Hua Road, Qin Huai District, Nanjing City, Jiangsu Province, 210007, China; Li, C.-X., Command and Control Engineering College, Army Engineering University, No. 1, Hai Fu Road, Guang Hua Road, Qin Huai District, Nanjing City, Jiangsu Province, 210007, China",,"Demonstrations; Entropy; Learning algorithms; Machine learning; Reinforcement learning; Classical control; Continuous actions; Continuous maxima; Continuous State Space; Environment modeling; Inverse reinforcement learning; Optimal policies; Training process; Deep learning","Chen, X.-L.; Command and Control Engineering College, Army Engineering University, No. 1, Hai Fu Road, Guang Hua Road, Qin Huai District, China; email: 383618393@qq.com",Article,"Final",Open Access,Scopus,2-s2.0-85065661758
"Alvarez N., Noda I.","55441230700;14719940800;","Inverse Reinforcement Learning for Agents Behavior in a Crowd Simulator",2019,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","11422 LNAI",,,"81","95",,,"10.1007/978-3-030-20937-7_6","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85066123482&doi=10.1007%2f978-3-030-20937-7_6&partnerID=40&md5=fc2f1fcadb6ca53730db7e4dcd44e09a","The National Institute of Advanced Industrial Science and Technology (AIST), Tokyo, Japan","Alvarez, N., The National Institute of Advanced Industrial Science and Technology (AIST), Tokyo, Japan; Noda, I., The National Institute of Advanced Industrial Science and Technology (AIST), Tokyo, Japan","Inverse reinforcement learning; Multi-agent systems; Pedestrian simulation","Behavioral research; Inverse problems; Machine learning; Reinforcement learning; Behavior model; Behavioral agents; Crowd behavior; Inverse reinforcement learning; ITS applications; Machine learning techniques; Pedestrian simulation; Working models; Multi agent systems","Alvarez, N.; The National Institute of Advanced Industrial Science and Technology (AIST)Japan; email: nahum.alvarez@aist.go.jp",Conference Paper,"Final",,Scopus,2-s2.0-85066123482
"Ghasemipour S.K.S., Gu S., Zemel R.","57210570038;57192162618;7004912699;","Understanding the relation between maximum-entropy inverse reinforcement learning and behaviour cloning",2019,"Deep Generative Models for Highly Structured Data, DGS@ICLR 2019 Workshop",,,,"","",,,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071179615&partnerID=40&md5=5843354afcf90e0204f7bfe54e31cf6f","University of Toronto, Vector Institute, Canada; Google Brain, United States","Ghasemipour, S.K.S., University of Toronto, Vector Institute, Canada; Gu, S., Google Brain, United States; Zemel, R., University of Toronto, Vector Institute, Canada",,"Clone cells; Cloning; Decision making; Inverse problems; Machine learning; Bc methods; Continuous control; Control policy; Inverse reinforcement learning; State of the art; Reinforcement learning",,Conference Paper,"Final",,Scopus,2-s2.0-85071179615
"Fu J., Korattikara A., Levine S., Guadarrama S.","57211527716;55208184500;35731728100;6506759870;","From language to goals: Inverse reinforcement learning for vision-based instruction following",2019,"7th International Conference on Learning Representations, ICLR 2019",,,,"","",,3,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071164474&partnerID=40&md5=e0380a9e6cd86be3dfb064a3c2891a9d","Google AI","Fu, J., Google AI; Korattikara, A., Google AI; Levine, S., Google AI; Guadarrama, S., Google AI",,"Deep neural networks; Inverse problems; Learning algorithms; Natural language processing systems; Reinforcement learning; Visual languages; Autonomous machines; Control problems; Grounding language; High-dimensional; Inverse reinforcement learning; Natural languages; Poor performance; Visual environments; Machine learning",,Conference Paper,"Final",,Scopus,2-s2.0-85071164474
"Arora S., Doshi P., Banerjee B.","57191058282;23008336000;7102466013;","Online inverse reinforcement learning under occlusion",2019,"Proceedings of the International Joint Conference on Autonomous Agents and Multiagent Systems, AAMAS","2",,,"1170","1178",,,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076970248&partnerID=40&md5=8bcf7cd30e16e078847989d605604e62","Dept of Computer Science, THINC Lab, University of Georgia, Athens, GA, United States; School of Computing Sciences and Computer Engineering, University of Southern Mississippi, Hattiesburg, MS, United States","Arora, S., Dept of Computer Science, THINC Lab, University of Georgia, Athens, GA, United States; Doshi, P., Dept of Computer Science, THINC Lab, University of Georgia, Athens, GA, United States; Banerjee, B., School of Computing Sciences and Computer Engineering, University of Southern Mississippi, Hattiesburg, MS, United States","Inverse reinforcement learning; Online learning; Reinforcement learning; Robot learning; Robotics",,,Conference Paper,"Final",,Scopus,2-s2.0-85076970248
"Wei E., Wicke D., Luke S.","57190427295;56682590400;8931990100;","Multiagent adversarial inverse reinforcement learning",2019,"Proceedings of the International Joint Conference on Autonomous Agents and Multiagent Systems, AAMAS","4",,,"2265","2266",,,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077052760&partnerID=40&md5=a2caeb9dbda089e0e81b072602ddd235","George Mason University, Fairfax, VA, United States","Wei, E., George Mason University, Fairfax, VA, United States; Wicke, D., George Mason University, Fairfax, VA, United States; Luke, S., George Mason University, Fairfax, VA, United States","Adversarial learning; Deep reinforcement learning; Multiagent","Autonomous agents; Deep learning; Game theory; Inverse problems; Machine learning; Multi agent systems; Actor critic models; Adversarial learning; Coordinated behavior; Imitation learning; Inverse reinforcement learning; Multiagent; Overgeneralization; State-of-the-art approach; Reinforcement learning",,Conference Paper,"Final",,Scopus,2-s2.0-85077052760
"Jarboui F., Gruson-Daniel C., Durmus A., Rocchisani V., Goulet Ebongue S.-H., Depoux A., Kirschenmann W., Perchet V.","57195918243;57208837447;56653659200;57195916372;57201896400;35868034100;35179800100;36020384800;","Markov decision process for MOOC users behavioral inference",2019,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","11475 LNCS",,,"70","80",,,"10.1007/978-3-030-19875-6_9","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065913448&doi=10.1007%2f978-3-030-19875-6_9&partnerID=40&md5=7ae40be25ba5d08eccaa70a6bebf151e","ANEO, Boulogne Billancourt, France; CMLA, École normale supérieur Paris Saclay, Université Paris Saclay, Paris, France; Centre Virchow-Villermé for Public Health Paris-Berlin, Université Sorbonne Paris-Cité, Paris, France; GRIPIC - EA 1498, Sorbonne Université, Paris, France; DRISS (Digital Research in Science & Society), Paris, France","Jarboui, F., ANEO, Boulogne Billancourt, France, CMLA, École normale supérieur Paris Saclay, Université Paris Saclay, Paris, France; Gruson-Daniel, C., Centre Virchow-Villermé for Public Health Paris-Berlin, Université Sorbonne Paris-Cité, Paris, France, DRISS (Digital Research in Science & Society), Paris, France; Durmus, A., CMLA, École normale supérieur Paris Saclay, Université Paris Saclay, Paris, France; Rocchisani, V., ANEO, Boulogne Billancourt, France; Goulet Ebongue, S.-H., Centre Virchow-Villermé for Public Health Paris-Berlin, Université Sorbonne Paris-Cité, Paris, France; Depoux, A., Centre Virchow-Villermé for Public Health Paris-Berlin, Université Sorbonne Paris-Cité, Paris, France, GRIPIC - EA 1498, Sorbonne Université, Paris, France; Kirschenmann, W., ANEO, Boulogne Billancourt, France; Perchet, V., CMLA, École normale supérieur Paris Saclay, Université Paris Saclay, Paris, France","Inverse Reinforcement Learning; Learning analytics; Markov Decision Process; User behaviour studies","Behavioral research; Education computing; Markov processes; Reinforcement learning; Inverse reinforcement learning; Learning analytics; Learning process; Log data; Markov Decision Processes; Massive open online course; User behaviour; E-learning","Jarboui, F.; ANEOFrance; email: firasjarboui@gmail.com",Conference Paper,"Final",,Scopus,2-s2.0-85065913448
"Nakata Y., Arai S.","57208792335;14057611500;","Estimating consistent reward of expert in multiple dynamics via linear programming inverse reinforcement learning",2019,"Transactions of the Japanese Society for Artificial Intelligence","34","6", B-J23,"","",,,"10.1527/tjsai.B-J23","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074440530&doi=10.1527%2ftjsai.B-J23&partnerID=40&md5=e7a2a6b21d75d3e4986941b6714f197b","Department of Urban Environment Systems, Graduate School of Science and Engineering, Chiba University, Japan; Department of Urban Environment Systems, Graduate School of Engineering, Chiba University, Japan","Nakata, Y., Department of Urban Environment Systems, Graduate School of Science and Engineering, Chiba University, Japan; Arai, S., Department of Urban Environment Systems, Graduate School of Engineering, Chiba University, Japan","Inverse reinforcement learning; Linear programming","Decision making; Inverse problems; Linear programming; Machine learning; Inverse reinforcement learning; Linear programming problem; Objective functions; Reward function; Reinforcement learning",,Article,"Final",Open Access,Scopus,2-s2.0-85074440530
[No author name available],[No author id available],"International Workshop on Massively Multi-agent Systems, MMAS 2018",2019,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","11422 LNAI",,,"","",162,,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85066147022&partnerID=40&md5=3fd5278b947bbabcbbf4c287c7221976",,"",,,,Conference Review,"Final",,Scopus,2-s2.0-85066147022
"Kostrikov I., Agrawal K.K., Dwibedi D., Levine S., Tompson J.","56422342900;57210638157;56613465600;35731728100;56374097400;","Discriminator-actor-critic: Addressing sample inefficiency and reward bias in adversarial imitation learning",2019,"7th International Conference on Learning Representations, ICLR 2019",,,,"","",,3,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071165223&partnerID=40&md5=6b58929085a085231d621b6ea865a6c1","Courant Institute of Mathematical Sciences, New York University, New York, NY, United States; Google Brain, Mountain View, CA, United States; Google AI Residency Program","Kostrikov, I., Courant Institute of Mathematical Sciences, New York University, New York, NY, United States, Google Brain, Mountain View, CA, United States; Agrawal, K.K., Google Brain, Mountain View, CA, United States, Google AI Residency Program; Dwibedi, D., Google Brain, Mountain View, CA, United States, Google AI Residency Program; Levine, S., Google Brain, Mountain View, CA, United States; Tompson, J., Google Brain, Mountain View, CA, United States",,"Discriminators; Efficiency; Machine learning; Reinforcement learning; Absorbing state; Actor critic; Imitation learning; Inherent instability; Inverse reinforcement learning; Real-world; Reward function; Learning algorithms","Kostrikov, I.; Courant Institute of Mathematical Sciences, New York UniversityUnited States; email: kostrikov@cs.nyu.edu",Conference Paper,"Final",,Scopus,2-s2.0-85071165223
"Lage I., Doshi-Velez F., Lifschitz D., Amir O.","57208445031;34874672900;57211744093;55023457200;","Toward robust policy summarization",2019,"Proceedings of the International Joint Conference on Autonomous Agents and Multiagent Systems, AAMAS","4",,,"2081","2083",,1,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069868681&partnerID=40&md5=aa27f20c7d8b7522f484beff1ce8af29","Harvard University, United States; Technion - Israel Institute of Technology, Israel","Lage, I., Harvard University, United States; Doshi-Velez, F., Harvard University, United States; Lifschitz, D., Technion - Israel Institute of Technology, Israel; Amir, O., Technion - Israel Institute of Technology, Israel","Explainable AI; Policy summarization","Behavioral research; Decision making; Extraction; Inverse problems; Multi agent systems; Reinforcement learning; Repair; Computational model; Decision making process; High quality reconstruction; Human-in-the-loop; Imitation learning; Inverse reinforcement learning; Reconstruction quality; Teaching problems; Autonomous agents",,Conference Paper,"Final",,Scopus,2-s2.0-85069868681
"Peng X.B., Kanazawa A., Toyer S., Abbeel P., Levine S.","57208216498;55430977000;57202418191;8269962600;35731728100;","Variational discriminator bottleneck: Improving imitation learning, inverse RL, and GANs by constraining information flow",2019,"7th International Conference on Learning Representations, ICLR 2019",,,,"","",,1,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071147079&partnerID=40&md5=7d44958bdd1bbbb5bb13f432252c9d3a","University of California, Berkeley, United States","Peng, X.B., University of California, Berkeley, United States; Kanazawa, A., University of California, Berkeley, United States; Toyer, S., University of California, Berkeley, United States; Abbeel, P., University of California, Berkeley, United States; Levine, S., University of California, Berkeley, United States",,"Discriminators; Image enhancement; Inverse problems; Learning algorithms; Adversarial learning; Imitation learning; Information bottleneck; Internal representation; Inverse reinforcement learning; Mutual informations; Stabilization methods; Video demonstration; Reinforcement learning",,Conference Paper,"Final",,Scopus,2-s2.0-85071147079
"Nam C., Walker P., Li H., Lewis M., Sycara K.","35590319700;54785528900;57205670207;57200329926;7006431929;","Models of Trust in Human Control of Swarms With Varied Levels of Autonomy",2019,"IEEE Transactions on Human-Machine Systems",,,,"","",,,"10.1109/THMS.2019.2896845","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062147993&doi=10.1109%2fTHMS.2019.2896845&partnerID=40&md5=ae12b41bc2efe1a8db189c585f7bc718","Center for Robotics Research, Korea Institute of Science and Technology, Seoul 02792, South Korea (e-mail: cjnam@kist.re.kr).; Smart Information Flow Technologies, Minneapolis, MN 55401 USA (e-mail: pmwalk@gmail.com).; School of Information Science, University of Pittsburgh, Pittsburgh, PA 15260 USA (e-mail: huao.li@pitt.edu).; School of Information Science, University of Pittsburgh, Pittsburgh, PA 15260 USA (e-mail: mlewis@sis.pitt.edu).; Robotics Institute, Carnegie Mellon University, Pittsburgh, PA 15213 USA (e-mail: katia@cs.cmu.edu).","Nam, C., Center for Robotics Research, Korea Institute of Science and Technology, Seoul 02792, South Korea (e-mail: cjnam@kist.re.kr).; Walker, P., Smart Information Flow Technologies, Minneapolis, MN 55401 USA (e-mail: pmwalk@gmail.com).; Li, H., School of Information Science, University of Pittsburgh, Pittsburgh, PA 15260 USA (e-mail: huao.li@pitt.edu).; Lewis, M., School of Information Science, University of Pittsburgh, Pittsburgh, PA 15260 USA (e-mail: mlewis@sis.pitt.edu).; Sycara, K., Robotics Institute, Carnegie Mellon University, Pittsburgh, PA 15213 USA (e-mail: katia@cs.cmu.edu).","Automation; Computational modeling; Human&#x2013;robot interaction; human&#x2013;swarm interaction; multirobot systems; Predictive models; Robot kinematics; Robot sensing systems; swarm robotics; Task analysis; trust","Automation; Computation theory; Control theory; Inverse problems; Job analysis; Learning algorithms; Markov processes; Reinforcement learning; Swarm intelligence; Computational model; Multi-robot systems; Predictive models; Robot interactions; Robot kinematics; Robot sensing system; Swarm robotics; Task analysis; trust; Human robot interaction",,Article in Press,"Article in Press",,Scopus,2-s2.0-85062147993
"Cushman F.","8286891500;","Rationalization is rational",2019,"Behavioral and Brain Sciences",,,,"","",,,"10.1017/S0140525X19001730","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85066405208&doi=10.1017%2fS0140525X19001730&partnerID=40&md5=952f9b92ec970813b43e6c0adbe18045","Harvard University, United States; Fiery Cushman, 1480 William James Hall, 33 Kirkland St., Cambridge, MA  02138, United States","Cushman, F., Harvard University, United States, Fiery Cushman, 1480 William James Hall, 33 Kirkland St., Cambridge, MA  02138, United States","Cognitive dissonance; Habitization; Inverse reinforcement learning; Rationalization; Reflective equilibrium; Representational exchange; Self-perception; Social learning; Theory of mind; Useful fiction","adult; article; decision making; defense mechanism; female; human; human experiment; male; perception; reinforcement; social learning; theory of mind","Cushman, F.; Harvard UniversityUnited States; email: cushman@fas.harvard.edu",Article,"Article in Press",Open Access,Scopus,2-s2.0-85066405208
"Lee D., Srinivasan S., Doshi-Velez F.","57211757866;57213354376;34874672900;","Truly batch apprenticeship learning with deep successor features",2019,"IJCAI International Joint Conference on Artificial Intelligence","2019-August",,,"5909","5915",,,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074956612&partnerID=40&md5=5e62df2f6a96744818a60e9af042199b","SEAS, Harvard University, United States","Lee, D., SEAS, Harvard University, United States; Srinivasan, S., SEAS, Harvard University, United States; Doshi-Velez, F., SEAS, Harvard University, United States",,,,Conference Paper,"Final",,Scopus,2-s2.0-85074956612
"Massimo D., Ricci F.","56433358100;35270071500;","Users’ evaluation of next-POI recommendations",2019,"CEUR Workshop Proceedings","2435",,,"1","8",,,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072657853&partnerID=40&md5=20f33cb85c149369a9996faab90cd62d","Free University of Bolzano, Italy","Massimo, D., Free University of Bolzano, Italy; Ricci, F., Free University of Bolzano, Italy","Clustering; Inverse reinforcement learning; Recommender systems; User study","Recommender systems; Reinforcement learning; Behaviour models; Behavioural model; Clustering; Inverse reinforcement learning; Performance measure; System accuracy; Traditional approaches; User study; Behavioral research",,Conference Paper,"Final",,Scopus,2-s2.0-85072657853
"Schwarting W., Pierson A., Alonso-Mora J., Karaman S., Rus D.","56724078600;56742977900;37057150200;24923242500;7004511052;","Social behavior for autonomous vehicles",2019,"Proceedings of the National Academy of Sciences of the United States of America","116","50",,"2492","24978",,1,"10.1073/pnas.1820676116","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076241999&doi=10.1073%2fpnas.1820676116&partnerID=40&md5=646282cf372473a85ad7a77987af02c0","Computer Science and Artificial Intelligence Laboratory, Massachusetts Institute of Technology, Cambridge, MA  02139, United States; Cognitive Robotics, Delft University of Technology, Delft, 2628 CD, Netherlands; Laboratory of Information and Decision Systems, Massachusetts Institute of Technology, Cambridge, MA  02139, United States","Schwarting, W., Computer Science and Artificial Intelligence Laboratory, Massachusetts Institute of Technology, Cambridge, MA  02139, United States; Pierson, A., Computer Science and Artificial Intelligence Laboratory, Massachusetts Institute of Technology, Cambridge, MA  02139, United States; Alonso-Mora, J., Cognitive Robotics, Delft University of Technology, Delft, 2628 CD, Netherlands; Karaman, S., Laboratory of Information and Decision Systems, Massachusetts Institute of Technology, Cambridge, MA  02139, United States; Rus, D., Computer Science and Artificial Intelligence Laboratory, Massachusetts Institute of Technology, Cambridge, MA  02139, United States","Autonomous driving; Game theory; Inverse reinforcement learning; Social compliance; Social Value Orientation","algorithm; altruism; article; decision making; human; human experiment; Nash equilibrium; prediction; reinforcement; simulation","Schwarting, W.; Computer Science and Artificial Intelligence Laboratory, Massachusetts Institute of TechnologyUnited States; email: wilkos@mit.edu",Article,"Final",Open Access,Scopus,2-s2.0-85076241999
"Massimo D., Ricci F.","56433358100;35270071500;","Tangible decision-making in sensors augmented spaces",2019,"CEUR Workshop Proceedings","2495",,,"53","62",,,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075109550&partnerID=40&md5=ef43617360ed8dbcac2ec734551c6175","Free University of Bolzano, Italy","Massimo, D., Free University of Bolzano, Italy; Ricci, F., Free University of Bolzano, Italy","Clustering; Inverse reinforcement learning; Recommender systems; User Study","Artificial intelligence; Decision making; Online systems; Recommender systems; Reinforcement learning; Augmented environments; Clustering; Inverse reinforcement learning; Nearest neighbour; Off-line analysis; On-line decision makings; Points of interest; User study; Behavioral research",,Conference Paper,"Final",,Scopus,2-s2.0-85075109550
"Yang S.Y., Yu Y., Almahdi S.","24802841600;57203202581;57194597306;","An investor sentiment reward-based trading system using Gaussian inverse reinforcement learning algorithm",2018,"Expert Systems with Applications","114",,,"388","401",,1,"10.1016/j.eswa.2018.07.056","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050940929&doi=10.1016%2fj.eswa.2018.07.056&partnerID=40&md5=7e4ed9ae031970e123d054c91c1b3d11","Financial Engineering Program, School of Business, Stevens Institute of Technology, 1 Castle Point on Hudson, Hoboken, NJ  07030, United States","Yang, S.Y., Financial Engineering Program, School of Business, Stevens Institute of Technology, 1 Castle Point on Hudson, Hoboken, NJ  07030, United States; Yu, Y., Financial Engineering Program, School of Business, Stevens Institute of Technology, 1 Castle Point on Hudson, Hoboken, NJ  07030, United States; Almahdi, S., Financial Engineering Program, School of Business, Stevens Institute of Technology, 1 Castle Point on Hudson, Hoboken, NJ  07030, United States","Inverse reinforcement learning; Investor sentiment; Sentiment reward; Support vector machine learning","Commerce; Costs; Inverse problems; Learning algorithms; Reinforcement learning; Signal processing; Support vector machines; Extraction mechanisms; Inverse reinforcement learning; Investor sentiments; Investor's sentiments; Market volatility; Sentiment reward; Transaction cost; Volatile markets; Investments","Yang, S.Y.; Financial Engineering Program, School of Business, Stevens Institute of Technology, 1 Castle Point on Hudson, United States; email: steve.yang@stevens.edu",Article,"Final",,Scopus,2-s2.0-85050940929
"Kasenberg D., Arnold T., Scheutz M.","57201556232;56895720600;6603548841;","Norms, Rewards, and the Intentional Stance: Comparing Machine Learning Approaches to Ethical Training",2018,"AIES 2018 - Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society",,,,"184","190",,3,"10.1145/3278721.3278774","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045238240&doi=10.1145%2f3278721.3278774&partnerID=40&md5=03f064b4f58eab282156155a9d5dbca0","Tufts University, Medford, MA, United States","Kasenberg, D., Tufts University, Medford, MA, United States; Arnold, T., Tufts University, Medford, MA, United States; Scheutz, M., Tufts University, Medford, MA, United States","intentional stance; norm inference; value alignment","Behavioral research; Machine learning; Reinforcement learning; AI systems; Ethical training; intentional stance; Inverse reinforcement learning; Machine learning approaches; norm inference; Reward function; Philosophical aspects",,Conference Paper,"Final",,Scopus,2-s2.0-85045238240
"Carey R.","57205675159;","Incorrigibility in the CIRL Framework",2018,"AIES 2018 - Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society",,,,"30","35",,,"10.1145/3278721.3278750","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061065414&doi=10.1145%2f3278721.3278750&partnerID=40&md5=e0611033fcf8297642824df480d04a87","Future of Humanity Institute, Oxford University, United Kingdom","Carey, R., Future of Humanity Institute, Oxford University, United Kingdom","ai safety; cirl; cooperative inverse reinforcement learning; corrigibility","Philosophical aspects; Reinforcement learning; cirl; corrigibility; Inverse reinforcement learning; Learning frameworks; Parameterized; Reward function; Technical sense; Probability distributions","Carey, R.; Future of Humanity Institute, Oxford UniversityUnited Kingdom; email: ry.duff@gmail.com",Conference Paper,"Final",,Scopus,2-s2.0-85061065414
"Hirakawa T., Yamashita T., Yoda K., Tamaki T., Fujiyoshi H.","55903063300;18042839400;7101801572;16204110300;6602601896;","Travel time-dependent maximum entropy inverse reinforcement learning for seabird trajectory prediction",2018,"Proceedings - 4th Asian Conference on Pattern Recognition, ACPR 2017",,, 8575862,"436","441",,,"10.1109/ACPR.2017.20","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060540464&doi=10.1109%2fACPR.2017.20&partnerID=40&md5=9b4df47ded97494301a45397999c21c0","Chubu University, Japan; Nagoya University, Japan; Hiroshima University, Japan","Hirakawa, T., Chubu University, Japan; Yamashita, T., Chubu University, Japan; Yoda, K., Nagoya University, Japan; Tamaki, T., Hiroshima University, Japan; Fujiyoshi, H., Chubu University, Japan","Animal Behavior Analysis; Markov Decision Process; Maximum Entropy Inverse Reinforcement Learning; Trajectory Prediction","Computer vision; Forecasting; Inverse problems; Learning algorithms; Machine learning; Markov processes; Reinforcement learning; Trajectories; Travel time; Animal behavior; Avoiding obstacle; Goal directed; Inverse reinforcement learning; Markov Decision Processes; Number of methods; Trajectory prediction; Maximum entropy methods",,Conference Paper,"Final",,Scopus,2-s2.0-85060540464
"Sun L., Zhan W., Tomizuka M.","56007948700;57193013312;35567325500;","Probabilistic Prediction of Interactive Driving Behavior via Hierarchical Inverse Reinforcement Learning",2018,"IEEE Conference on Intelligent Transportation Systems, Proceedings, ITSC","2018-November",, 8569453,"2111","2117",,8,"10.1109/ITSC.2018.8569453","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060465934&doi=10.1109%2fITSC.2018.8569453&partnerID=40&md5=5b13b0da39d2cb5dd1373402becbc5a4","Department of Mechanical Engineering, University of California, Berkeley, CA  94720, United States","Sun, L., Department of Mechanical Engineering, University of California, Berkeley, CA  94720, United States; Zhan, W., Department of Mechanical Engineering, University of California, Berkeley, CA  94720, United States; Tomizuka, M., Department of Mechanical Engineering, University of California, Berkeley, CA  94720, United States",,"Behavioral research; Forecasting; Highway traffic control; Intelligent systems; Intelligent vehicle highway systems; Roads and streets; Trajectories; Vehicles; Autonomous Vehicles; Hierarchical trajectory; Historical information; Human demonstrations; Inverse reinforcement learning; Mixture of distributions; Probabilistic prediction; Quantitative result; Reinforcement learning",,Conference Paper,"Final",,Scopus,2-s2.0-85060465934
"Morales L.Y., Naoki A., Yoshihara Y., Murase H.","57205545886;57205546702;57007898500;7101900108;","Towards Predictive Driving through Blind Intersections",2018,"IEEE Conference on Intelligent Transportation Systems, Proceedings, ITSC","2018-November",, 8569931,"716","722",,3,"10.1109/ITSC.2018.8569931","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060457479&doi=10.1109%2fITSC.2018.8569931&partnerID=40&md5=97fdbb1221961744765b46794d3f8011","Institute of Innovation for Future Society, Nagoya University, Nagoya, 464-8603, Japan; Graduate School of Information Science, Nagoya University, Nagoya, 464-8603, Japan","Morales, L.Y., Institute of Innovation for Future Society, Nagoya University, Nagoya, 464-8603, Japan; Naoki, A., Institute of Innovation for Future Society, Nagoya University, Nagoya, 464-8603, Japan; Yoshihara, Y., Institute of Innovation for Future Society, Nagoya University, Nagoya, 464-8603, Japan; Murase, H., Graduate School of Information Science, Nagoya University, Nagoya, 464-8603, Japan",,"Cost functions; Facings; Intelligent systems; Intelligent vehicle highway systems; Reinforcement learning; Trajectories; Blind intersection; Dangerous situations; Driving features; Feature weight; Inverse reinforcement learning; Modified Hausdorff Distance; Predictive drivings; State of the art; Cost benefit analysis",,Conference Paper,"Final",,Scopus,2-s2.0-85060457479
"Zhan W., Sun L., Hu Y., Li J., Tomizuka M.","57193013312;56007948700;57201981248;57201996480;35567325500;","Towards a Fatality-Aware Benchmark of Probabilistic Reaction Prediction in Highly Interactive Driving Scenarios",2018,"IEEE Conference on Intelligent Transportation Systems, Proceedings, ITSC","2018-November",, 8569785,"3274","3280",,7,"10.1109/ITSC.2018.8569785","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060498838&doi=10.1109%2fITSC.2018.8569785&partnerID=40&md5=2e122f2bc8445c448bed53dca4903820","Department of Mechanical Engineering, University of California, Berkeley, CA  94720, United States","Zhan, W., Department of Mechanical Engineering, University of California, Berkeley, CA  94720, United States; Sun, L., Department of Mechanical Engineering, University of California, Berkeley, CA  94720, United States; Hu, Y., Department of Mechanical Engineering, University of California, Berkeley, CA  94720, United States; Li, J., Department of Mechanical Engineering, University of California, Berkeley, CA  94720, United States; Tomizuka, M., Department of Mechanical Engineering, University of California, Berkeley, CA  94720, United States",,"Decision making; Forecasting; Intelligent systems; Intelligent vehicle highway systems; Motion planning; Problem solving; Reinforcement learning; Trajectories; Autonomous Vehicles; Interacting entities; Inverse reinforcement learning; Neural network (nn); Probabilistic graphical models (PGM); Probabilistic prediction; Probabilistic reaction; Situation prediction; Inverse problems",,Conference Paper,"Final",,Scopus,2-s2.0-85060498838
"Ezzeddine A., Mourad N., Araabi B.N., Ahmadabadi M.N.","57202777893;57202773807;35560470000;57189586135;","Combination of learning from non-optimal demonstrations and feedbacks using inverse reinforcement learning and Bayesian policy improvement",2018,"Expert Systems with Applications","112",,,"331","341",,,"10.1016/j.eswa.2018.06.035","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049349995&doi=10.1016%2fj.eswa.2018.06.035&partnerID=40&md5=a900d2e0d857d00c65794189adfcec98","Machine Learning and Computational Modeling Laboratory, School of ECE, College of Engineering, University of TehranTehran, Iran; Cognitive Systems Laboratory, School of ECE, College of Engineering, University of TehranTehran, Iran","Ezzeddine, A., Machine Learning and Computational Modeling Laboratory, School of ECE, College of Engineering, University of TehranTehran, Iran; Mourad, N., Cognitive Systems Laboratory, School of ECE, College of Engineering, University of TehranTehran, Iran; Araabi, B.N., Machine Learning and Computational Modeling Laboratory, School of ECE, College of Engineering, University of TehranTehran, Iran; Ahmadabadi, M.N., Cognitive Systems Laboratory, School of ECE, College of Engineering, University of TehranTehran, Iran","Human evaluative feedbacks; Interactive learning; Inverse reinforcement learning; Teaching by demonstrations","Air navigation; Demonstrations; Educational technology; Inverse problems; Iterative methods; Robots; Interactive learning; Inverse reinforcement learning; Learner Agent; Navigation tasks; Probabilistic modeling; Reward function; Teaching by demonstration; Transition model; Reinforcement learning","Ezzeddine, A.; Machine Learning and Computational Modeling Laboratory, School of ECE, College of Engineering, University of TehranIran; email: a.ezedin@ut.ac.ir",Article,"Final",,Scopus,2-s2.0-85049349995
"Li G., He B., Gomez R., Nakamura K.","56132381600;8355392200;12806763700;55489762000;","Interactive Reinforcement Learning from Demonstration and Human Evaluative Feedback",2018,"RO-MAN 2018 - 27th IEEE International Symposium on Robot and Human Interactive Communication",,, 8525837,"1156","1162",,1,"10.1109/ROMAN.2018.8525837","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058071354&doi=10.1109%2fROMAN.2018.8525837&partnerID=40&md5=aaaa1b008207c9a7ce324816b16d4ed1","College of Information Science and Engineering, Ocean University of China, Songling Road 238, Qingdao, 266100, China; Honda Research Institute Japan Ltd. Co., Wako, Japan","Li, G., College of Information Science and Engineering, Ocean University of China, Songling Road 238, Qingdao, 266100, China; He, B., College of Information Science and Engineering, Ocean University of China, Songling Road 238, Qingdao, 266100, China; Gomez, R., Honda Research Institute Japan Ltd. Co., Wako, Japan; Nakamura, K., Honda Research Institute Japan Ltd. Co., Wako, Japan",,"Demonstrations; Inverse problems; Robots; Discount factors; Incorrect action; Interactive Reinforcement Learning; Inverse reinforcement learning; Learning from demonstration; Model-based method; Natural interactions; Optimal policies; Reinforcement learning",,Conference Paper,"Final",,Scopus,2-s2.0-85058071354
"Gao H., Shi G., Xie G., Cheng B.","57195674051;57205095621;57195683595;36611317200;","Car-following method based on inverse reinforcement learning for autonomous vehicle decision-making",2018,"International Journal of Advanced Robotic Systems","15","6",,"","",,3,"10.1177/1729881418817162","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058520448&doi=10.1177%2f1729881418817162&partnerID=40&md5=1fef28768842be0feec0041c987b8f09","State Key Laboratory of Automotive Safety and Energy, Tsinghua University, Beijing, China; Center for Intelligent Connected Vehicles and Transportation, Tsinghua University, Beijing, China; Electrical Engineering Department, California Institute of Technology, Pasadena, CA, United States; Department of Automotive Engineering, Hunan University, Changsha, Hunan, China","Gao, H., State Key Laboratory of Automotive Safety and Energy, Tsinghua University, Beijing, China, Center for Intelligent Connected Vehicles and Transportation, Tsinghua University, Beijing, China; Shi, G., Electrical Engineering Department, California Institute of Technology, Pasadena, CA, United States; Xie, G., Department of Automotive Engineering, Hunan University, Changsha, Hunan, China; Cheng, B., State Key Laboratory of Automotive Safety and Energy, Tsinghua University, Beijing, China","automatic driving; autonomous vehicle; Car-following; decision-making; inverse reinforcement learning (IRL)","Automobile drivers; Behavioral research; Data visualization; Decision making; Inverse problems; Learning algorithms; Automatic driving; Autonomous Vehicles; Car following; Decision-making systems; Driving characteristics; Inverse reinforcement learning; Optimization problems; Sequential decisions; Reinforcement learning","Shi, G.; Electrical Engineering Department, California Institute of TechnologyUnited States; email: gshi@caltech.edu",Article,"Final",Open Access,Scopus,2-s2.0-85058520448
"Sheffield E.C., Shah M.D.","57205078055;57190221969;","Dungeon digger: Apprenticeship learning for procedural dungeon building agents",2018,"CHI PLAY 2018 - Proceedings of the 2018 Annual Symposium on Computer-Human Interaction in Play Companion Extended Abstracts",,,,"603","610",,1,"10.1145/3270316.3271539","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058437749&doi=10.1145%2f3270316.3271539&partnerID=40&md5=d52053ed1174c8a49a6ac018a4129706","College of Computer and Information Science, Northeastern University, Boston, MA  02115, United States","Sheffield, E.C., College of Computer and Information Science, Northeastern University, Boston, MA  02115, United States; Shah, M.D., College of Computer and Information Science, Northeastern University, Boston, MA  02115, United States","Apprenticeship Learning; Inverse Reinforcement Learning; Machine Learning; Procedural Content Generation","Apprentices; Artificial intelligence; Interactive computer systems; Learning systems; Reinforcement learning; Static analysis; Apprenticeship learning; Digital games; Game Levels; Inverse reinforcement learning; Level design; Procedural content generations; Usability studies; Video game levels; Human computer interaction",,Conference Paper,"Final",,Scopus,2-s2.0-85058437749
"Zou Q., Li H., Zhang R.","54406213100;57201675742;7404861962;","Inverse Reinforcement Learning via Neural Network in Driver Behavior Modeling",2018,"IEEE Intelligent Vehicles Symposium, Proceedings","2018-June",, 8500666,"1245","1250",,1,"10.1109/IVS.2018.8500666","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056785788&doi=10.1109%2fIVS.2018.8500666&partnerID=40&md5=2956ed31f8466b3b023f7e03611d0a13","BeiDou High Precision Positioning Service Technology Engineering Laboratory, Information Engineering College, Dalian University, DaLian, China; Mechanical and Electrical Engineering College, Dalian Minzu University, DaLian, China","Zou, Q., BeiDou High Precision Positioning Service Technology Engineering Laboratory, Information Engineering College, Dalian University, DaLian, China; Li, H., BeiDou High Precision Positioning Service Technology Engineering Laboratory, Information Engineering College, Dalian University, DaLian, China; Zhang, R., Mechanical and Electrical Engineering College, Dalian Minzu University, DaLian, China","Advanced Driver Assistance Systems; Automated Vehicles; Human Factors and Human Machine Interaction","Automobile drivers; Behavioral research; Decision making; Human computer interaction; Inverse problems; Markov processes; Neural networks; Reinforcement learning; Vehicles; Accuracy of decision makings; Automated vehicles; Convolutional neural network; Driver behavior modeling; Human machine interaction; Inverse reinforcement learning; Linear combinations; Markov Decision Processes; Advanced driver assistance systems",,Conference Paper,"Final",,Scopus,2-s2.0-85056785788
"Bogert K., Doshi P.","36095803300;23008336000;","Multi-robot inverse reinforcement learning under occlusion with estimation of state transitions",2018,"Artificial Intelligence","263",,,"46","73",,,"10.1016/j.artint.2018.07.002","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050363079&doi=10.1016%2fj.artint.2018.07.002&partnerID=40&md5=434c2a884a42d57e2e7d8b3a676c010a","Department of Computer Science, University of North Carolina Asheville, Asheville, NC  28804, United States; THINC Lab, Department of Computer Science, University of Georgia, Athens, GA  30602, United States","Bogert, K., Department of Computer Science, University of North Carolina Asheville, Asheville, NC  28804, United States; Doshi, P., THINC Lab, Department of Computer Science, University of Georgia, Athens, GA  30602, United States","Inverse reinforcement learning; Machine learning; Maximum entropy; Robotics","Learning systems; Maximum entropy methods; Reinforcement learning; Robotics; Robots; Application scenario; Future position; Inverse reinforcement learning; Learning agents; Learning from demonstration; Reward function; Robotic applications; State transitions; Inverse problems","Doshi, P.; THINC Lab, Department of Computer Science, University of GeorgiaUnited States; email: pdoshi@uga.edu",Article,"Final",,Scopus,2-s2.0-85050363079
"Massimo D., Ricci F.","56433358100;35270071500;","Harnessing a generalised user behaviour model for next-POI recommendation",2018,"RecSys 2018 - 12th ACM Conference on Recommender Systems",,,,"402","406",,5,"10.1145/3240323.3240392","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056800692&doi=10.1145%2f3240323.3240392&partnerID=40&md5=1ec935bc808b253e9dbc2cb395d6c23b","Free University of Bolzano-Bozen Bolzano, Italy","Massimo, D., Free University of Bolzano-Bozen Bolzano, Italy; Ricci, F., Free University of Bolzano-Bozen Bolzano, Italy","Inverse Reinforcement Learning; Recommender Systems; Tourism; User Modelling","Decision making; Recommender systems; Reinforcement learning; Action prediction; Experimental analysis; Human decision making; Inverse reinforcement learning; Points of interest; Recommendation strategies; Tourism; User Modelling; Behavioral research",,Conference Paper,"Final",,Scopus,2-s2.0-85056800692
"Ikenaga A., Arai S.","57204111901;14057611500;","Inverse reinforcement learning approach for elicitation of preferences in multi-objective sequential optimization",2018,"Proceedings - 2018 IEEE International Conference on Agents, ICA 2018",,, 8460075,"117","118",,,"10.1109/AGENTS.2018.8460075","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054488963&doi=10.1109%2fAGENTS.2018.8460075&partnerID=40&md5=08fa6d218fdcc74845dd63d553b11a6f","Chiba University, Dept. of Urban Environment Systems, Chiba, Japan","Ikenaga, A., Chiba University, Dept. of Urban Environment Systems, Chiba, Japan; Arai, S., Chiba University, Dept. of Urban Environment Systems, Chiba, Japan","Inverse reinforcement learning; Multi-objective sequential decision making; Preference elicitation","Decision making; Inverse problems; Pareto principle; Elicitation of preferences; Inverse reinforcement learning; Multi objective decision making; Pareto optimal solutions; Preference elicitation; Preference order; Sequential decision making; Sequential optimization; Reinforcement learning",,Conference Paper,"Final",,Scopus,2-s2.0-85054488963
"Li K., Rath M., Burdick J.W.","57207262277;55893097600;7103361644;","Inverse Reinforcement Learning via Function Approximation for Clinical Motion Analysis",2018,"Proceedings - IEEE International Conference on Robotics and Automation",,, 8460563,"610","617",,,"10.1109/ICRA.2018.8460563","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063164463&doi=10.1109%2fICRA.2018.8460563&partnerID=40&md5=ca197bf81bf63364b8ea0f1afd1b11bc","California Institute of Technology, Department of Mechanical and Civil Engineering, Pasadena, CA  91125, United States; University of California Los Angeles, Los Angeles, CA  90095, United States","Li, K., California Institute of Technology, Department of Mechanical and Civil Engineering, Pasadena, CA  91125, United States; Rath, M., University of California Los Angeles, Los Angeles, CA  90095, United States; Burdick, J.W., California Institute of Technology, Department of Mechanical and Civil Engineering, Pasadena, CA  91125, United States",,"Inverse problems; Machine learning; Motion analysis; Patient rehabilitation; Robotics; Function approximation; High-dimensional; Human movements; Inverse reinforcement learning; Linearly proportional; Optimality equation; Simulated environment; Spinal cord injuries (SCI); Reinforcement learning",,Conference Paper,"Final",,Scopus,2-s2.0-85063164463
"Rehder E., Wirth F., Lauer M., Stiller C.","56724034800;57208073979;56207151400;7102694878;","Pedestrian Prediction by Planning Using Deep Neural Networks",2018,"Proceedings - IEEE International Conference on Robotics and Automation",,, 8460203,"5903","5908",,12,"10.1109/ICRA.2018.8460203","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061302737&doi=10.1109%2fICRA.2018.8460203&partnerID=40&md5=7947d3bbf3174e02d503ee26c325ecdb","Daimler RandD, Environment Perception, Sindelfingen, Germany; Institute of Measurement and Control Systems, Karlsruhe Institute of Technology (KIT), Karlsruhe, 76131, Germany","Rehder, E., Daimler RandD, Environment Perception, Sindelfingen, Germany; Wirth, F., Institute of Measurement and Control Systems, Karlsruhe Institute of Technology (KIT), Karlsruhe, 76131, Germany; Lauer, M., Institute of Measurement and Control Systems, Karlsruhe Institute of Technology (KIT), Karlsruhe, 76131, Germany; Stiller, C., Institute of Measurement and Control Systems, Karlsruhe Institute of Technology (KIT), Karlsruhe, 76131, Germany",,"Autonomous vehicles; Forecasting; Motion estimation; Reinforcement learning; Robotics; Behavior patterns; Convolutional networks; Entire system; Experimental validations; Inverse reinforcement learning; Mixture density function; Motion prediction; Planning stages; Deep neural networks",,Conference Paper,"Final",,Scopus,2-s2.0-85061302737
"Guo M., Andersson S., Dimarogonas D.V.","55921188600;57196085202;6506281602;","Human-in-the-Loop Mixed-Initiative Control under Temporal Tasks",2018,"Proceedings - IEEE International Conference on Robotics and Automation",,, 8460793,"6395","6400",,1,"10.1109/ICRA.2018.8460793","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063131458&doi=10.1109%2fICRA.2018.8460793&partnerID=40&md5=836fbadebfa913c634c76c2fa906ee9f","School of Electrical Engineering, KTH Royal Institute of Technology, ACCESS Linnaeus CenterSE-100 44, Sweden","Guo, M., School of Electrical Engineering, KTH Royal Institute of Technology, ACCESS Linnaeus CenterSE-100 44, Sweden; Andersson, S., School of Electrical Engineering, KTH Royal Institute of Technology, ACCESS Linnaeus CenterSE-100 44, Sweden; Dimarogonas, D.V., School of Electrical Engineering, KTH Royal Institute of Technology, ACCESS Linnaeus CenterSE-100 44, Sweden",,"Controllers; Coordination reactions; Iterative methods; Reinforcement learning; Robotics; Robots; Temporal logic; Continuous controller; Hard and soft constraints; Human-in-the-loop; Human-in-the-loop simulations; Inverse reinforcement learning; Linear temporal logic; Mixed initiative; Online coordination; Robot programming",,Conference Paper,"Final",,Scopus,2-s2.0-85063131458
"Shkurti F., Kakodkar N., Dudek G.","47762168300;57193132765;7006413576;","Model-Based Probabilistic Pursuit via Inverse Reinforcement Learning",2018,"Proceedings - IEEE International Conference on Robotics and Automation",,, 8463196,"7804","7811",,1,"10.1109/ICRA.2018.8463196","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063153304&doi=10.1109%2fICRA.2018.8463196&partnerID=40&md5=f932861979d5226fd748eff6de9bae2d","Center for Intelligent Machines (CIM), School of Computer Science, McGill University in Montreal, Mobile Robotics Lab, Canada","Shkurti, F., Center for Intelligent Machines (CIM), School of Computer Science, McGill University in Montreal, Mobile Robotics Lab, Canada; Kakodkar, N., Center for Intelligent Machines (CIM), School of Computer Science, McGill University in Montreal, Mobile Robotics Lab, Canada; Dudek, G., Center for Intelligent Machines (CIM), School of Computer Science, McGill University in Montreal, Mobile Robotics Lab, Canada",,"Inverse problems; Machine learning; Robot programming; Robotics; Robots; Semantics; Visual servoing; Combinatorial search; Domain-specific knowledge; Graph representation; Integrated prediction; Inverse reinforcement learning; Multiple satellites; Pursuit algorithms; Short-term behavior; Reinforcement learning",,Conference Paper,"Final",,Scopus,2-s2.0-85063153304
"Gonzalez D.S., Erkent O., Romero-Cano V., Dibangoye J., Laugier C.","56382000700;16549364800;55920292800;25640845700;7007031683;","Modeling Driver Behavior from Demonstrations in Dynamic Environments Using Spatiotemporal Lattices",2018,"Proceedings - IEEE International Conference on Robotics and Automation",,, 8460208,"3384","3390",,1,"10.1109/ICRA.2018.8460208","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063162728&doi=10.1109%2fICRA.2018.8460208&partnerID=40&md5=8ba2c7dc745aa5dfea2919ced1ae3097","Inria Rhone-Alpes, Grenoble, France; Universidad Autónoma de Occidente, Cali, Colombia","Gonzalez, D.S., Inria Rhone-Alpes, Grenoble, France; Erkent, O., Inria Rhone-Alpes, Grenoble, France; Romero-Cano, V., Universidad Autónoma de Occidente, Cali, Colombia; Dibangoye, J., Inria Rhone-Alpes, Grenoble, France; Laugier, C., Inria Rhone-Alpes, Grenoble, France",,"Collision avoidance; Cost functions; Motion planning; Reinforcement learning; Robotics; Trajectories; Vehicles; Computational power; Dynamic environments; Dynamic obstacles; Instrumented vehicle; Inverse reinforcement learning; Model assessment; Model parameters; Path planning techniques; Inverse problems",,Conference Paper,"Final",,Scopus,2-s2.0-85063162728
"Zhou Z., Bloem M., Bambos N.","55971412900;23570762400;7004840524;","Infinite Time Horizon Maximum Causal Entropy Inverse Reinforcement Learning",2018,"IEEE Transactions on Automatic Control","63","9", 8115277,"2787","2802",,3,"10.1109/TAC.2017.2775960","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85035769716&doi=10.1109%2fTAC.2017.2775960&partnerID=40&md5=9ff127d1b5cd0f94f452dd030dbc991c","Department of Electrical Engineering, Department of Management Science and Engineering, Stanford University, Stanford, CA  94305, United States; Advanced Analytics, Steelcase, Inc., Grand Rapids, MI  49508, United States","Zhou, Z., Department of Electrical Engineering, Department of Management Science and Engineering, Stanford University, Stanford, CA  94305, United States; Bloem, M., Advanced Analytics, Steelcase, Inc., Grand Rapids, MI  49508, United States; Bambos, N., Department of Electrical Engineering, Department of Management Science and Engineering, Stanford University, Stanford, CA  94305, United States","Decision making; learning (artificial intelligence); Markov processes; optimization","Behavioral research; Convex optimization; Decision making; Entropy; Inverse problems; Markov processes; Analytical properties; Convex optimization problems; Efficient computation; Gradient based algorithm; Infinite time horizon; Inverse reinforcement learning; Markov Decision Processes; Optimization programs; Reinforcement learning","Zhou, Z.; Department of Electrical Engineering, Department of Management Science and Engineering, Stanford UniversityUnited States; email: zyzhou@stanford.edu",Article,"Final",,Scopus,2-s2.0-85035769716
"Bezzo N.","36547454800;","Predicting Malicious Intention in CPS under Cyber-Attack",2018,"Proceedings - 9th ACM/IEEE International Conference on Cyber-Physical Systems, ICCPS 2018",,, 8443756,"351","352",,,"10.1109/ICCPS.2018.00049","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053515139&doi=10.1109%2fICCPS.2018.00049&partnerID=40&md5=85840601a39fd91aeb4fa4d1818fac2d","Department of Systems and Information Engineering, Department of Electrical and Computer Engineering, University of Virginia, United States","Bezzo, N., Department of Systems and Information Engineering, Department of Electrical and Computer Engineering, University of Virginia, United States","autonomous vehicles; CPS; cyber security; machine learning; malicious intention; reachability analysis; resileincy; robotics; sensor spoofing","Crime; Cyber Physical System; Embedded systems; Forecasting; Inverse problems; Learning systems; Network security; Reinforcement learning; Risk assessment; Robotics; Autonomous Vehicles; Cyber security; malicious intention; Reachability analysis; resileincy; Computer crime","Bezzo, N.; Department of Systems and Information Engineering, Department of Electrical and Computer Engineering, University of VirginiaUnited States; email: nbezzo@virginia.edu",Conference Paper,"Final",,Scopus,2-s2.0-85053515139
"Kamalapurkar R.","55210138200;","Linear inverse reinforcement learning in continuous time and space",2018,"Proceedings of the American Control Conference","2018-June",, 8431430,"1683","1688",,1,"10.23919/ACC.2018.8431430","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052564010&doi=10.23919%2fACC.2018.8431430&partnerID=40&md5=a7f69319e3a63eee4b97b01d974621db","Oklahoma State University, School of Mechanical and Aerospace Engineering, United States","Kamalapurkar, R., Oklahoma State University, School of Mechanical and Aerospace Engineering, United States",,,"Kamalapurkar, R.; Oklahoma State University, School of Mechanical and Aerospace EngineeringUnited States; email: rushikesh.kamalapurkar@okstate.edu",Conference Paper,"Final",,Scopus,2-s2.0-85052564010
"Ouattara A., Aswani A.","57203659630;23391951400;","Duality Approach to Bilevel Programs with a Convex Lower Level",2018,"Proceedings of the American Control Conference","2018-June",, 8431802,"1388","1395",,,"10.23919/ACC.2018.8431802","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052592802&doi=10.23919%2fACC.2018.8431802&partnerID=40&md5=b4cc2b7588691958e06e4a96788aa645","University of California, Department of Industrial Engineering and Operations Research, Berkeley, CA  94720, United States","Ouattara, A., University of California, Department of Industrial Engineering and Operations Research, Berkeley, CA  94720, United States; Aswani, A., University of California, Department of Industrial Engineering and Operations Research, Berkeley, CA  94720, United States",,,,Conference Paper,"Final",,Scopus,2-s2.0-85052592802
"Imani M., Braga-Neto U.","57189031055;6603390822;","Optimal Control of Gene Regulatory Networks with Unknown Cost Function",2018,"Proceedings of the American Control Conference","2018-June",, 8431514,"3939","3944",,2,"10.23919/ACC.2018.8431514","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052573917&doi=10.23919%2fACC.2018.8431514&partnerID=40&md5=1c71a2419ebfe49214826375d313bd5e","Texas AM University, Department of Electrical and Computer Engineering, College Station, TX, United States","Imani, M., Texas AM University, Department of Electrical and Computer Engineering, College Station, TX, United States; Braga-Neto, U., Texas AM University, Department of Electrical and Computer Engineering, College Station, TX, United States",,,,Conference Paper,"Final",,Scopus,2-s2.0-85052573917
"Elnaggar M., Bezzo N.","57196424673;36547454800;","An IRL Approach for Cyber-Physical Attack Intention Prediction and Recovery",2018,"Proceedings of the American Control Conference","2018-June",, 8430922,"222","227",,2,"10.23919/ACC.2018.8430922","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052605411&doi=10.23919%2fACC.2018.8430922&partnerID=40&md5=be92afe98b5c45424b356be77129ff34","Department of Electrical Computer Engineering, University of Virginia, Department of Systems Information Engineering, United States","Elnaggar, M., Department of Electrical Computer Engineering, University of Virginia, Department of Systems Information Engineering, United States; Bezzo, N., Department of Electrical Computer Engineering, University of Virginia, Department of Systems Information Engineering, United States",,,,Conference Paper,"Final",,Scopus,2-s2.0-85052605411
"Lelerre M., Mouaddib A.-I., Jeanpierre L.","57192427231;6701745605;24490808200;","Robust inverse planning approaches for policy estimation of semi-autonomous agents",2018,"Proceedings - International Conference on Tools with Artificial Intelligence, ICTAI","2017-November",,,"951","958",,,"10.1109/ICTAI.2017.00146","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048476528&doi=10.1109%2fICTAI.2017.00146&partnerID=40&md5=c8fd3f12b78525c91f9514f951adfb18","Universite de Caen Normandie, Caen, France","Lelerre, M., Universite de Caen Normandie, Caen, France; Mouaddib, A.-I., Universite de Caen Normandie, Caen, France; Jeanpierre, L., Universite de Caen Normandie, Caen, France","Activity and Plan Recognition; Inverse reinforcement learning; MDP","Artificial intelligence; Efficiency; Inverse problems; Learning algorithms; Reinforcement learning; Coordination technique; Inverse planning; Inverse reinforcement learning; Optimal policies; Plan recognition; Prediction methods; Semi-autonomous agents; Autonomous agents",,Conference Paper,"Final",,Scopus,2-s2.0-85048476528
"Yamaguchi S., Naoki H., Ikeda M., Tsukada Y., Nakano S., Mori I., Ishii S.","57202389056;36608736300;57202389441;42862484600;7401622196;7201752391;7403110142;","Identification of animal behavioral strategies by inverse reinforcement learning",2018,"PLoS Computational Biology","14","5", e1006122,"","",,2,"10.1371/journal.pcbi.1006122","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048196142&doi=10.1371%2fjournal.pcbi.1006122&partnerID=40&md5=e7e3c630193f9442757a83682cb43263","Integrated Systems Biology Laboratory, Graduate School of Informatics, Kyoto University, Sakyo, Kyoto, Japan; Laboratory of Theoretical Biology, Graduate School of Biostudies, Kyoto University, Yoshidakonoecho, Sakyo, Kyoto, Japan; Data-driven Modeling Team, Research Center for Dynamic Living Systems, Graduate School of Biostudies, Kyoto University, Yoshidakonoecho, Sakyo, Kyoto, Japan; Group of Molecular Neurobiology, Graduate School of Science, Nagoya University, Furoucho, Chikusa, Nagoya, Aichi, Japan","Yamaguchi, S., Integrated Systems Biology Laboratory, Graduate School of Informatics, Kyoto University, Sakyo, Kyoto, Japan; Naoki, H., Laboratory of Theoretical Biology, Graduate School of Biostudies, Kyoto University, Yoshidakonoecho, Sakyo, Kyoto, Japan, Data-driven Modeling Team, Research Center for Dynamic Living Systems, Graduate School of Biostudies, Kyoto University, Yoshidakonoecho, Sakyo, Kyoto, Japan; Ikeda, M., Group of Molecular Neurobiology, Graduate School of Science, Nagoya University, Furoucho, Chikusa, Nagoya, Aichi, Japan; Tsukada, Y., Group of Molecular Neurobiology, Graduate School of Science, Nagoya University, Furoucho, Chikusa, Nagoya, Aichi, Japan; Nakano, S., Group of Molecular Neurobiology, Graduate School of Science, Nagoya University, Furoucho, Chikusa, Nagoya, Aichi, Japan; Mori, I., Group of Molecular Neurobiology, Graduate School of Science, Nagoya University, Furoucho, Chikusa, Nagoya, Aichi, Japan; Ishii, S., Integrated Systems Biology Laboratory, Graduate School of Informatics, Kyoto University, Sakyo, Kyoto, Japan",,"animal experiment; article; Caenorhabditis elegans; decision making; nerve cell; nonhuman; reinforcement; time series analysis; animal; animal behavior; biology; learning; physiology; taxis response; Animals; Behavior, Animal; Caenorhabditis elegans; Computational Biology; Decision Making; Learning; Reinforcement (Psychology); Taxis Response","Naoki, H.; Laboratory of Theoretical Biology, Graduate School of Biostudies, Kyoto University, Yoshidakonoecho, Sakyo, Japan; email: honda.naoki.4v@kyoto-u.ac.jp",Article,"Final",Open Access,Scopus,2-s2.0-85048196142
"González D.S., Romero-Cano V., DIbangoye J.S., Laugier C.","56382000700;55920292800;25640845700;7007031683;","Interaction-aware driver maneuver inference in highways using realistic driver models",2018,"IEEE Conference on Intelligent Transportation Systems, Proceedings, ITSC","2018-March",,,"1","8",,4,"10.1109/ITSC.2017.8317709","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046258026&doi=10.1109%2fITSC.2017.8317709&partnerID=40&md5=c7f71e3ed36f67f2ac86eefd0559ad8f","Inria Grenoble Rhône-Alpes, Montbonnot-Saint-Martin, 38330, France; Universidad Autónoma de Occidente, Cali, Colombia","González, D.S., Inria Grenoble Rhône-Alpes, Montbonnot-Saint-Martin, 38330, France; Romero-Cano, V., Universidad Autónoma de Occidente, Cali, Colombia; DIbangoye, J.S., Inria Grenoble Rhône-Alpes, Montbonnot-Saint-Martin, 38330, France; Laugier, C., Inria Grenoble Rhône-Alpes, Montbonnot-Saint-Martin, 38330, France",,"Intelligent systems; Intelligent vehicle highway systems; Reinforcement learning; State space methods; Behavior reasoning; False positive rates; Instrumented vehicle; Interacting multiple model filters; Inverse reinforcement learning; Lane change maneuvers; Scene understanding; State - space models; Forecasting",,Conference Paper,"Final",,Scopus,2-s2.0-85046258026
"Massimo D.","56433358100;","User preference modeling and exploitation in IoT scenarios",2018,"International Conference on Intelligent User Interfaces, Proceedings IUI",,,,"675","676",,2,"10.1145/3172944.3173151","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045153051&doi=10.1145%2f3172944.3173151&partnerID=40&md5=a140c55ea7ee7fc4dbbc1a1b96503b75","Free University of Bozen-Bolzano, Bolzano, Italy","Massimo, D., Free University of Bozen-Bolzano, Bolzano, Italy","Behaviour learning; Internet of Things; Inverse reinforcement learning; Recommender System","Decision making; Internet of things; Recommender systems; Reinforcement learning; User interfaces; Behaviour learning; Collective dynamics; Human behaviours; Human decision making; Inverse reinforcement learning; Learning approach; Points of interest; User preference modeling; Behavioral research","Massimo, D.; Free University of Bozen-BolzanoItaly; email: damassimo@inf.unibz.it",Conference Paper,"Final",,Scopus,2-s2.0-85045153051
"Tan C., Li Y., Cheng Y.","57201779251;55719284400;9733966500;","An inverse reinforcement learning algorithm for semi-Markov decision processes",2018,"2017 IEEE Symposium Series on Computational Intelligence, SSCI 2017 - Proceedings","2018-January",,,"1","6",,1,"10.1109/SSCI.2017.8280816","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046081256&doi=10.1109%2fSSCI.2017.8280816&partnerID=40&md5=5b926860778e8d141fcb45f6da89c1e7","Harbin Institute of Technology Shenzhen Graduate School, Shenzhen, Guangdong Province, China; China University of Mining and Technology, Xuzhou, Jiangsu Province, China","Tan, C., Harbin Institute of Technology Shenzhen Graduate School, Shenzhen, Guangdong Province, China; Li, Y., Harbin Institute of Technology Shenzhen Graduate School, Shenzhen, Guangdong Province, China; Cheng, Y., China University of Mining and Technology, Xuzhou, Jiangsu Province, China","Inverse Reinforcement Learning; Optimization; Performance Sensitivity; Reward Function; SMDP","Artificial intelligence; Convex optimization; Inverse problems; Markov processes; Optimization; Reinforcement learning; Sensitivity analysis; Average reward; Convex optimization problems; Inverse reinforcement learning; Performance sensitivity; Reward function; Semi-Markov decision process; SMDP; Structural form; Learning algorithms",,Conference Paper,"Final",,Scopus,2-s2.0-85046081256
"Tateo D., Pirotta M., Restelli M., Bonarini A.","57201780401;49362257000;6603404086;7003349352;","Gradient-based minimization for multi-expert Inverse Reinforcement Learning",2018,"2017 IEEE Symposium Series on Computational Intelligence, SSCI 2017 - Proceedings","2018-January",,,"1","8",,1,"10.1109/SSCI.2017.8280919","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046080056&doi=10.1109%2fSSCI.2017.8280919&partnerID=40&md5=bd97e3e2f0e1cd234cfb3f63e807ac51","Department of Electronics, Information and Bioengineering, Politecnico Di Milano, Milano, Italy","Tateo, D., Department of Electronics, Information and Bioengineering, Politecnico Di Milano, Milano, Italy; Pirotta, M., Department of Electronics, Information and Bioengineering, Politecnico Di Milano, Milano, Italy; Restelli, M., Department of Electronics, Information and Bioengineering, Politecnico Di Milano, Milano, Italy; Bonarini, A., Department of Electronics, Information and Bioengineering, Politecnico Di Milano, Milano, Italy",,"Iterative methods; Problem solving; Reinforcement learning; Direct problems; Gradient based; Human expert; Inverse reinforcement learning; Model-free method; Multi-expert; Reward function; Inverse problems",,Conference Paper,"Final",,Scopus,2-s2.0-85046080056
"Bhattacharyya R., Hazarika S.M.","57203215262;24343762400;","Object Affordance Driven Inverse Reinforcement Learning Through Conceptual Abstraction and Advice",2018,"Paladyn","9","1",,"277","294",,,"10.1515/pjbr-2018-0021","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054413782&doi=10.1515%2fpjbr-2018-0021&partnerID=40&md5=9d854aa8b94ced757826204d35626860","Department of CSE, Indian Institute of Information Technology Bhagalpur, Bihar, India; Biomimetic and Cognitive Robotics Lab, Department of Mechanical Engineering, Indian Institute of Technology Guwahati, Assam, India","Bhattacharyya, R., Department of CSE, Indian Institute of Information Technology Bhagalpur, Bihar, India; Hazarika, S.M., Biomimetic and Cognitive Robotics Lab, Department of Mechanical Engineering, Indian Institute of Technology Guwahati, Assam, India","human intent recognition; inverse reinforcement learning; MDP; object affordance",,"Bhattacharyya, R.; Department of CSE, Indian Institute of Information Technology BhagalpurIndia; email: omrbhattacharyya@gmail.com",Article,"Final",Open Access,Scopus,2-s2.0-85054413782
"Xie N., Yang Y., Shen H.T., Zhao T.T.","57203385327;57208586877;7404523209;53664758000;","Stroke-based stylization by learning sequential drawing examples",2018,"Journal of Visual Communication and Image Representation","51",,,"29","39",,,"10.1016/j.jvcir.2017.12.012","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040464828&doi=10.1016%2fj.jvcir.2017.12.012&partnerID=40&md5=8f331ef3ebd1c327e184f6c3b7fd1b08","Center for Future Media, School of Computer Science and Engineering, University of Electronic Science and Technology of China, Chengdu, 611731, China; School of Computer Science and Information Engineering, Tianjin University of Science and Technology, Tianjin, 300457, China","Xie, N., Center for Future Media, School of Computer Science and Engineering, University of Electronic Science and Technology of China, Chengdu, 611731, China; Yang, Y., Center for Future Media, School of Computer Science and Engineering, University of Electronic Science and Technology of China, Chengdu, 611731, China; Shen, H.T., Center for Future Media, School of Computer Science and Engineering, University of Electronic Science and Technology of China, Chengdu, 611731, China; Zhao, T.T., School of Computer Science and Information Engineering, Tianjin University of Science and Technology, Tianjin, 300457, China","Inverse reinforcement learning; Reinforcement learning; Stroke-based stylization","Computer graphics; Gradient methods; Rendering (computer graphics); Brush stroke; Drawing styles; Inverse reinforcement learning; Non-Photorealistic Rendering; PhotoShop; Policy gradient methods; Stroke-based stylization; Reinforcement learning","Zhao, T.T.; School of Computer Science and Information Engineering, Tianjin University of Science and TechnologyChina; email: tingting@tust.edu.cn",Article,"Final",,Scopus,2-s2.0-85040464828
"Bazenkov N., Goubko M.","6506058697;55826165700;","Advanced planning of home appliances with consumer’s preference learning",2018,"Communications in Computer and Information Science","934",,,"249","259",,1,"10.1007/978-3-030-00617-4_23","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061644446&doi=10.1007%2f978-3-030-00617-4_23&partnerID=40&md5=cbbd3b8eea1da85e139de0d431cf54f9","Trapeznikov Institute of Control Sciences, 65 Profsoyuznaya st., Moscow, 117997, Russian Federation","Bazenkov, N., Trapeznikov Institute of Control Sciences, 65 Profsoyuznaya st., Moscow, 117997, Russian Federation; Goubko, M., Trapeznikov Institute of Control Sciences, 65 Profsoyuznaya st., Moscow, 117997, Russian Federation","Bayesian learning; Consumer simulation; Inverse reinforcement learning; Preference learning; Smart grid","Automation; Bayesian networks; Costs; Decision trees; Domestic appliances; Energy efficiency; Energy utilization; Inverse problems; Machine learning; Parameter estimation; Reinforcement learning; Sales; Bayesian learning; Consumer simulation; Inverse reinforcement learning; Preference learning; Smart grid; Learning algorithms","Bazenkov, N.; Trapeznikov Institute of Control Sciences, 65 Profsoyuznaya st., Russian Federation; email: n.bazenkov@yandex.ru",Conference Paper,"Final",,Scopus,2-s2.0-85061644446
"Wang X., Klabjan D.","57204816856;6603598029;","Competitive multi-agent inverse reinforcement learning with sub-optimal demonstrations",2018,"35th International Conference on Machine Learning, ICML 2018","11",,,"8148","8175",,1,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057312102&partnerID=40&md5=720543ce79910a2c818996d6e417fc2b","Department of Industrial Engineering and Management Sciences, Northwestern University, Evanston, IL, United States","Wang, X., Department of Industrial Engineering and Management Sciences, Northwestern University, Evanston, IL, United States; Klabjan, D., Department of Industrial Engineering and Management Sciences, Northwestern University, Evanston, IL, United States",,"Approximation algorithms; Computation theory; Deep neural networks; Demonstrations; Game theory; Inverse problems; Multi agent systems; Reinforcement learning; Stochastic systems; Inverse reinforcement learning; Model approximations; Nash equilibrium policy; Numerical experiments; Objective functions; Reward function; Training algorithms; Zero-sum stochastic games; Learning algorithms","Klabjan, D.; Department of Industrial Engineering and Management Sciences, Northwestern UniversityUnited States; email: d-klabjan@northwestern.edu",Conference Paper,"Final",,Scopus,2-s2.0-85057312102
"Rhinehart N., Kitani K.","56743358400;15835267300;","First-Person Activity Forecasting from Video with Online Inverse Reinforcement Learning",2018,"IEEE Transactions on Pattern Analysis and Machine Intelligence",,,,"","",,1,"10.1109/TPAMI.2018.2873794","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054551080&doi=10.1109%2fTPAMI.2018.2873794&partnerID=40&md5=c449aee89097d1389c27cca215171710","Robotics Institute, Carnegie Mellon University, 6612 Pittsburgh, Pennsylvania United States 15213-3815 (e-mail: nrhineha@cs.cmu.edu); Robotics Institute, Carnegie Mellon University, Pittsburgh, Pennsylvania United States (e-mail: kkitani@cs.cmu.edu)","Rhinehart, N., Robotics Institute, Carnegie Mellon University, 6612 Pittsburgh, Pennsylvania United States 15213-3815 (e-mail: nrhineha@cs.cmu.edu); Kitani, K., Robotics Institute, Carnegie Mellon University, Pittsburgh, Pennsylvania United States (e-mail: kkitani@cs.cmu.edu)","Activity Forecasting; Cameras; First-Person Vision; Forecasting; Inverse Reinforcement Learning; Learning (artificial intelligence); Online Learning; Predictive models; Task analysis; Trajectory; Visualization","E-learning; Forecasting; Inverse problems; Semantics; Activity forecasting; Daily behaviors; First-person visions; Inverse reinforcement learning; Long-term goals; Modeling and forecasting; Online learning; Visual observations; Reinforcement learning",,Article in Press,"Article in Press",,Scopus,2-s2.0-85054551080
"Linares R., Furfaro R.","57202005934;12645288600;","Maneuvering detection and prediction using inverse reinforcement learning for space situational awareness",2018,"Advances in the Astronautical Sciences","162",,,"527","536",,,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049380771&partnerID=40&md5=bc05db72e7b6ece1533e15b199d75559","Department of Aerospace Engineering and Mechanics, University of Minnesota, Minneapolis, MN  55455, United States; Department of Systems and Industrial Engineering, University of Arizona, Tucson, AZ  85721, United States","Linares, R., Department of Aerospace Engineering and Mechanics, University of Minnesota, Minneapolis, MN  55455, United States; Furfaro, R., Department of Systems and Industrial Engineering, University of Arizona, Tucson, AZ  85721, United States",,"Astrophysics; Reinforcement learning; Systems engineering; Feature matching; Inverse reinforcement learning; Observational data; Orbital element; Proof of concept; Reward function; Simulation example; Space situational awareness; Inverse problems",,Conference Paper,"Final",,Scopus,2-s2.0-85049380771
"Henderson P., Chang W.-D., Bacon P.-L., Meger D., Pineau J., Precup D.","57202852272;57205539126;56742851900;23009425800;13404973100;6603288659;","Optiongan: Learning joint reward-policy options using generative adversarial inverse reinforcement learning",2018,"32nd AAAI Conference on Artificial Intelligence, AAAI 2018",,,,"3199","3206",,3,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060430951&partnerID=40&md5=543b0636d2b448bbde3bee62e5463107","School of Computer Science, McGill University, Montreal, Canada; Department of Electrical, Computer, and Software Engineering, McGill University, Montreal, Canada","Henderson, P., School of Computer Science, McGill University, Montreal, Canada; Chang, W.-D., Department of Electrical, Computer, and Software Engineering, McGill University, Montreal, Canada; Bacon, P.-L., School of Computer Science, McGill University, Montreal, Canada; Meger, D., School of Computer Science, McGill University, Montreal, Canada; Pineau, J., School of Computer Science, McGill University, Montreal, Canada; Precup, D., School of Computer Science, McGill University, Montreal, Canada",,"Artificial intelligence; Inverse problems; Complex problems; Continuous control; Inverse reinforcement learning; Joint rewards; Learning policy; Policy options; Reward function; Transfer learning; Reinforcement learning",,Conference Paper,"Final",,Scopus,2-s2.0-85060430951
"Fu J., Singh A., Ghosh D., Yang L., Levine S.","57192429455;57212845940;57208438508;57200615246;35731728100;","Variational inverse control with events: A general framework for data-driven reward definition",2018,"Advances in Neural Information Processing Systems","2018-December",,,"8538","8547",,,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064846573&partnerID=40&md5=355461d82ca6b877144a2c9ffde7f3a6","University of California, Berkeley, United States","Fu, J., University of California, Berkeley, United States; Singh, A., University of California, Berkeley, United States; Ghosh, D., University of California, Berkeley, United States; Yang, L., University of California, Berkeley, United States; Levine, S., University of California, Berkeley, United States",,"Inverse problems; Machine learning; Continuous control; Data driven; High-dimensional; Inverse control; Inverse reinforcement learning; Real-world; Reward function; Reinforcement learning",,Conference Paper,"Final",,Scopus,2-s2.0-85064846573
"Fu J., Luo K., Levine S.","57192429455;57210643709;35731728100;","Learning robust rewards with adversarial inverse reinforcement learning",2018,"6th International Conference on Learning Representations, ICLR 2018 - Conference Track Proceedings",,,,"","",,12,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056134319&partnerID=40&md5=48adeaf58164f352650de69ed07213a0","Department of Electrical Engineering and Computer Science, University of California, Berkeley, Berkeley, CA  94720, United States","Fu, J., Department of Electrical Engineering and Computer Science, University of California, Berkeley, Berkeley, CA  94720, United States; Luo, K., Department of Electrical Engineering and Computer Science, University of California, Berkeley, Berkeley, CA  94720, United States; Levine, S., Department of Electrical Engineering and Computer Science, University of California, Berkeley, Berkeley, CA  94720, United States",,"Decision making; Deep learning; Inverse problems; Learning algorithms; Machine learning; High-dimensional problems; Inverse reinforcement learning; ITS applications; Learning formulation; Reinforcement learning method; Reward function; Reinforcement learning",,Conference Paper,"Final",,Scopus,2-s2.0-85056134319
"Xie N., Zhao T.-T., Yang Y., Wei Q., Heng T.S.","57203385327;53664758000;57206629968;57202837194;57202829500;","Creative Sequential Data Learning Method for Artistic Stylisation and Rendering System",2018,"Ruan Jian Xue Bao/Journal of Software","29","4",,"1071","1084",,,"10.13328/j.cnki.jos.005414","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049536351&doi=10.13328%2fj.cnki.jos.005414&partnerID=40&md5=3469f5065f4b59520a3ace753ae3b64c","Center for Future Media, University of Electronic Science and Technology of China, Chengdu, 611731, China; School of Computer Science and Engineering, University of Electronic Science and Technology of China, Chengdu, 611731, China; School of Computer Science and Information Engineering, Tianjin University of Science and Technology, Tianjin, 300457, China; Guizhou Provincial Key Laboratory of Public Big Data, Guizhou University, Guiyang, 550025, China","Xie, N., Center for Future Media, University of Electronic Science and Technology of China, Chengdu, 611731, China, School of Computer Science and Engineering, University of Electronic Science and Technology of China, Chengdu, 611731, China; Zhao, T.-T., School of Computer Science and Information Engineering, Tianjin University of Science and Technology, Tianjin, 300457, China; Yang, Y., Center for Future Media, University of Electronic Science and Technology of China, Chengdu, 611731, China, School of Computer Science and Engineering, University of Electronic Science and Technology of China, Chengdu, 611731, China; Wei, Q., Guizhou Provincial Key Laboratory of Public Big Data, Guizhou University, Guiyang, 550025, China; Heng, T.S., Center for Future Media, University of Electronic Science and Technology of China, Chengdu, 611731, China, School of Computer Science and Engineering, University of Electronic Science and Technology of China, Chengdu, 611731, China","Image artistic stylization; IRL (inverse reinforcement learning); Multimedia information processing; Policy search; Sequential data analysis; Stroke-based rendering","Gradient methods; Reinforcement learning; Image artistic stylization; Inverse reinforcement learning; Multimedia information processing; Policy search; Sequential data analysis; Stroke based rendering; Rendering (computer graphics)","Wei, Q.; Guizhou Provincial Key Laboratory of Public Big Data, Guizhou UniversityChina; email: pkupiano@foxmail.com",Article,"Final",,Scopus,2-s2.0-85049536351
"Song J., Ren H., Sadigh D., Ermon S.","57202059115;57207374365;55052871500;35791579200;","Multi-agent generative adversarial imitation learning",2018,"6th International Conference on Learning Representations, ICLR 2018 - Workshop Track Proceedings",,,,"","",,3,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062993622&partnerID=40&md5=aeba4a4b8f53d03cfa71b79c9de4e5ec","Computer Science Department, Stanford University, United States","Song, J., Computer Science Department, Stanford University, United States; Ren, H., Computer Science Department, Stanford University, United States; Sadigh, D., Computer Science Department, Stanford University, United States; Ermon, S., Computer Science Department, Stanford University, United States",,"Inverse problems; Reinforcement learning; Actor-critic algorithm; Competitive agents; Empirical performance; High dimensional environment; Imitation learning; Inverse reinforcement learning; Markov games; Multi agent; Multi agent systems",,Conference Paper,"Final",,Scopus,2-s2.0-85062993622
"The Nguyen H., Bui L.T., Garratt M., Abbass H.","57204162280;8961832500;14015693000;6701824380;","Apprenticeship bootstrapping: Inverse Reinforcement learning in a multi-skill UAV-UGV coordination task",2018,"Proceedings of the International Joint Conference on Autonomous Agents and Multiagent Systems, AAMAS","3",,,"2204","2206",,1,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054712049&partnerID=40&md5=6e740b0ce76602b698947f6764f26649","School of Engineering and IT, UNSW-Canberra, Canberra, Australia; Department of Information Technology, Le Quy Don Technical University, Hanoi, Viet Nam","The Nguyen, H., School of Engineering and IT, UNSW-Canberra, Canberra, Australia; Bui, L.T., Department of Information Technology, Le Quy Don Technical University, Hanoi, Viet Nam; Garratt, M., School of Engineering and IT, UNSW-Canberra, Canberra, Australia; Abbass, H., School of Engineering and IT, UNSW-Canberra, Canberra, Australia","Apprenticeship learning; Deep Q-learning; Ground-air interaction; Inverse reinforcement Learning; Uavs; UGVs","Antennas; Apprentices; Autonomous agents; Deep learning; Demonstrations; Ground vehicles; Intelligent vehicle highway systems; Learning algorithms; Multi agent systems; Unmanned aerial vehicles (UAV); Apprenticeship learning; Ground-air interaction; Inverse reinforcement learning; Q-learning; UGVs; Reinforcement learning",,Conference Paper,"Final",,Scopus,2-s2.0-85054712049
"Li S., Xiao S., Zhu S., Du N., Xie Y., Song L.","57189095555;57192388438;57204050114;55697607100;35729495800;55587150100;","Learning temporal point processes via reinforcement learning",2018,"Advances in Neural Information Processing Systems","2018-December",,,"10781","10791",,1,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064806693&partnerID=40&md5=deb62993a6674599215aafe528b1a833","Georgia Institute of Technology, United States; Ant Financial, China; Google Brain, United States","Li, S., Georgia Institute of Technology, United States; Xiao, S., Ant Financial, China; Zhu, S., Georgia Institute of Technology, United States; Du, N., Google Brain, United States; Xie, Y., Georgia Institute of Technology, United States; Song, L., Georgia Institute of Technology, United States, Ant Financial, China",,"Continuous time systems; Information services; Learning algorithms; Machine learning; Maximum likelihood estimation; Recurrent neural networks; Stochastic systems; Generative process; Information networks; Intensity functions; Inverse reinforcement learning; Learning paradigms; Model misspecification; Stochastic policy; Synthetic and real data; Reinforcement learning","Li, S.; Georgia Institute of TechnologyUnited States; email: sli370@gatech.edu",Conference Paper,"Final",,Scopus,2-s2.0-85064806693
"Song J., Ren H., Ermon S., Sadigh D.","57202059115;57207374365;35791579200;55052871500;","Multi-agent generative adversarial imitation learning",2018,"Advances in Neural Information Processing Systems","2018-December",,,"7461","7472",,5,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064825526&partnerID=40&md5=bab83320612366687c47fe8c01edbaaf","Stanford University, United States","Song, J., Stanford University, United States; Ren, H., Stanford University, United States; Ermon, S., Stanford University, United States; Sadigh, D., Stanford University, United States",,"Inverse problems; Learning algorithms; Reinforcement learning; Actor-critic algorithm; Empirical performance; High dimensional environment; Imitation learning; Inverse reinforcement learning; Markov games; Multi-agent setting; Non-stationary environment; Multi agent systems",,Conference Paper,"Final",,Scopus,2-s2.0-85064825526
"Pan X., Shen Y.","57196036026;35180368000;","Human-interactive subgoal supervision for efficient inverse reinforcement learning",2018,"Proceedings of the International Joint Conference on Autonomous Agents and Multiagent Systems, AAMAS","2",,,"1380","1387",,2,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054640918&partnerID=40&md5=fe0f214fe890bc1243ff94b17aa148cf","University of California, Berkeley, CA, United States; Samsung Research America, Mountain View, CA, United States","Pan, X., University of California, Berkeley, CA, United States; Shen, Y., Samsung Research America, Mountain View, CA, United States","Human-in-the-loop; Inverse reinforcement learning; Sub-goals","Autonomous agents; Demonstrations; Human robot interaction; Motion planning; Multi agent systems; Efficient learning; Human-in-the-loop; Inverse reinforcement learning; Learning agents; Learning process; Reward function; Sequential task; Subgoals; Reinforcement learning",,Conference Paper,"Final",,Scopus,2-s2.0-85054640918
"Malik D., Palaniappan M., Fisac J.F., Hadfield-Menell D., Russell S., Dragan A.D.","57204799631;57204809339;56692857500;55921863400;7401538237;55193779100;","An efficient, generalized bellman update for cooperative inverse reinforcement learning",2018,"35th International Conference on Machine Learning, ICML 2018","8",,,"5435","5443",,1,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057282681&partnerID=40&md5=81944ea55dc9e370b62bf02d9e041d66","Department of Electrical Engineering and Computer Sciences, University of California, Berkeley, United States","Malik, D., Department of Electrical Engineering and Computer Sciences, University of California, Berkeley, United States; Palaniappan, M., Department of Electrical Engineering and Computer Sciences, University of California, Berkeley, United States; Fisac, J.F., Department of Electrical Engineering and Computer Sciences, University of California, Berkeley, United States; Hadfield-Menell, D., Department of Electrical Engineering and Computer Sciences, University of California, Berkeley, United States; Russell, S., Department of Electrical Engineering and Computer Sciences, University of California, Berkeley, United States; Dragan, A.D., Department of Electrical Engineering and Computer Sciences, University of California, Berkeley, United States",,"Artificial intelligence; Game theory; Human robot interaction; Inverse problems; Reinforcement learning; Alignment Problems; Exponential factors; Full informations; Inverse reinforcement learning; Non trivial problems; Parameter spaces; Specific properties; Two-player games; Behavioral research","Malik, D.; Department of Electrical Engineering and Computer Sciences, University of CaliforniaUnited States; email: dhruvmalik@berkeley.edu",Conference Paper,"Final",,Scopus,2-s2.0-85057282681
"Yang J., Ye X., Trivedi R., Xu H., Zha H.","57202454087;57205479712;36976457200;55430229800;57201737688;","Learning deep mean field games for modeling large population behavior",2018,"6th International Conference on Learning Representations, ICLR 2018 - Conference Track Proceedings",,,,"","",,1,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071178232&partnerID=40&md5=2abd6f849f34b2037d638fe5714c4315","Georgia Institute of Technology, United States; Georgia State University, United States","Yang, J., Georgia Institute of Technology, United States; Ye, X., Georgia State University, United States; Trivedi, R., Georgia Institute of Technology, United States; Xu, H., Georgia Institute of Technology, United States; Zha, H., Georgia Institute of Technology, United States",,"Game theory; Inverse problems; Markov processes; Population distribution; Population statistics; Reinforcement learning; Collective behavior; Discrete state space; Inverse reinforcement learning; Large population; Markov Decision Processes; Mean field games; Real-world system; Temporal evolution; Deep learning",,Conference Paper,"Final",,Scopus,2-s2.0-85071178232
"Jiang Y., Deng W., Wang J., Zhu B.","57190565558;35229007600;57202342421;51666195400;","Studies on Drivers' Driving Styles Based on Inverse Reinforcement Learning",2018,"SAE Technical Papers","2018-April",,,"","",,,"10.4271/2018-01-0612","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045526693&doi=10.4271%2f2018-01-0612&partnerID=40&md5=b739715ea81417e25a5fbccb4160734d","Jilin University, China; General Motors LLC, China","Jiang, Y., Jilin University, China; Deng, W., Jilin University, China; Wang, J., General Motors LLC, China; Zhu, B., Jilin University, China",,"Accident prevention; Automobile drivers; Automotive industry; Behavioral research; Boltzmann equation; Highway accidents; Human computer interaction; Inverse problems; Maximum likelihood; Reinforcement learning; Boltzmann distribution; Driving characteristics; Inverse reinforcement learning; Longitudinal acceleration; Longitudinal driving; Physical approaches; Reinforcement Learning theories; Surrounding environment; Advanced driver assistance systems",,Conference Paper,"Final",,Scopus,2-s2.0-85045526693
"Lin H.-I., Nguyen X.-A., Chen W.-K.","24780828300;57192820181;56027664700;","Active intention inference for robot-human collaboration",2018,"International Journal of Computational Methods and Experimental Measurements","6","4",,"772","784",,,"10.2495/CMEM-V6-N4-772-784","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064214841&doi=10.2495%2fCMEM-V6-N4-772-784&partnerID=40&md5=b4a049894f1ec7805e4e4a5df4e4d8f8","Graduate Institute of Automation Technology, National Taipei University of Technology, Taipei, Taiwan","Lin, H.-I., Graduate Institute of Automation Technology, National Taipei University of Technology, Taipei, Taiwan; Nguyen, X.-A., Graduate Institute of Automation Technology, National Taipei University of Technology, Taipei, Taiwan; Chen, W.-K., Graduate Institute of Automation Technology, National Taipei University of Technology, Taipei, Taiwan","Human gesture recognition; Human-robot collaboration; Markov decision process",,"LIN, H.-I.; Graduate Institute of Automation Technology, National Taipei University of TechnologyTaiwan",Article,"Final",Open Access,Scopus,2-s2.0-85064214841
"Rhinehart N., Kitani K.M.","56743358400;15835267300;","First-Person Activity Forecasting with Online Inverse Reinforcement Learning",2017,"Proceedings of the IEEE International Conference on Computer Vision","2017-October",, 8237661,"3716","3725",,20,"10.1109/ICCV.2017.399","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041899383&doi=10.1109%2fICCV.2017.399&partnerID=40&md5=3bae9da81be00c91f5f10c896e53d69a","Robotics Institute, Carnegie Mellon University, Pittsburgh, PA  15213, United States","Rhinehart, N., Robotics Institute, Carnegie Mellon University, Pittsburgh, PA  15213, United States; Kitani, K.M., Robotics Institute, Carnegie Mellon University, Pittsburgh, PA  15213, United States",,"Behavioral research; Computer vision; Forecasting; Inverse problems; Reinforcement learning; Semantics; Activity forecasting; Daily behaviors; Inverse reinforcement learning; Long-term goals; Modeling and forecasting; Space and time; Streaming data; Visual observations; E-learning",,Conference Paper,"Final",,Scopus,2-s2.0-85041899383
"Nam C., Walker P., Lewis M., Sycara K.","35590319700;54785528900;57200329926;7006431929;","Predicting trust in human control of swarms via inverse reinforcement learning",2017,"RO-MAN 2017 - 26th IEEE International Symposium on Robot and Human Interactive Communication","2017-January",,,"528","533",,2,"10.1109/ROMAN.2017.8172353","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045762040&doi=10.1109%2fROMAN.2017.8172353&partnerID=40&md5=b14fde749d9be2526be024136c488c8d","Robotics Institute, Carnegie Mellon University, Pittsburgh, PA, United States; School of Information Science, University of PittsburghPA, United States","Nam, C., Robotics Institute, Carnegie Mellon University, Pittsburgh, PA, United States; Walker, P., School of Information Science, University of PittsburghPA, United States; Lewis, M., School of Information Science, University of PittsburghPA, United States; Sycara, K., Robotics Institute, Carnegie Mellon University, Pittsburgh, PA, United States",,"Learning algorithms; Markov processes; Robots; Human-in-the-loop; Inverse reinforcement learning; Markov Decision Processes; Operator control; Physical characteristics; Physical parameters; Search missions; Task performance; Reinforcement learning",,Conference Paper,"Final",,Scopus,2-s2.0-85045762040
"Inga J., Köpf F., Flad M., Hohmann S.","57188985251;57201292669;6603246805;56027574000;","Individual human behavior identification using an inverse reinforcement learning method",2017,"2017 IEEE International Conference on Systems, Man, and Cybernetics, SMC 2017","2017-January",,,"99","104",,3,"10.1109/SMC.2017.8122585","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044202103&doi=10.1109%2fSMC.2017.8122585&partnerID=40&md5=a08f6e202a8945f74f285a104624c29a","Institute of Control Systems, Karlsruhe Institute of Technology, Karlsruhe, Germany","Inga, J., Institute of Control Systems, Karlsruhe Institute of Technology, Karlsruhe, Germany; Köpf, F., Institute of Control Systems, Karlsruhe Institute of Technology, Karlsruhe, Germany; Flad, M., Institute of Control Systems, Karlsruhe Institute of Technology, Karlsruhe, Germany; Hohmann, S., Institute of Control Systems, Karlsruhe Institute of Technology, Karlsruhe, Germany","Human behavior identification; Inverse optimal control; Inverse reinforcement learning; Shared control","Cost functions; Costs; Cybernetics; Inverse problems; Navigation; Reinforcement learning; Human behaviors; Human machine interaction; Inverse reinforcement learning; Inverse-optimal control; Movement trajectories; Optimal control theory; Reference trajectories; Shared control; Behavioral research",,Conference Paper,"Final",,Scopus,2-s2.0-85044202103
"Pang Y., Yabe T., Tsubouchi K., Sekimoto Y.","57196486490;57189643794;24315190800;53364561900;","Modeling and reproducing human daily travel behavior from GPS data: A markov decision process approach",2017,"Proceedings of the 1st ACM SIGSPATIAL Workshop on Prediction of Human Mobility, PredictGIS 2017","2017-January",,,"","",,,"10.1145/3152341.3152347","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050971626&doi=10.1145%2f3152341.3152347&partnerID=40&md5=ed62d514ac52e551346fc444dbde1d21","Department of Civil Engineering, University of Tokyo, 4-6-1, Komaba, Meguro, Tokyo, 153-8505, Japan; Yahoo Japan Corporation, Chiyoda, Tokyo, Japan; Institute of Industrial Science, University of Tokyo, 4-6-1, Komaba, Meguro, Tokyo, 153-8505, Japan","Pang, Y., Department of Civil Engineering, University of Tokyo, 4-6-1, Komaba, Meguro, Tokyo, 153-8505, Japan; Yabe, T., Department of Civil Engineering, University of Tokyo, 4-6-1, Komaba, Meguro, Tokyo, 153-8505, Japan; Tsubouchi, K., Yahoo Japan Corporation, Chiyoda, Tokyo, Japan; Sekimoto, Y., Institute of Industrial Science, University of Tokyo, 4-6-1, Komaba, Meguro, Tokyo, 153-8505, Japan","Daily travel behavior; Human mobility; Urban dynamic","Decision making; Global positioning system; Inverse problems; Markov processes; mHealth; Population statistics; Reinforcement learning; Smartphones; Urban planning; Urban transportation; Different granularities; Human mobility; Inverse reinforcement learning; Markov Decision Processes; Smart-phone applications; Tokyo metropolitan areas; Travel behaviors; Urban dynamics; Behavioral research",,Conference Paper,"Final",,Scopus,2-s2.0-85050971626
"Li C., Cao L., Zhang Y., Chen X., Zhou Y., Duan L.","56103739800;57198528477;56653349600;57192470874;35760002400;57200961456;","Knowledge-based deep reinforcement learning: a review",2017,"Xi Tong Gong Cheng Yu Dian Zi Ji Shu/Systems Engineering and Electronics","39","11",,"2603","2613",,4,"10.3969/j.issn.1001-506X.2017.11.30","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042690702&doi=10.3969%2fj.issn.1001-506X.2017.11.30&partnerID=40&md5=323703ebfb8ffdca926e6267bbe78122","Institute of Command Information System, PLA University of Science and Technology, Nanjing, 210007, China; College of Mechanical Engineering, Zhejiang University, Hangzhou, 310027, China","Li, C., Institute of Command Information System, PLA University of Science and Technology, Nanjing, 210007, China; Cao, L., Institute of Command Information System, PLA University of Science and Technology, Nanjing, 210007, China; Zhang, Y., Institute of Command Information System, PLA University of Science and Technology, Nanjing, 210007, China; Chen, X., Institute of Command Information System, PLA University of Science and Technology, Nanjing, 210007, China; Zhou, Y., Institute of Command Information System, PLA University of Science and Technology, Nanjing, 210007, China; Duan, L., College of Mechanical Engineering, Zhejiang University, Hangzhou, 310027, China","Deep reinforcement learning; Exploration strategy; Inverse reinforcement learning; Knowledge","Deep learning; Knowledge based systems; Exploration strategies; Inverse reinforcement learning; Knowledge; Knowledge based; Learning efficiency; Sequential decisions; Structured information; Trial and error; Reinforcement learning",,Review,"Final",,Scopus,2-s2.0-85042690702
"Li Z., Kiseleva J., De Rijke M., Grotov A.","57195630442;36170720700;54790525600;56568207600;","Towards learning reward functions from user interactions",2017,"ICTIR 2017 - Proceedings of the 2017 ACM SIGIR International Conference on the Theory of Information Retrieval",,,,"289","292",,1,"10.1145/3121050.3121098","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85033215376&doi=10.1145%2f3121050.3121098&partnerID=40&md5=ddbe414389c116035f0fe3ceee854a01","University of Amsterdam, Amsterdam, Netherlands; UserSat.com, University of Amsterdam, Amsterdam, Netherlands","Li, Z., University of Amsterdam, Amsterdam, Netherlands; Kiseleva, J., UserSat.com, University of Amsterdam, Amsterdam, Netherlands; De Rijke, M., University of Amsterdam, Amsterdam, Netherlands; Grotov, A., University of Amsterdam, Amsterdam, Netherlands","Interactive systems; Inverse reinforcement learning; Online evaluation","Behavioral research; Function evaluation; Information retrieval; Inverse problems; Learning systems; Online systems; Reinforcement learning; Search engines; Analytic approach; Cultural heritages; Current situation; Dynamic approaches; Interactive system; Inverse reinforcement learning; On-line evaluation; Sequence of actions; Recommender systems",,Conference Paper,"Final",,Scopus,2-s2.0-85033215376
"Truong X.-T., Ngo T.D.","56581881400;15755660100;","Toward Socially Aware Robot Navigation in Dynamic and Crowded Environments: A Proactive Social Motion Model",2017,"IEEE Transactions on Automation Science and Engineering","14","4", 8011466,"1743","1760",,15,"10.1109/TASE.2017.2731371","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028458074&doi=10.1109%2fTASE.2017.2731371&partnerID=40&md5=e86e54f4fefe9c9188bd30c966d79287","Faculty of Science, University of Brunei Darussalam, Bandar Seri Begawan, BE1410, Brunei Darussalam; Faculty of Control Engineering, Le Quy Don Technical University, Hanoi, 10000, Viet Nam; More-Than-One Robotics Laboratory, School of Sustainable Design Engineering, University of Prince Edward Island, Charlottetown, PE  C1A 4P3, Canada","Truong, X.-T., Faculty of Science, University of Brunei Darussalam, Bandar Seri Begawan, BE1410, Brunei Darussalam, Faculty of Control Engineering, Le Quy Don Technical University, Hanoi, 10000, Viet Nam; Ngo, T.D., Faculty of Science, University of Brunei Darussalam, Bandar Seri Begawan, BE1410, Brunei Darussalam, More-Than-One Robotics Laboratory, School of Sustainable Design Engineering, University of Prince Edward Island, Charlottetown, PE  C1A 4P3, Canada","Human comfortable safety; mobile service robots; proactive social motion model (PSMM); social robots; socially aware robot navigation","Behavioral research; Human robot interaction; Intelligent robots; Mobile robots; Mobile telecommunication systems; Motion planning; Navigation; Robot programming; Robots; Dynamic environments; Interactive informations; Mobile service robots; Motion modeling; Path planning techniques; Reciprocal velocity obstacles; Robot navigation; Social robots; Economic and social effects","Truong, X.-T.; Faculty of Science, University of Brunei DarussalamBrunei Darussalam; email: xuantung.truong@gmail.com",Article,"Final",,Scopus,2-s2.0-85028458074
"Wang X., Zhang W., Chen J.","57197749223;35812744600;46661788200;","Comparison and implementation of high cited inverse reinforcement learning algorithms in object world",2017,"Proceedings - 9th International Conference on Intelligent Human-Machine Systems and Cybernetics, IHMSC 2017","2",, 8048179,"374","377",,,"10.1109/IHMSC.2017.195","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85034427860&doi=10.1109%2fIHMSC.2017.195&partnerID=40&md5=778cb9e35f26b700d29dc0db0e824af0","College of Mechatronic Engineering and Automation, National University of Defense Technology, Changsha, China","Wang, X., College of Mechatronic Engineering and Automation, National University of Defense Technology, Changsha, China; Zhang, W., College of Mechatronic Engineering and Automation, National University of Defense Technology, Changsha, China; Chen, J., College of Mechatronic Engineering and Automation, National University of Defense Technology, Changsha, China","Inverse reinforcement learning; MAXENT; MMP; MMPBOOST","Cybernetics; Intelligent agents; Inverse problems; Learning algorithms; Man machine systems; Energy-based methods; Environment change; Inverse reinforcement learning; MAXENT; Maximum margin planning; MMPBOOST; Reinforcement learning agent; Reward function; Reinforcement learning",,Conference Paper,"Final",,Scopus,2-s2.0-85034427860
"Saha O., Dasgupta P.","57188823078;7201405936;","Improved reward estimation for efficient robot navigation using inverse reinforcement learning",2017,"2017 NASA/ESA Conference on Adaptive Hardware and Systems, AHS 2017",,, 8046385,"245","252",,2,"10.1109/AHS.2017.8046385","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032889169&doi=10.1109%2fAHS.2017.8046385&partnerID=40&md5=04f8bc8c076b6efc6467926d58f5769b","Computer Science Department, University of Nebraska, Omaha, United States","Saha, O., Computer Science Department, University of Nebraska, Omaha, United States; Dasgupta, P., Computer Science Department, University of Nebraska, Omaha, United States",,"Hardware; Inverse problems; Learning algorithms; Learning systems; Markov processes; NASA; Navigation; Random errors; Reinforcement learning; Central problems; Distance minimizations; Extraterrestrial environments; Inverse reinforcement learning; Navigation algorithms; Reinforcement learning techniques; Robot navigation; Semi-Markov decision process; Robots",,Conference Paper,"Final",,Scopus,2-s2.0-85032889169
"Choi S., Kim S., Jin Kim H.","56031605700;55268251400;6506659733;","Inverse reinforcement learning control for trajectory tracking of a multirotor UAV",2017,"International Journal of Control, Automation and Systems","15","4",,"1826","1834",,9,"10.1007/s12555-015-0483-3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85022187278&doi=10.1007%2fs12555-015-0483-3&partnerID=40&md5=fb628e8e312b5b7d40ae309e470410dd","Department of Mechanical and Aerospace engineering, Seoul National University, Seoul, South Korea","Choi, S., Department of Mechanical and Aerospace engineering, Seoul National University, Seoul, South Korea; Kim, S., Department of Mechanical and Aerospace engineering, Seoul National University, Seoul, South Korea; Jin Kim, H., Department of Mechanical and Aerospace engineering, Seoul National University, Seoul, South Korea","Inverse reinforcement learning; learning from demonstration; multirotor control; particle swarm optimization","Aircraft control; Controllers; Demonstrations; Education; Hidden Markov models; Markov processes; Number theory; Particle swarm optimization (PSO); Trajectories; Unmanned aerial vehicles (UAV); Control performance; Dynamic time warping; Flight maneuvers; Inverse reinforcement learning; Learning from demonstration; Trajectory tracking; Trajectory tracking errors; UAV (unmanned aerial vehicle); Reinforcement learning","Jin Kim, H.; Department of Mechanical and Aerospace engineering, Seoul National UniversitySouth Korea; email: hjinkim@snu.ac.kr",Article,"Final",,Scopus,2-s2.0-85022187278
"Piot B., Geist M., Pietquin O.","55697434100;25929145100;16040586900;","Bridging the gap between imitation learning and inverse reinforcement learning",2017,"IEEE Transactions on Neural Networks and Learning Systems","28","8", 7464854,"1814","1826",,10,"10.1109/TNNLS.2016.2543000","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84966372828&doi=10.1109%2fTNNLS.2016.2543000&partnerID=40&md5=7f1b1a0062b3c2a8ae88733fc40554ac","Université Lille 1, Centrale Lille, INRIA, CNRS, Lille, 59000, France; UMI 2958, Georgia Tech-CNRS, CentraleSupélec, Université Paris-Saclay, Metz, 57070, France; IUF, Université Lille 1, CNRS, Lille, 59000, France","Piot, B., Université Lille 1, Centrale Lille, INRIA, CNRS, Lille, 59000, France; Geist, M., UMI 2958, Georgia Tech-CNRS, CentraleSupélec, Université Paris-Saclay, Metz, 57070, France; Pietquin, O., IUF, Université Lille 1, CNRS, Lille, 59000, France","Imitation learning (IL); inverse reinforcement learning (IRL); learning from demonstrations (LfD)","Algorithms; Apprentices; Demonstrations; Inverse problems; Markov processes; Dynamic environments; Imitation learning; Inverse reinforcement learning; Learning from demonstration; Markov Decision Processes; Policy framework; Robust solutions; Trajectory matching; Reinforcement learning","Piot, B.; Université Lille 1, Centrale Lille, INRIA, CNRSFrance; email: bilal.piot@univ-lille1.fr",Article,"Final",,Scopus,2-s2.0-84966372828
"Shiarlis K., Messias J., Whiteson S.","57193494529;55208968600;10240257400;","Rapidly exploring learning trees",2017,"Proceedings - IEEE International Conference on Robotics and Automation",,, 7989184,"1541","1548",,5,"10.1109/ICRA.2017.7989184","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028006248&doi=10.1109%2fICRA.2017.7989184&partnerID=40&md5=7ba03142234d21e98caffa7f2ed02da4","Informatics Institute, University of Amsterdam, Netherlands; Department of Computer Science, University of Oxford, United Kingdom","Shiarlis, K., Informatics Institute, University of Amsterdam, Netherlands; Messias, J., Informatics Institute, University of Amsterdam, Netherlands; Whiteson, S., Department of Computer Science, University of Oxford, United Kingdom",,"Cost functions; Costs; Inverse problems; Motion planning; Reinforcement learning; Robot programming; Robotics; Visual communication; Computational costs; Inverse learning; Inverse reinforcement learning; Maximum margin planning; Planning procedure; Rapidly-exploring random trees; Social navigation; Telepresence robots; Robots",,Conference Paper,"Final",,Scopus,2-s2.0-85028006248
"Massimo D., Elahi M., Ricci F.","56433358100;34879649100;35270071500;","Learning user preferences by observing user-items interactions in an IoT augmented space",2017,"UMAP 2017 - Adjunct Publication of the 25th Conference on User Modeling, Adaptation and Personalization",,,,"35","40",,5,"10.1145/3099023.3099070","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026852471&doi=10.1145%2f3099023.3099070&partnerID=40&md5=472589b29ddc14b835e24dc467efea30","Free University of Bolzano-Bozen Piazza Domenicani, 3 Bolzano, Italy","Massimo, D., Free University of Bolzano-Bozen Piazza Domenicani, 3 Bolzano, Italy; Elahi, M., Free University of Bolzano-Bozen Piazza Domenicani, 3 Bolzano, Italy; Ricci, F., Free University of Bolzano-Bozen Piazza Domenicani, 3 Bolzano, Italy",,"Decision making; Internet of things; Inverse problems; Reinforcement learning; Decision making process; Inverse reinforcement learning; Making decision; Preference learning; Proof of concept; Sequential manners; User behaviour; Behavioral research",,Conference Paper,"Final",,Scopus,2-s2.0-85026852471
"Budhraja K.K., Oates T.","55605655400;7005499321;","Neuroevolution-based Inverse Reinforcement Learning",2017,"2017 IEEE Congress on Evolutionary Computation, CEC 2017 - Proceedings",,, 7969297,"67","76",,,"10.1109/CEC.2017.7969297","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027867586&doi=10.1109%2fCEC.2017.7969297&partnerID=40&md5=1841976a29abf97075f0b5db4073fbd3","Computer Science and Electrical Engineering, University of Maryland, Baltimore County, MD  21250, United States","Budhraja, K.K., Computer Science and Electrical Engineering, University of Maryland, Baltimore County, MD  21250, United States; Oates, T., Computer Science and Electrical Engineering, University of Maryland, Baltimore County, MD  21250, United States",,,,Conference Paper,"Final",,Scopus,2-s2.0-85027867586
"Köpf F., Inga J., Rothfuß S., Flad M., Hohmann S.","57201292669;57188985251;56236786400;6603246805;56027574000;","Inverse Reinforcement Learning for Identification in Linear-Quadratic Dynamic Games",2017,"IFAC-PapersOnLine","50","1",,"14902","14908",,4,"10.1016/j.ifacol.2017.08.2537","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044866145&doi=10.1016%2fj.ifacol.2017.08.2537&partnerID=40&md5=8f45822eae64fc413bee409ffabc4b7a","Institute of Control Systems, Karlsruhe Institute of Technology, Karlsruhe, 76131, Germany","Köpf, F., Institute of Control Systems, Karlsruhe Institute of Technology, Karlsruhe, 76131, Germany; Inga, J., Institute of Control Systems, Karlsruhe Institute of Technology, Karlsruhe, 76131, Germany; Rothfuß, S., Institute of Control Systems, Karlsruhe Institute of Technology, Karlsruhe, 76131, Germany; Flad, M., Institute of Control Systems, Karlsruhe Institute of Technology, Karlsruhe, 76131, Germany; Hohmann, S., Institute of Control Systems, Karlsruhe Institute of Technology, Karlsruhe, 76131, Germany","Game Theory; Identification; Inverse Optimal Control; Inverse Reinforcement Learning; Maximum Entropy; Shared Control","Behavioral research; Cost functions; Costs; Game theory; Identification (control systems); Inverse problems; Maximum entropy methods; Automatic controllers; Identification algorithms; Inverse reinforcement learning; Inverse-optimal control; Real-time application; Shared control; Simulation example; Theory of dynamics; Reinforcement learning",,Article,"Final",Open Access,Scopus,2-s2.0-85044866145
"Banovic N., Wang A., Jin Y., Chang C., Ramos J., Dey A.K., Mankoff J.","54794750000;57201449827;57194274303;57201451844;24587910500;7101701731;57203175552;","Leveraging human routine models to detect and generate human behaviors",2017,"Conference on Human Factors in Computing Systems - Proceedings","2017-May",,,"6683","6694",,5,"10.1145/3025453.3025571","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019650891&doi=10.1145%2f3025453.3025571&partnerID=40&md5=3f0a221d2e13a150ca49cd103e362bac","Human-Computer Interaction Institute, CMU, Pittsburgh, PA  15213, United States; Computer Science, Middlebury College, Middlebury, VA  05753, United States","Banovic, N., Human-Computer Interaction Institute, CMU, Pittsburgh, PA  15213, United States; Wang, A., Human-Computer Interaction Institute, CMU, Pittsburgh, PA  15213, United States; Jin, Y., Computer Science, Middlebury College, Middlebury, VA  05753, United States; Chang, C., Human-Computer Interaction Institute, CMU, Pittsburgh, PA  15213, United States; Ramos, J., Human-Computer Interaction Institute, CMU, Pittsburgh, PA  15213, United States; Dey, A.K., Human-Computer Interaction Institute, CMU, Pittsburgh, PA  15213, United States; Mankoff, J., Human-Computer Interaction Institute, CMU, Pittsburgh, PA  15213, United States","Inverse reinforcement learning; Maximum entropy","Human engineering; Maximum entropy methods; Reinforcement learning; Supervised learning; Aggressive driving behaviors; Data labeling; Domain experts; Human behaviors; Inverse reinforcement learning; Supervised machine learning; Wellbeing; Behavioral research",,Conference Paper,"Final",,Scopus,2-s2.0-85019650891
"Daniele A.F., Bansal M., Walter M.R.","57194715661;16466939600;15063686700;","Navigational Instruction Generation as Inverse Reinforcement Learning with Neural Machine Translation",2017,"ACM/IEEE International Conference on Human-Robot Interaction","Part F127194",,,"109","118",,6,"10.1145/2909824.3020241","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021795561&doi=10.1145%2f2909824.3020241&partnerID=40&md5=3e5ff2ad7553420b7728ab511e9e4f2f","TTI-Chicago, United States; UNC Chapel Hill, United States","Daniele, A.F., TTI-Chicago, United States; Bansal, M., UNC Chapel Hill, United States; Walter, M.R., TTI-Chicago, United States","human-robot interaction; natural language generation; selective generation","Education; Information dissemination; Inverse problems; Man machine systems; Natural language processing systems; Reinforcement learning; Robots; Translation (languages); Human demonstrations; Instruction generations; Inverse reinforcement learning; Machine translations; Natural language generation; Natural languages; Robotics applications; Selective generation; Human robot interaction",,Conference Paper,"Final",,Scopus,2-s2.0-85021795561
"Nouri E., Georgila K., Traum D.","54938669800;14017952500;6603622906;","Culture-specific models of negotiation for virtual characters: multi-attribute decision-making based on culture-specific values",2017,"AI and Society","32","1",,"51","63",,3,"10.1007/s00146-014-0570-7","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85011661373&doi=10.1007%2fs00146-014-0570-7&partnerID=40&md5=f509e501fa5bb2d27e24cd20a62ee4a3","Institute for Creative Technologies, University of Southern California, 12015 Waterfront Drive, Playa Vista, CA  90094, United States","Nouri, E., Institute for Creative Technologies, University of Southern California, 12015 Waterfront Drive, Playa Vista, CA  90094, United States; Georgila, K., Institute for Creative Technologies, University of Southern California, 12015 Waterfront Drive, Playa Vista, CA  90094, United States; Traum, D., Institute for Creative Technologies, University of Southern California, 12015 Waterfront Drive, Playa Vista, CA  90094, United States","Cultural decision-making; Inverse reinforcement learning; Negotiation; Ultimatum Game; Virtual agents","Behavioral research; Reinforcement learning; Virtual reality; Inverse reinforcement learning; Multi attribute decision making; Negotiation; Specific values; Ultimatum game; Virtual agent; Virtual character; Virtual humans; Decision making","Nouri, E.; Institute for Creative Technologies, University of Southern California, 12015 Waterfront Drive, United States; email: nouri@ict.usc.edu",Article,"Final",,Scopus,2-s2.0-85011661373
"Previtali F., Bordallo A., Iocchi L., Ramamoorthy S.","56210921700;57118029200;6602922345;15042865300;","Predicting future agent motions for dynamic environments",2017,"Proceedings - 2016 15th IEEE International Conference on Machine Learning and Applications, ICMLA 2016",,, 7838128,"94","99",,,"10.1109/ICMLA.2016.137","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85015386109&doi=10.1109%2fICMLA.2016.137&partnerID=40&md5=f326d38c25b2ecfc55e5653a135b63da","Sapienza University of Rome, Italy; University of Edinburgh, United Kingdom","Previtali, F., Sapienza University of Rome, Italy; Bordallo, A., University of Edinburgh, United Kingdom; Iocchi, L., Sapienza University of Rome, Italy; Ramamoorthy, S., University of Edinburgh, United Kingdom",,"Artificial intelligence; Behavioral research; Forecasting; Learning algorithms; Learning systems; Markov processes; Reinforcement learning; Semantics; Computational ability; Emerging applications; Empirical experiments; Inverse reinforcement learning; Markov Decision Processes; Multi resolution representation; Multi-camera tracking; State-of-the-art methods; Inverse problems",,Conference Paper,"Final",,Scopus,2-s2.0-85015386109
"Saitake R., Arai S.","57193403941;14057611500;","Parameter estimation of multi-objeetive reinforeement learning to reaeh arbitrary pareto solution",2017,"Proceedings - 2016 International Conference on Agents, ICA 2016",,, 7812982,"110","111",,1,"10.1109/ICA.2016.17","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85013679345&doi=10.1109%2fICA.2016.17&partnerID=40&md5=ea654622395bc27b538594f14000dd3e","Graduate School of Engineering, Chiba University, Chiba, Japan","Saitake, R., Graduate School of Engineering, Chiba University, Chiba, Japan; Arai, S., Graduate School of Engineering, Chiba University, Chiba, Japan","Inverse problem; Multi-Objective Reinforcement Learning; Parameter estimation","Inverse problems; Optimal systems; Pareto principle; Reinforcement learning; Apprenticeship learning; Bench-mark problems; Computational costs; Inverse reinforcement learning; Multi objective; Optimal sequence; Pareto optimal solutions; Pareto solution; Parameter estimation",,Conference Paper,"Final",,Scopus,2-s2.0-85013679345
"Shahryari S., Doshi P.","57189354278;23008336000;","Inverse reinforcement learning under noisy observations",2017,"Proceedings of the International Joint Conference on Autonomous Agents and Multiagent Systems, AAMAS","3",,,"1733","1735",,2,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046423089&partnerID=40&md5=ffa79201abab0b43941dd7e29469ccc8","Institute for Artificial Intelligence, University of Georgia, Athens, GA  30602, United States; THINC Lab, Dept. Of Computer Science, University of Georgia, Athens, GA  30602, United States","Shahryari, S., Institute for Artificial Intelligence, University of Georgia, Athens, GA  30602, United States; Doshi, P., THINC Lab, Dept. Of Computer Science, University of Georgia, Athens, GA  30602, United States",,"Autonomous agents; Inverse problems; Maximum principle; Multi agent systems; Trajectories; Expectation - maximizations; Inverse reinforcement learning; Noisy observations; Observation model; Reinforcement learning",,Conference Paper,"Final",,Scopus,2-s2.0-85046423089
"Zouzou A., Bouhoute A., Boubouh K., El Kamili M., Berrada I.","57200511399;56321351000;57200511274;14041412100;8963275500;","Predicting lane change maneuvers using Inverse Reinforcement Learning",2017,"Proceedings - 2017 International Conference on Wireless Networks and Mobile Communications, WINCOM 2017",,, 8238204,"","",,,"10.1109/WINCOM.2017.8238204","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041448992&doi=10.1109%2fWINCOM.2017.8238204&partnerID=40&md5=61f20a10697b55a2872eb32b214bddd7","LIMS Laboratory, Faculty of Sciences Dhar Mahraz, Université Sidi Mohammed Ben Abdellah - Fez, Morocco","Zouzou, A., LIMS Laboratory, Faculty of Sciences Dhar Mahraz, Université Sidi Mohammed Ben Abdellah - Fez, Morocco; Bouhoute, A., LIMS Laboratory, Faculty of Sciences Dhar Mahraz, Université Sidi Mohammed Ben Abdellah - Fez, Morocco; Boubouh, K., LIMS Laboratory, Faculty of Sciences Dhar Mahraz, Université Sidi Mohammed Ben Abdellah - Fez, Morocco; El Kamili, M., LIMS Laboratory, Faculty of Sciences Dhar Mahraz, Université Sidi Mohammed Ben Abdellah - Fez, Morocco; Berrada, I., LIMS Laboratory, Faculty of Sciences Dhar Mahraz, Université Sidi Mohammed Ben Abdellah - Fez, Morocco","Inverse reinforcement learning; Machine learning; Markov decision process; Maximum causal entropy","Behavioral research; Data visualization; Entropy; Inverse problems; Learning algorithms; Learning systems; Markov processes; Mobile telecommunication systems; Wireless networks; Human behaviors; Inverse reinforcement learning; Lane change; Lane change maneuvers; Markov Decision Processes; Open-source solutions; Plug-ins; Reinforcement learning",,Conference Paper,"Final",,Scopus,2-s2.0-85041448992
"Šošic A., KhudaBukhsh W.R., Zoubir A.M., Koeppl H.","55416190200;57190680542;35584414100;6603491586;","Inverse reinforcement learning in swarm systems",2017,"Proceedings of the International Joint Conference on Autonomous Agents and Multiagent Systems, AAMAS","3",,,"1413","1420",,6,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040758666&partnerID=40&md5=5c6d8eb1951628959f8c41370a525801","Department of Electrical Engineering and Information Technology, Technische Universität, Darmstadt, Germany","Šošic, A., Department of Electrical Engineering and Information Technology, Technische Universität, Darmstadt, Germany; KhudaBukhsh, W.R., Department of Electrical Engineering and Information Technology, Technische Universität, Darmstadt, Germany; Zoubir, A.M., Department of Electrical Engineering and Information Technology, Technische Universität, Darmstadt, Germany; Koeppl, H., Department of Electrical Engineering and Information Technology, Technische Universität, Darmstadt, Germany","Inverse reinforcement learning; Multi-agent systems; Swarms","Autonomous agents; Inverse problems; Markov processes; Reinforcement learning; Swarm intelligence; Behavioral model; Control problems; Inverse reinforcement learning; Large-scale problem; Learning schemes; Partially observable Markov decision process; Swarming behavior; Swarms; Multi agent systems",,Conference Paper,"Final",,Scopus,2-s2.0-85040758666
"Amin K., Jiang N., Singh S.","51461010600;56421285700;55548164600;","Repeated inverse reinforcement learning",2017,"Advances in Neural Information Processing Systems","2017-December",,,"1816","1825",,10,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046996869&partnerID=40&md5=7b1b60c8ae6a41e7d0d84be20f126e6f","Google Research, New York, NY  10011, United States; Computer Science and Engineering, University of Michigan, Ann Arbor, MI  48104, United States","Amin, K., Google Research, New York, NY  10011, United States; Jiang, N., Computer Science and Engineering, University of Michigan, Ann Arbor, MI  48104, United States; Singh, S., Computer Science and Engineering, University of Michigan, Ann Arbor, MI  48104, United States",,"Behavioral research; Reinforcement learning; Inverse reinforcement learning; Inverse problems",,Conference Paper,"Final",,Scopus,2-s2.0-85046996869
"Prasad V., Jangir R., Balaraman R., Krishna K.M.","57201912364;57201903033;57201895562;7102445320;","Data driven strategies for active monocular SLAM using inverse reinforcement learning",2017,"Proceedings of the International Joint Conference on Autonomous Agents and Multiagent Systems, AAMAS","3",,,"1697","1699",,,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046472099&partnerID=40&md5=f5a535ebd0a8f43ffb1c0b7f05fd4a44","Robotics Research Centre, International Institute of Information Technology, Hyderabad, India; Department of Physics, Indian Institute of Technology Guwahati, Guwahati, India; Department of Computer Science, Indian Institute of Technology Madras, Madras, India","Prasad, V., Robotics Research Centre, International Institute of Information Technology, Hyderabad, India; Jangir, R., Department of Physics, Indian Institute of Technology Guwahati, Guwahati, India; Balaraman, R., Department of Computer Science, Indian Institute of Technology Madras, Madras, India; Krishna, K.M., Robotics Research Centre, International Institute of Information Technology, Hyderabad, India","Active monocular SLAM; Inverse reinforcement learning","Autonomous agents; Inverse problems; Markov processes; Multi agent systems; Vision; Complex task; Computational model; Data driven; Inverse reinforcement learning; Markov Decision Processes; Monocular SLAM; Reward function; State-of-the-art methods; Reinforcement learning",,Conference Paper,"Final",,Scopus,2-s2.0-85046472099
"Bogert K., Doshi P.","36095803300;23008336000;","Scaling expectation-maximization for inverse reinforcement learning to multiple robots under occlusion",2017,"Proceedings of the International Joint Conference on Autonomous Agents and Multiagent Systems, AAMAS","1",,,"522","529",,1,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046430383&partnerID=40&md5=36cf821fe8e0fa8c124e848462cf729e","Department of Computer Science, University of North Carolina, Asheville, NC  28804, United States; Dept. of Computer Science, THINC Lab, University of Georgia, Athens, GA  30602, United States","Bogert, K., Department of Computer Science, University of North Carolina, Asheville, NC  28804, United States; Doshi, P., Dept. of Computer Science, THINC Lab, University of Georgia, Athens, GA  30602, United States",,"Industrial robots; Inverse problems; Maximum principle; Multi agent systems; Reinforcement learning; Trajectories; Close proximity; Conditional expectation; Expectation - maximizations; Gibbs sampling; Inverse reinforcement learning; Multi-robot domains; Multiple agents; Multiple robot; Autonomous agents",,Conference Paper,"Final",,Scopus,2-s2.0-85046430383
"Mobley D.","57196122373;","Multi-agent systems of inverse reinforcement learners in complex games",2017,"IJCAI International Joint Conference on Artificial Intelligence",,,,"5191","5192",,,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031923562&partnerID=40&md5=e5a55f7f4d68ef69e6aa9c66310ba5f4","University of Kentucky, United States","Mobley, D., University of Kentucky, United States",,"Artificial intelligence; Intelligent agents; Inverse problems; Reinforcement learning; Core problems; Inverse reinforcement learning; Task modeling; Task-based; Multi agent systems","Mobley, D.; University of KentuckyUnited States; email: dave.mobley@uky.edu",Conference Paper,"Final",,Scopus,2-s2.0-85031923562
"Pulido J.V., Fields M., Barnes L.","57202675123;22957625600;7103077338;","On-the-fly learning and monitoring of partially observed navigation plan",2017,"Proceedings of the International Joint Conference on Autonomous Agents and Multiagent Systems, AAMAS","3",,,"1700","1702",,1,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046480828&partnerID=40&md5=d4ac25ddd1220265f0f10d02757c5d38","University of Virginia, 151 Engineer's Way, Charlottesville, VA, United States; U.S. Army Research Laboratory, 4727 Deer Creek Loop, Aberdeen Proving Grounds, MD, United States","Pulido, J.V., University of Virginia, 151 Engineer's Way, Charlottesville, VA, United States; Fields, M., U.S. Army Research Laboratory, 4727 Deer Creek Loop, Aberdeen Proving Grounds, MD, United States; Barnes, L., University of Virginia, 151 Engineer's Way, Charlottesville, VA, United States","Intent recognition; Inverse reinforcement learning","Autonomous agents; Inverse problems; Multi agent systems; Navigation; Robots; Intent recognition; Inverse reinforcement learning; Maximum margin; Navigation model; On the flies; Predictive abilities; Real time; Reinforcement learning",,Conference Paper,"Final",,Scopus,2-s2.0-85046480828
"Everitt T., Krakovna V., Orseau L., Legg S.","56404552200;57191328116;36146416600;12242205800;","Reinforcement learning with a corrupted reward channel",2017,"IJCAI International Joint Conference on Artificial Intelligence",,,,"4705","4713",,7,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031900858&partnerID=40&md5=f1539c72e01b8653c50b20361c24581f","Australian National University, Australia; DeepMind, Australia","Everitt, T., Australian National University, Australia; Krakovna, V., DeepMind, Australia; Orseau, L., DeepMind, Australia; Legg, S., DeepMind, Australia",,"Artificial intelligence; Crime; Errors; Program debugging; Software agents; Systematic errors; Inverse reinforcement learning; Markov decision problem; Optimisations; Randomisation; Reinforcement learning agent; Reward function; Semi-supervised; Simplifying assumptions; Reinforcement learning",,Conference Paper,"Final",,Scopus,2-s2.0-85031900858
"Metelli A.M., Pirotta M., Restelli M.","57195947711;49362257000;6603404086;","Compatible reward inverse reinforcement learning",2017,"Advances in Neural Information Processing Systems","2017-December",,,"2051","2060",,,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047016144&partnerID=40&md5=8ab01ef18feb66d08f8c8a4f1491129a","DEIB, Politecnico di Milano, Italy; SequeL Team, Inria, Lille, France","Metelli, A.M., DEIB, Politecnico di Milano, Italy; Pirotta, M., SequeL Team, Inria, Lille, France; Restelli, M., DEIB, Politecnico di Milano, Italy",,"Inverse problems; Taxicabs; Basis functions; Effective approaches; Function spaces; Inverse reinforcement learning; Linear quadratic Gaussian; Optimal policies; Policy gradient; Reward function; Reinforcement learning",,Conference Paper,"Final",,Scopus,2-s2.0-85047016144
"Arnold T., Kasenberg D., Scheutz M.","56895720600;57201556232;6603548841;","Value alignment or misalignment - What will keep systems accountable?",2017,"AAAI Workshop - Technical Report","WS-17-01 - WS-17-15",,,"81","88",,17,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045276127&partnerID=40&md5=fa6c749c157a02113fb2891967a225da","Department of Computer Science, Tufts University, Medford, MA  02155, United States","Arnold, T., Department of Computer Science, Tufts University, Medford, MA  02155, United States; Kasenberg, D., Department of Computer Science, Tufts University, Medford, MA  02155, United States; Scheutz, M., Department of Computer Science, Tufts University, Medford, MA  02155, United States",,"Alignment; Computer games; Deep learning; Inverse problems; Knowledge based systems; Operations research; Philosophical aspects; Reinforcement learning; Computational architecture; Counterfactuals; Hybrid approach; Inverse reinforcement learning; Temporal dimensions; Problem solving",,Conference Paper,"Final",,Scopus,2-s2.0-85045276127
"Kohjima M., Matsubayashi T., Sawada H.","56925050300;55111598300;7401969520;","Generalized Inverse Reinforcement Learning with Linearly Solvable MDP",2017,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","10535 LNAI",,,"373","388",,,"10.1007/978-3-319-71246-8_23","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040255772&doi=10.1007%2f978-3-319-71246-8_23&partnerID=40&md5=7dfcbb708253715da7764a6deb6081ff","NTT Service Evolution Laboratories, NTT Corporation, Kanagawa, Japan","Kohjima, M., NTT Service Evolution Laboratories, NTT Corporation, Kanagawa, Japan; Matsubayashi, T., NTT Service Evolution Laboratories, NTT Corporation, Kanagawa, Japan; Sawada, H., NTT Service Evolution Laboratories, NTT Corporation, Kanagawa, Japan","Bayesian method; Inverse reinforcement learning; Linearly solvable MDP","Artificial intelligence; Bayesian networks; Cost benefit analysis; Cost estimating; Cost functions; Costs; Inverse problems; Learning algorithms; Learning systems; Markov processes; Bayesian methods; Generalized inverse; Inverse reinforcement learning; Linearly solvable MDP; Markov Decision Processes; Synthetic data; Theoretical study; Transition probabilities; Reinforcement learning","Kohjima, M.; NTT Service Evolution Laboratories, NTT CorporationJapan; email: kohjima.masahiro@lab.ntt.co.jp",Conference Paper,"Final",,Scopus,2-s2.0-85040255772
"Lin Z., Gehring J., Khalidov V., Synnaeve G.","57208446046;57210079450;25927242400;36987111200;","STARDATA: A starcraft AI research dataset",2017,"Proceedings of the 13th AAAI Conference on Artificial Intelligence and Interactive Digital Entertainment, AIIDE 2017",,,,"50","56",,6,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056824016&partnerID=40&md5=45f6eb4d7b474016a53c3a5579d93e18","Facebook, 770 Broadway, New York, NY  10003, United States; Facebook, 6, rue Ménars, Paris, 75002, France","Lin, Z., Facebook, 770 Broadway, New York, NY  10003, United States; Gehring, J., Facebook, 6, rue Ménars, Paris, 75002, France; Khalidov, V., Facebook, 6, rue Ménars, Paris, 75002, France; Synnaeve, G., Facebook, 770 Broadway, New York, NY  10003, United States",,"Data mining; Human computer interaction; Inverse problems; Machine learning; Reinforcement learning; Forward modeling; Imitation learning; Inverse reinforcement learning; Player action; Classification (of information)",,Conference Paper,"Final",,Scopus,2-s2.0-85056824016
"Santos E., Jr., Nguyen H., Russell J., Kim K., Veenhuis L., Boparai R., Stautland T.K.","23980706000;55667736200;7404208804;14619319600;57195074097;57195071094;57195071293;","Capturing a Commander's decision making style",2017,"Proceedings of SPIE - The International Society for Optical Engineering","10184",, 1018412,"","",,1,"10.1117/12.2268452","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85025141788&doi=10.1117%2f12.2268452&partnerID=40&md5=9dfa7b28398ca54dc72712b49e15f7ba","Thayer Engineering School, Dartmouth College, 8000 Cummings Hall, Hanover, NH  03755, United States; Dept. of Computer Science, UW-Whitewater, 800 W. Main Street, Whitewater, WI  53190, United States","Santos, E., Jr., Thayer Engineering School, Dartmouth College, 8000 Cummings Hall, Hanover, NH  03755, United States; Nguyen, H., Dept. of Computer Science, UW-Whitewater, 800 W. Main Street, Whitewater, WI  53190, United States; Russell, J., Thayer Engineering School, Dartmouth College, 8000 Cummings Hall, Hanover, NH  03755, United States; Kim, K., Thayer Engineering School, Dartmouth College, 8000 Cummings Hall, Hanover, NH  03755, United States; Veenhuis, L., Dept. of Computer Science, UW-Whitewater, 800 W. Main Street, Whitewater, WI  53190, United States; Boparai, R., Dept. of Computer Science, UW-Whitewater, 800 W. Main Street, Whitewater, WI  53190, United States; Stautland, T.K., Dept. of Computer Science, UW-Whitewater, 800 W. Main Street, Whitewater, WI  53190, United States","cognitive states; Decision making process; Double Transition Model; inverse optimal control; inverse reinforcement learning","Education; Law enforcement; National security; Naval warfare; Network security; Reinforcement learning; Security systems; Cognitive state; Decision making process; Inverse reinforcement learning; Inverse-optimal control; Transition model; Decision making",,Conference Paper,"Final",,Scopus,2-s2.0-85025141788
[No author name available],[No author id available],"Uncertainty in Artificial Intelligence - Proceedings of the 33rd Conference, UAI 2017",2017,"Uncertainty in Artificial Intelligence - Proceedings of the 33rd Conference, UAI 2017",,,,"","",1203,,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031106481&partnerID=40&md5=057f494148c6ed759a4911fd8054c479",,"",,,,Conference Review,"Final",,Scopus,2-s2.0-85031106481
"Uchida S., Oba S., Ishii S.","57197729458;7007003360;7403110142;","Estimation of the change of agents behavior strategy using state-action history",2017,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","10614 LNCS",,,"100","107",,,"10.1007/978-3-319-68612-7_12","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85034220134&doi=10.1007%2f978-3-319-68612-7_12&partnerID=40&md5=ae13d12fb643e3b22489e93958a5b34b","Kyoto University, Yoshidahonmachi 36-1, Sakyo-ku, Kyoto-city, Japan; ATR Cognitive Mechanism Laboratories, Kyoto, Japan","Uchida, S., Kyoto University, Yoshidahonmachi 36-1, Sakyo-ku, Kyoto-city, Japan; Oba, S., Kyoto University, Yoshidahonmachi 36-1, Sakyo-ku, Kyoto-city, Japan; Ishii, S., Kyoto University, Yoshidahonmachi 36-1, Sakyo-ku, Kyoto-city, Japan, ATR Cognitive Mechanism Laboratories, Kyoto, Japan","Behavior strategy; Change point; Inverse reinforcement learning; Maximum likelihood estimation; Reinforcement learning","Intelligent agents; Inverse problems; Learning algorithms; Learning systems; Maximum likelihood; Maximum likelihood estimation; Neural networks; Behavior strategy; Change-points; Computational model; Current situation; Environmental uncertainty; Inverse reinforcement learning; Stationary policy; Uncertain environments; Reinforcement learning","Uchida, S.; Kyoto University, Yoshidahonmachi 36-1, Sakyo-ku, Japan; email: uchida-s@sys.i.kyoto-u.ac.jp",Conference Paper,"Final",,Scopus,2-s2.0-85034220134
"Huang S.H., Held D., Abbeel P., Dragan A.D.","56422734600;35955893400;8269962600;55193779100;","Enabling robots to communicate their objectives",2017,"Robotics: Science and Systems","13",,,"","",,2,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048814981&partnerID=40&md5=0dd2774a9519120c230b491ea66ecd37","University of California, EECS, Berkeley, CA, United States; OpenAI, United States","Huang, S.H., University of California, EECS, Berkeley, CA, United States; Held, D., University of California, EECS, Berkeley, CA, United States; Abbeel, P., University of California, EECS, Berkeley, CA, United States, OpenAI, United States; Dragan, A.D., University of California, EECS, Berkeley, CA, United States",,"Cognitive systems; Inverse problems; Reinforcement learning; Robotics; Approximate inference; Autonomous driving; Human learning; Inverse reinforcement learning; Mental model; Objective functions; Robot behavior; Robot model; Robots",,Conference Paper,"Final",,Scopus,2-s2.0-85048814981
"Rzepka R., Araki K.","6603550196;7402551308;","What people say? Web-based casuistry for artificial morality experiments",2017,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","10414 LNAI",,,"178","187",,,"10.1007/978-3-319-63703-7_17","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028467326&doi=10.1007%2f978-3-319-63703-7_17&partnerID=40&md5=29ea934e93eb208ddcac3d5b0d9e8e85","Graduate School of Information Science and Technology, Hokkaido University, Kita 14 Nishi 9, Kita-ku, Sapporo, 060-0814, Japan","Rzepka, R., Graduate School of Information Science and Technology, Hokkaido University, Kita 14 Nishi 9, Kita-ku, Sapporo, 060-0814, Japan; Araki, K., Graduate School of Information Science and Technology, Hokkaido University, Kita 14 Nishi 9, Kita-ku, Sapporo, 060-0814, Japan",,"Inverse problems; Philosophical aspects; Reinforcement learning; Affect recognition; Cognitive bias; Decision-based; Human subjects; Human values; Inverse reinforcement learning; Real world situations; Web mining systems; Autonomous agents","Rzepka, R.; Graduate School of Information Science and Technology, Hokkaido University, Kita 14 Nishi 9, Kita-ku, Japan; email: rzepka@ist.hokudai.ac.jp",Conference Paper,"Final",,Scopus,2-s2.0-85028467326
"González D.S., Dibangoye J.S., Laugier C.","56382000700;25640845700;7007031683;","High-speed highway scene prediction based on driver models learned from demonstrations",2016,"IEEE Conference on Intelligent Transportation Systems, Proceedings, ITSC",,, 7795546,"149","155",,10,"10.1109/ITSC.2016.7795546","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85010042392&doi=10.1109%2fITSC.2016.7795546&partnerID=40&md5=da85968a3efa84f73c5d5bcefc155c8c","INRIA Grenoble Rhone-Alpes, Montbonnot- Saint-Martin, 38330, France","González, D.S., INRIA Grenoble Rhone-Alpes, Montbonnot- Saint-Martin, 38330, France; Dibangoye, J.S., INRIA Grenoble Rhone-Alpes, Montbonnot- Saint-Martin, 38330, France; Laugier, C., INRIA Grenoble Rhone-Alpes, Montbonnot- Saint-Martin, 38330, France",,"Collision avoidance; Cost functions; Highway traffic control; Intelligent systems; Intelligent vehicle highway systems; Long Term Evolution (LTE); Markov processes; Reinforcement learning; Remotely operated vehicles; Transportation; Wireless telecommunication systems; Dynamic environments; Dynamic obstacles; Inverse reinforcement learning; Linear combinations; Markov Decision Processes; Number of vehicles; Prediction-based; Semi-autonomous vehicles; Forecasting",,Conference Paper,"Final",,Scopus,2-s2.0-85010042392
"Hwang K.-S., Chiang H.-Y., Jiang W.-C.","7402426737;57192553650;48361586500;","Adaboost-like method for inverse reinforcement learning",2016,"2016 IEEE International Conference on Fuzzy Systems, FUZZ-IEEE 2016",,, 7737926,"1922","1925",,2,"10.1109/FUZZ-IEEE.2016.7737926","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85006782990&doi=10.1109%2fFUZZ-IEEE.2016.7737926&partnerID=40&md5=8eb4f4357f872f0b296f65f1fd916279","Department of Electrical Engineering, National Sun Yat-sen University, Kaohsiung, Taiwan","Hwang, K.-S., Department of Electrical Engineering, National Sun Yat-sen University, Kaohsiung, Taiwan; Chiang, H.-Y., Department of Electrical Engineering, National Sun Yat-sen University, Kaohsiung, Taiwan; Jiang, W.-C., Department of Electrical Engineering, National Sun Yat-sen University, Kaohsiung, Taiwan","Adaboost classifier; Inverse reinforcement learning; Reinforcement learning; Upper confidence bounds(UCB)","Adaptive boosting; Behavioral research; Fuzzy systems; Inverse problems; Ada boost classifiers; Computation time; Intelligent behavior; Inverse reinforcement learning; Learning tasks; Reward function; Trial-and-error method; Upper confidence bound; Reinforcement learning",,Conference Paper,"Final",,Scopus,2-s2.0-85006782990
"Xia C., El Kamel A.","57191588472;7003630805;","Neural inverse reinforcement learning in autonomous navigation",2016,"Robotics and Autonomous Systems","84",,,"1","14",,14,"10.1016/j.robot.2016.06.003","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84991678526&doi=10.1016%2fj.robot.2016.06.003&partnerID=40&md5=e2060b765006061c8588137dc6ff5d06","Research Center CRIStAL, UMR CNRS 9189, École Centrale de Lille, Villeneuve d'Ascq, 59651, France","Xia, C., Research Center CRIStAL, UMR CNRS 9189, École Centrale de Lille, Villeneuve d'Ascq, 59651, France; El Kamel, A., Research Center CRIStAL, UMR CNRS 9189, École Centrale de Lille, Villeneuve d'Ascq, 59651, France","Autonomous navigation; Dynamic environments; Inverse reinforcement learning; Learning from demonstration; Markov decision processes; Neural network","Air navigation; Demonstrations; Intelligent robots; Markov processes; Mobile robots; Navigation; Navigation systems; Neural networks; Robotics; Robots; Stochastic systems; Autonomous navigation; Dynamic environments; Inverse reinforcement learning; Learning from demonstration; Markov Decision Processes; Reinforcement learning","Xia, C.; Research Center CRIStAL, UMR CNRS 9189, École Centrale de LilleFrance; email: chen.xia@centraliens-lille.org",Article,"Final",,Scopus,2-s2.0-84991678526
"Gu T., Dolan J.M., Lee J.-W.","56001409400;56045990600;56237914700;","Human-like planning of swerve maneuvers for autonomous vehicles",2016,"IEEE Intelligent Vehicles Symposium, Proceedings","2016-August",, 7535466,"716","721",,6,"10.1109/IVS.2016.7535466","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84983242921&doi=10.1109%2fIVS.2016.7535466&partnerID=40&md5=cefe5da7b3f916d65477906bac576367","Department of Electrical and Computer Engineering, Carnegie Mellon University, Pittsburgh, PA, United States; Robotics Institute, Carnegie Mellon University, Pittsburgh, PA, United States; Research and Development, General Motors, Warren, MI, United States","Gu, T., Department of Electrical and Computer Engineering, Carnegie Mellon University, Pittsburgh, PA, United States; Dolan, J.M., Robotics Institute, Carnegie Mellon University, Pittsburgh, PA, United States; Lee, J.-W., Research and Development, General Motors, Warren, MI, United States",,"Behavioral research; Intelligent vehicle highway systems; Reinforcement learning; Stochastic models; Stochastic systems; Autonomous Vehicles; Driving styles; Inverse reinforcement learning; Motion planners; Parameter-tuning; Sampling-based; Smooth trajectories; Stochastic nature; Vehicles",,Conference Paper,"Final",,Scopus,2-s2.0-84983242921
"Arora S., Tanner H.G.","57213211071;7007108972;","Programming by demonstration for Locally k-Testable tasks",2016,"24th Mediterranean Conference on Control and Automation, MED 2016",,, 7535995,"1146","1151",,,"10.1109/MED.2016.7535995","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84986183429&doi=10.1109%2fMED.2016.7535995&partnerID=40&md5=1d7b0dc34e9530f06879646585d3e99c","Department of Mechanical Engineering, University of Delaware, Newark, DE, United States","Arora, S., Department of Mechanical Engineering, University of Delaware, Newark, DE, United States; Tanner, H.G., Department of Mechanical Engineering, University of Delaware, Newark, DE, United States","grammatical inference; imitation learning; Learning by demonstration","Inverse problems; Markov processes; Natural language processing systems; Reinforcement learning; Grammatical inferences; Imitation learning; Inverse reinforcement learning; Language identification; Learning by demonstration; Markov Decision Processes; Predicate abstractions; Programming by demon-stration; Demonstrations",,Conference Paper,"Final",,Scopus,2-s2.0-84986183429
"El-Hussieny H., Assal S.F.M., Abouelsoud A.A., Megahed S.M., Ogasawara T.","20433398600;8571391800;6601958127;6701420832;7201579979;","Incremental learning of reach-to-grasp behavior: A PSO-based Inverse optimal control approach",2016,"Proceedings of the 2015 7th International Conference of Soft Computing and Pattern Recognition, SoCPaR 2015",,, 7492796,"129","135",,2,"10.1109/SOCPAR.2015.7492796","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84979300847&doi=10.1109%2fSOCPAR.2015.7492796&partnerID=40&md5=81892d7cc42bf2b05c4128040dda53ae","Mechatronics and Robotics Engineering Department, School of Innovative Design Engineering, Egypt-Japan University of Science and Technology (E-JUST), Egypt; Nara Institute of Science and Technology, 8916-5, Takayama, Ikoma, Nara, 630-0192, Japan; Department of Production Engineering and Mechanical Design, Faculty of Engineering, Tanta University, Egypt; Electronics and Communications Eng. Dept., Faculty of Engineering, Cairo University, Egypt; Mechanical Design and Production Engineering Department, Faculty of Engineering, Cairo University, Egypt","El-Hussieny, H., Mechatronics and Robotics Engineering Department, School of Innovative Design Engineering, Egypt-Japan University of Science and Technology (E-JUST), Egypt, Nara Institute of Science and Technology, 8916-5, Takayama, Ikoma, Nara, 630-0192, Japan; Assal, S.F.M., Mechatronics and Robotics Engineering Department, School of Innovative Design Engineering, Egypt-Japan University of Science and Technology (E-JUST), Egypt, Department of Production Engineering and Mechanical Design, Faculty of Engineering, Tanta University, Egypt; Abouelsoud, A.A., Mechatronics and Robotics Engineering Department, School of Innovative Design Engineering, Egypt-Japan University of Science and Technology (E-JUST), Egypt, Electronics and Communications Eng. Dept., Faculty of Engineering, Cairo University, Egypt; Megahed, S.M., Mechatronics and Robotics Engineering Department, School of Innovative Design Engineering, Egypt-Japan University of Science and Technology (E-JUST), Egypt, Mechanical Design and Production Engineering Department, Faculty of Engineering, Cairo University, Egypt; Ogasawara, T., Nara Institute of Science and Technology, 8916-5, Takayama, Ikoma, Nara, 630-0192, Japan","Incremental learning; Inverse reinforcement learning; Linear Quadratic Regulator; Particle Swarm Optimization; Reach-to-grasp modeling","Computation theory; Cost functions; Costs; Evolutionary algorithms; Optimization; Particle swarm optimization (PSO); Pattern recognition; Reinforcement learning; Soft computing; Evolutionary Optimization Techniques; Incremental learning; Inverse reinforcement learning; Linear dynamic equations; Linear quadratic regulator; Neuroscience literature; Reach to grasp; Reach-to-grasp movements; Inverse problems",,Conference Paper,"Final",,Scopus,2-s2.0-84979300847
"Tegelund B., Son H., Lee D.","57188668395;55428430500;55645739400;","A task-oriented service personalization scheme for smart environments using reinforcement learning",2016,"2016 IEEE International Conference on Pervasive Computing and Communication Workshops, PerCom Workshops 2016",,, 7457110,"","",,4,"10.1109/PERCOMW.2016.7457110","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84966666615&doi=10.1109%2fPERCOMW.2016.7457110&partnerID=40&md5=baf5e68163fa278f820db16929ce294f","School of Computer Science, KAIST, Daejeon, South Korea","Tegelund, B., School of Computer Science, KAIST, Daejeon, South Korea; Son, H., School of Computer Science, KAIST, Daejeon, South Korea; Lee, D., School of Computer Science, KAIST, Daejeon, South Korea",,"Inverse problems; Optimization; Ubiquitous computing; Inverse reinforcement learning; Optimization problems; Service personalization; Smart environment; Task-oriented; User's preferences; Reinforcement learning",,Conference Paper,"Final",,Scopus,2-s2.0-84966666615
"Herman M., Gindele T., Wagner J., Schmitt F., Burgard W.","56742755300;24476503400;57191835760;57190805464;7003610380;","Inverse reinforcement learning with simultaneous estimation of rewards and dynamics",2016,"Proceedings of the 19th International Conference on Artificial Intelligence and Statistics, AISTATS 2016",,,,"102","110",,11,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052981199&partnerID=40&md5=4e1525d964d621ec5b67c295a6b86bb0","Robert Bosch GmbH, Stuttgart, D-70442, Germany; University of Freiburg, Freiburg, D-79110, Germany","Herman, M., Robert Bosch GmbH, Stuttgart, D-70442, Germany, University of Freiburg, Freiburg, D-79110, Germany; Gindele, T., Robert Bosch GmbH, Stuttgart, D-70442, Germany; Wagner, J., Robert Bosch GmbH, Stuttgart, D-70442, Germany; Schmitt, F., Robert Bosch GmbH, Stuttgart, D-70442, Germany; Burgard, W., University of Freiburg, Freiburg, D-79110, Germany",,"Machine learning; Markov processes; Petroleum reservoir evaluation; Problem solving; Reinforcement learning; Stochastic systems; Additional samples; Combined optimization problem; Inverse reinforcement learning; Markov Decision Processes; Reward function; Simultaneous estimation; Transfer learning; Transition model; Inverse problems",,Conference Paper,"Final",,Scopus,2-s2.0-85052981199
"Herman M., Gindele T., Wagner J., Schmitt F., Burgard W.","56742755300;24476503400;57191835760;57190805464;7003610380;","Simultaneous estimation of rewards and dynamics from noisy expert demonstrations",2016,"ESANN 2016 - 24th European Symposium on Artificial Neural Networks",,,,"677","682",,,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84994180781&partnerID=40&md5=38353fa4ecbdb0d2c533896ca6509b29","Robert Bosch GmbH, Stuttgart, 70442, Germany; University of Freiburg, Department of Computer Science, Freiburg, 79110, Germany","Herman, M., Robert Bosch GmbH, Stuttgart, 70442, Germany, University of Freiburg, Department of Computer Science, Freiburg, 79110, Germany; Gindele, T., Robert Bosch GmbH, Stuttgart, 70442, Germany; Wagner, J., Robert Bosch GmbH, Stuttgart, 70442, Germany; Schmitt, F., Robert Bosch GmbH, Stuttgart, 70442, Germany; Burgard, W., University of Freiburg, Department of Computer Science, Freiburg, 79110, Germany",,"Artificial intelligence; Demonstrations; Learning algorithms; Learning systems; Markov processes; Neural networks; Reinforcement learning; Stochastic systems; System theory; Gradient based; Inverse reinforcement learning; Markov Decision Processes; Reward function; Simultaneous estimation; State transitions; Stochastic policy; System Dynamics; Inverse problems",,Conference Paper,"Final",,Scopus,2-s2.0-84994180781
"Shiarlis K., Messias J., Whiteson S.","57193494529;55208968600;10240257400;","Inverse reinforcement learning from failure",2016,"Proceedings of the International Joint Conference on Autonomous Agents and Multiagent Systems, AAMAS",,,,"1060","1068",,10,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85014246612&partnerID=40&md5=37d2f2a842952f6d75f19704e22eb23d","Informatics Institute, University of Amsterdam, Science Park 904, Amsterdam, Netherlands; Dept. of Computer Science, University of Oxford, Wolfson Building, Parks Rd, Oxford, United Kingdom","Shiarlis, K., Informatics Institute, University of Amsterdam, Science Park 904, Amsterdam, Netherlands; Messias, J., Informatics Institute, University of Amsterdam, Science Park 904, Amsterdam, Netherlands; Whiteson, S., Dept. of Computer Science, University of Oxford, Wolfson Building, Parks Rd, Oxford, United Kingdom","Inverse reinforcement learning; Learning from demonstration; Machine learning; Robotics; Social navigation","Autonomous agents; Constrained optimization; Demonstrations; Entropy; Inverse problems; Learning systems; Multi agent systems; Robotics; Robots; Complex task; Inverse reinforcement learning; Learning from demonstration; Optimisations; Reward function; Social navigation; State of the art; Trial and error; Reinforcement learning",,Conference Paper,"Final",,Scopus,2-s2.0-85014246612
"Kim D., Kim D.-H., Moon I.-C.","57188761105;57198637806;56038773000;","Inverse modeling of combat behavior with virtual-constructive simulation training",2016,"Communications in Computer and Information Science","644",,,"597","606",,1,"10.1007/978-981-10-2666-9_60","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988805130&doi=10.1007%2f978-981-10-2666-9_60&partnerID=40&md5=54451e46706bf69be9352cdff810a42a","Department of Industrial and Systems Engineering, KAIST, Daejeon, 305701, South Korea","Kim, D., Department of Industrial and Systems Engineering, KAIST, Daejeon, 305701, South Korea; Kim, D.-H., Department of Industrial and Systems Engineering, KAIST, Daejeon, 305701, South Korea; Moon, I.-C., Department of Industrial and Systems Engineering, KAIST, Daejeon, 305701, South Korea",,"E-learning; Inverse problems; Reinforcement learning; Combat environments; Descriptive statistics; Inverse modeling; Inverse reinforcement learning; Simple modeling; Simulation applications; Simulation training; Synthetic environments; Virtual reality","Moon, I.-C.; Department of Industrial and Systems Engineering, KAISTSouth Korea; email: icmoon@kaist.ac.kr",Conference Paper,"Final",,Scopus,2-s2.0-84988805130
"El Asri L., Piot B., Geist M., Laroche R., Pietquin O.","55767861100;55697434100;25929145100;35179791100;16040586900;","Score-based inverse reinforcement learning",2016,"Proceedings of the International Joint Conference on Autonomous Agents and Multiagent Systems, AAMAS",,,,"457","465",,5,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85014225758&partnerID=40&md5=063b9b21bb00add08f128d9a5ad7b000","Orange Labs and Maluuba, Montréal, Canada; Univ. Lille, CNRS, Centrale Lille, Inria UMR 9189, CRIStAL, Lille, France; UMI 2958, Georgia Tech-CNRS, Centrale Supélec, Université Paris-Saclay, Metz, France; Orange Labs, Issy Les Moulineaux, France","El Asri, L., Orange Labs and Maluuba, Montréal, Canada; Piot, B., Univ. Lille, CNRS, Centrale Lille, Inria UMR 9189, CRIStAL, Lille, France; Geist, M., UMI 2958, Georgia Tech-CNRS, Centrale Supélec, Université Paris-Saclay, Metz, France; Laroche, R., Orange Labs, Issy Les Moulineaux, France; Pietquin, O., Univ. Lille, CNRS, Centrale Lille, Inria UMR 9189, CRIStAL, Lille, France","Inverse reinforcement learning; Learning from demonstration; Markov decision processes; Reinforcement learning; Spoken dialogue systems","Autonomous agents; Markov processes; Multi agent systems; Optimization; Speech processing; Trajectories; Inverse reinforcement learning; Learning from demonstration; Least square regression; Markov Decision Processes; Near-optimal policies; Spoken dialogue system; Standard setting; Theoretical guarantees; Reinforcement learning",,Conference Paper,"Final",,Scopus,2-s2.0-85014225758
"Sexton T., Ren M.Y.","57192815712;57194968344;","Learning human search strategies from a crowdsourcing game",2016,"Proceedings of the ASME Design Engineering Technical Conference","2A-2016",,,"","",,2,"10.1115/DETC2016-59775","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85008234786&doi=10.1115%2fDETC2016-59775&partnerID=40&md5=c030376dfde97fc88765512bdc42c5a5","Mechanical Engineering, Arizona State University, Tempe, AZ  85287, United States","Sexton, T., Mechanical Engineering, Arizona State University, Tempe, AZ  85287, United States; Ren, M.Y., Mechanical Engineering, Arizona State University, Tempe, AZ  85287, United States",,"Computer aided design; Design; Optimal systems; Optimization; Reinforcement learning; Algorithmic parameters; Bayesian optimization; Human demonstrations; Inverse reinforcement learning; Learning capabilities; Near-optimal solutions; Optimal control solution; Simulation studies; Inverse problems",,Conference Paper,"Final",,Scopus,2-s2.0-85008234786
"Pirotta M., Restelli M.","49362257000;6603404086;","Inverse reinforcement learning through policy gradient minimization",2016,"30th AAAI Conference on Artificial Intelligence, AAAI 2016",,,,"1993","1999",,6,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85007188327&partnerID=40&md5=79d1e0384f7b49e1ea8c97eb87a0d253","Dipartimento di ElettronicaInformazione e Bioingegneria, Politecnico di Milano, Piazza Leonardo da Vinci, 32, Milan, I-20133, Italy","Pirotta, M., Dipartimento di ElettronicaInformazione e Bioingegneria, Politecnico di Milano, Piazza Leonardo da Vinci, 32, Milan, I-20133, Italy; Restelli, M., Dipartimento di ElettronicaInformazione e Bioingegneria, Politecnico di Milano, Piazza Leonardo da Vinci, 32, Milan, I-20133, Italy",,"Artificial intelligence; Optimization; Reinforcement learning; Empirical evaluations; Inverse reinforcement learning; Linear combinations; Linear quadratic regulator; Optimal policies; Optimization problems; Policy gradient; State of the art; Inverse problems",,Conference Paper,"Final",,Scopus,2-s2.0-85007188327
"Odom P., Natarajan S.","56050615500;57203254125;","Active advice seeking for inverse reinforcement learning",2016,"Proceedings of the International Joint Conference on Autonomous Agents and Multiagent Systems, AAMAS",,,,"503","511",,8,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85014229219&partnerID=40&md5=c8c00a55bba7aa2336e4fee588e9494f","Indiana University, Bloomington, IN, United States","Odom, P., Indiana University, Bloomington, IN, United States; Natarajan, S., Indiana University, Bloomington, IN, United States","Active advice seeking; Advice-based learning; Inverse reinforcement learning","Artificial intelligence; Autonomous agents; Decision making; Demonstrations; Intelligent systems; Inverse problems; Learning systems; Multi agent systems; Reinforcement learning; Active advice seeking; Active Learning; Active learning systems; Advice-based learning; Human expert; Inverse reinforcement learning; Optimal decision making; Traditional systems; Learning algorithms",,Conference Paper,"Final",,Scopus,2-s2.0-85014229219
"Junges S., Katoen J.-P., Jansen N., Topcu U.","55321319200;7003679176;36646082100;23570155900;","Probabilistic verification for cognitive models: Controller synthesis and model evaluation",2016,"AAAI Fall Symposium - Technical Report","FS-16-01 - FS-16-05",,,"185","188",,,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85025834417&partnerID=40&md5=76d1506991ee9735282b1a2802fa4cde","RWTH Aachen University, Germany; University of Texas, Austin, United States","Junges, S., RWTH Aachen University, Germany; Katoen, J.-P., RWTH Aachen University, Germany; Jansen, N., University of Texas, Austin, United States; Topcu, U., RWTH Aachen University, Germany",,"Behavioral research; Cognitive systems; Formal verification; Human robot interaction; Intelligent robots; Model checking; Reinforcement learning; Robots; Stochastic systems; Controller synthesis; Flexible framework; Inverse reinforcement learning; Parallel composition; Probabilistic model checking; Probabilistic verification; Robotics applications; Stochastic behavior; Stochastic models",,Conference Paper,"Final",,Scopus,2-s2.0-85025834417
"Hadfield-Menell D., Dragan A., Abbeel P., Russell S.","55921863400;55193779100;8269962600;7401538237;","Cooperative inverse reinforcement learning",2016,"Advances in Neural Information Processing Systems",,,,"3916","3924",,57,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018873749&partnerID=40&md5=60d0969695bd71e2eeaa97a256706d97","Electrical Engineering and Computer Science, University of California at Berkeley, Berkeley, CA  94709, United States","Hadfield-Menell, D., Electrical Engineering and Computer Science, University of California at Berkeley, Berkeley, CA  94709, United States; Dragan, A., Electrical Engineering and Computer Science, University of California at Berkeley, Berkeley, CA  94709, United States; Abbeel, P., Electrical Engineering and Computer Science, University of California at Berkeley, Berkeley, CA  94709, United States; Russell, S., Electrical Engineering and Computer Science, University of California at Berkeley, Berkeley, CA  94709, United States",,"Computer games; Reinforcement learning; Active teachings; Alignment Problems; Autonomous systems; Communicative actions; Formal definition; Inverse reinforcement learning; Partial information; Reward function; Inverse problems",,Conference Paper,"Final",,Scopus,2-s2.0-85018873749
"Bogert K., Lin J.F.-S., Doshi P., Kulic D.","36095803300;55891993000;23008336000;10244652900;","Expectation-maximization for inverse reinforcement learning with hidden data",2016,"Proceedings of the International Joint Conference on Autonomous Agents and Multiagent Systems, AAMAS",,,,"1034","1042",,4,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85014148697&partnerID=40&md5=6492a3e2b9a3c155bd95db4381b4158a","THINC Lab., Dept. of Computer Science, University of Georgia, Athens, GA  30602, United States; Dept. of Electrical and Computer Engg., University of Waterloo, Waterloo, ON  N2L 3G1, Canada","Bogert, K., THINC Lab., Dept. of Computer Science, University of Georgia, Athens, GA  30602, United States; Lin, J.F.-S., Dept. of Electrical and Computer Engg., University of Waterloo, Waterloo, ON  N2L 3G1, Canada; Doshi, P., THINC Lab., Dept. of Computer Science, University of Georgia, Athens, GA  30602, United States; Kulic, D., Dept. of Electrical and Computer Engg., University of Waterloo, Waterloo, ON  N2L 3G1, Canada",,"Autonomous agents; Inverse problems; Maximum principle; Multi agent systems; Apprenticeship learning; Expectation - maximizations; Hidden variable; Information missing; Inverse reinforcement learning; Missing information; Missing values; Nonconvex problem; Reinforcement learning",,Conference Paper,"Final",,Scopus,2-s2.0-85014148697
"Okal B., Arras K.O.","56132081300;6701603183;","Practical bayesian inverse reinforcement learning for robot navigation",2016,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","9853 LNCS",,,"271","274",,,"10.1007/978-3-319-46131-1_33","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988646125&doi=10.1007%2f978-3-319-46131-1_33&partnerID=40&md5=e16ec10110f86e270ec8f08b523c03e0","Social Robotics Lab, University of Freiburg, Freiburg, Germany; Bosch Corporate Research, Robert-Bosch GmbH, Renningen, Germany","Okal, B., Social Robotics Lab, University of Freiburg, Freiburg, Germany; Arras, K.O., Social Robotics Lab, University of Freiburg, Freiburg, Germany, Bosch Corporate Research, Robert-Bosch GmbH, Renningen, Germany","Inverse reinforcement learning; Representation; Robot navigation","Artificial intelligence; Behavioral research; Graphic methods; Learning systems; Navigation; Robots; Experimental evaluation; Human demonstrations; Inverse reinforcement learning; Learning behavior; Real-world task; Representation; Robot navigation; Task constraints; Reinforcement learning","Okal, B.; Social Robotics Lab, University of FreiburgGermany; email: okal@cs.uni-freiburg.de",Conference Paper,"Final",,Scopus,2-s2.0-84988646125
"Suay H.B., Brys T., Taylor M.E., Chernova S.","35106246500;55420448800;10339318000;56084330100;","Learning from demonstration for shaping through inverse reinforcement learning",2016,"Proceedings of the International Joint Conference on Autonomous Agents and Multiagent Systems, AAMAS",,,,"429","437",,18,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85014262693&partnerID=40&md5=037cfbf97668f842417be3e12bdcbcb7","Worcester Polytechnic Institute, United States; Vrije Universiteit, Brussel, Netherlands; Washington State University, United States; Georgia Institute of Technology, United States","Suay, H.B., Worcester Polytechnic Institute, United States; Brys, T., Vrije Universiteit, Brussel, Netherlands; Taylor, M.E., Washington State University, United States; Chernova, S., Georgia Institute of Technology, United States","Communication; Learning agent capabilities (agent models; Learning and adaptation; Observation); Reward structures for learning","Autonomous agents; Communication; Demonstrations; Intelligent agents; Inverse problems; Multi agent systems; Agent model; Asymptotic performance; Inverse reinforcement learning; Learning and adaptation; Learning from demonstration; Observation); Reinforcement learning agent; Reward structures for learning; Reinforcement learning",,Conference Paper,"Final",,Scopus,2-s2.0-85014262693
"Loftin R., MacGlashan J., Peng B., Taylor M.E., Littman M.L., Roberts D.L.","55669790200;21934008200;56393762000;10339318000;7006510438;55338221500;","Towards behavior-aware model learning from human-generated trajectories",2016,"AAAI Fall Symposium - Technical Report","FS-16-01 - FS-16-05",,,"67","70",,,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85025802345&partnerID=40&md5=346c0ecb0615a8337f1c610bc6440fa0","North Carolina State University, United States; Brown University, United States; Washington State University, United States","Loftin, R., North Carolina State University, United States; MacGlashan, J., Brown University, United States; Peng, B., Washington State University, United States; Taylor, M.E., North Carolina State University, United States; Littman, M.L., Brown University, United States; Roberts, D.L., North Carolina State University, United States",,"Behavioral research; Cognitive systems; Education; Human robot interaction; Intelligent robots; Inverse problems; Markov processes; Problem solving; Reinforcement learning; Complementary problems; Continuous State Space; Generated trajectories; Inverse reinforcement learning; Markov Decision Processes; Model representation; Policy gradient; Transition dynamics; Learning algorithms",,Conference Paper,"Final",,Scopus,2-s2.0-85025802345
"Kim B., Pineau J.","56123347300;13404973100;","Socially Adaptive Path Planning in Human Environments Using Inverse Reinforcement Learning",2016,"International Journal of Social Robotics","8","1",,"51","66",,47,"10.1007/s12369-015-0310-2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84957605937&doi=10.1007%2fs12369-015-0310-2&partnerID=40&md5=a22e71d3d9cf72bc78001d0e0625f50c","School of Computer Science, McGill University, 3480 University, Montreal, Canada","Kim, B., School of Computer Science, McGill University, 3480 University, Montreal, Canada; Pineau, J., School of Computer Science, McGill University, 3480 University, Montreal, Canada","Inverse reinforcement learning; Learning from demonstration; Navigation; Obstacle avoidance; RGB-D optical flow","Adaptive optics; Collision avoidance; Cost functions; Extraction; Feature extraction; Graph theory; Motion planning; Navigation; Robots; Trajectories; Adaptive path planning; Dynamic environments; Inverse reinforcement learning; Learning from demonstration; Performance criterion; Robotic wheelchairs; Surrounding obstacles; Threelayer architecture; Reinforcement learning","Kim, B.; School of Computer Science, McGill University, 3480 University, Canada; email: beomjoon.kim0@gmail.com",Article,"Final",,Scopus,2-s2.0-84957605937
"Sadigh D., Sastry S., Seshia S.A., Dragan A.D.","55052871500;56111601700;56434045800;55193779100;","Planning for autonomous cars that leverage effects on human actions",2016,"Robotics: Science and Systems","12",,,"","",,71,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041945463&partnerID=40&md5=b96312420b8f99d534323c7ac22e0bb0","University of California, Berkeley, United States","Sadigh, D., University of California, Berkeley, United States; Sastry, S., University of California, Berkeley, United States; Seshia, S.A., University of California, Berkeley, United States; Dragan, A.D., University of California, Berkeley, United States",,"Dynamical systems; Inverse problems; Reinforcement learning; Robot programming; Robotics; Autonomous car; Human actions; Human drivers; Inverse reinforcement learning; Leverage effects; Reward function; Robot plan; User study; Human robot interaction",,Conference Paper,"Final",,Scopus,2-s2.0-85041945463
[No author name available],[No author id available],"29th Australasian Joint Conference on Artificial Intelligence, AI 2016",2016,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","9992 LNAI",,,"1","728",,,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85007197675&partnerID=40&md5=d7c4d395b993804f5ab821d97b0edc8a",,"",,,,Conference Review,"Final",,Scopus,2-s2.0-85007197675
[No author name available],[No author id available],"23rd International Conference on Neural Information Processing, ICONIP 2016",2016,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","9947 LNCS",,,"1","638",,,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84992623347&partnerID=40&md5=e229a32c0bd42694699b6bbda1ffeea1",,"",,,,Conference Review,"Final",,Scopus,2-s2.0-84992623347
"Masuyama G., Umeda K.","55211221400;7102144600;","Apprenticeship learning based on inconsistent demonstrations",2015,"IEEE International Conference on Intelligent Robots and Systems","2015-December",, 7354121,"5273","5278",,,"10.1109/IROS.2015.7354121","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84958231206&doi=10.1109%2fIROS.2015.7354121&partnerID=40&md5=30fff1d701bbde99af7a27f1ab8a5987","Department of Precision Mechanics, Faculty of Science and Engineering, Chuo University, 1-13-27 Kasuga, Bunkyo-ku, Tokyo, 113-8551, Japan","Masuyama, G., Department of Precision Mechanics, Faculty of Science and Engineering, Chuo University, 1-13-27 Kasuga, Bunkyo-ku, Tokyo, 113-8551, Japan; Umeda, K., Department of Precision Mechanics, Faculty of Science and Engineering, Chuo University, 1-13-27 Kasuga, Bunkyo-ku, Tokyo, 113-8551, Japan","Context; Estimation; Learning (artificial intelligence); Robot control; Training data; Trajectory","Apprentices; Artificial intelligence; Demonstrations; Estimation; Intelligent robots; Robots; Trajectories; Affine transformations; Apprenticeship learning; Context; Inverse reinforcement learning; Learning (artificial intelligence); Non-stationarities; Robot controls; Training data; Reinforcement learning",,Conference Paper,"Final",,Scopus,2-s2.0-84958231206
"Almingol J., Montesano L.","56096608600;6602849539;","Learning multiple behaviours using hierarchical clustering of rewards",2015,"IEEE International Conference on Intelligent Robots and Systems","2015-December",, 7354033,"4608","4613",,,"10.1109/IROS.2015.7354033","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84958205829&doi=10.1109%2fIROS.2015.7354033&partnerID=40&md5=7ab7cee65d8be18e898e6de5048c046e","Universidad de Zaragoza, Instituto de Investigación en Ingeniería de Aragón (I3A), Spain","Almingol, J., Universidad de Zaragoza, Instituto de Investigación en Ingeniería de Aragón (I3A), Spain; Montesano, L., Universidad de Zaragoza, Instituto de Investigación en Ingeniería de Aragón (I3A), Spain","Aerospace electronics; Clustering algorithms; Cost function; Entropy; Learning (artificial intelligence); Space exploration; Trajectory","Artificial intelligence; Cost functions; Entropy; Intelligent robots; Inverse problems; Reinforcement learning; Robotics; Robots; Space research; Trajectories; Aerospace electronics; Hier-archical clustering; Hierarchical clustering approach; Inverse reinforcement learning; Learning (artificial intelligence); Learning from demonstration; Similarity metrics; Space explorations; Clustering algorithms",,Conference Paper,"Final",,Scopus,2-s2.0-84958205829
"Daniel C., Kroemer O., Viering M., Metz J., Peters J.","55556874400;35194171900;56721374300;56720872800;35248912800;","Active reward learning with a novel acquisition function",2015,"Autonomous Robots","39","3",,"389","405",,9,"10.1007/s10514-015-9454-z","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84941937390&doi=10.1007%2fs10514-015-9454-z&partnerID=40&md5=646393eb6c69b07a2e63eab0f30a0874","Technische Universität Darmstadt, Hochschulstrasse 10, Darmstadt, 64289, Germany; Max-Planck-Institut für Intelligente Systeme, Spemannstraße 38, Tübingen, 72076, Germany","Daniel, C., Technische Universität Darmstadt, Hochschulstrasse 10, Darmstadt, 64289, Germany; Kroemer, O., Technische Universität Darmstadt, Hochschulstrasse 10, Darmstadt, 64289, Germany; Viering, M., Technische Universität Darmstadt, Hochschulstrasse 10, Darmstadt, 64289, Germany; Metz, J., Technische Universität Darmstadt, Hochschulstrasse 10, Darmstadt, 64289, Germany; Peters, J., Technische Universität Darmstadt, Hochschulstrasse 10, Darmstadt, 64289, Germany, Max-Planck-Institut für Intelligente Systeme, Spemannstraße 38, Tübingen, 72076, Germany","Acquisition functions; Active learning; Bayesian optimization; Inverse reinforcement learning; Preference learning; Reinforcement learning; Reward functions","Artificial intelligence; Mergers and acquisitions; Reinforcement learning; Robot learning; Robots; Active Learning; Bayesian optimization; Inverse reinforcement learning; Preference learning; Reward function; Function evaluation","Daniel, C.; Technische Universität Darmstadt, Hochschulstrasse 10, Germany; email: daniel@ias.tu-darmstadt.de",Article,"Final",,Scopus,2-s2.0-84941937390
"Tsunekawa H., Suzuki T., Hamagami T.","57140196900;56808086500;8317852200;","Examination of skill-based learning by inverse reinforcement learning using evolutionary process",2015,"Proceedings of 2015 IEEE 9th International Conference on Intelligent Systems and Control, ISCO 2015",,, 7282296,"","",,3,"10.1109/ISCO.2015.7282296","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84959162902&doi=10.1109%2fISCO.2015.7282296&partnerID=40&md5=96f23e00cf57e49d013c86be0dd94775","Yokohama National University, Yokohama, Japan","Tsunekawa, H., Yokohama National University, Yokohama, Japan; Suzuki, T., Yokohama National University, Yokohama, Japan; Hamagami, T., Yokohama National University, Yokohama, Japan","Evolutionary Process; Inverse Reinforcement Learning; Reinforcement Learning; Skill-based Learning","Intelligent systems; Inverse problems; Behavior rules; Driving tasks; Evolutionary process; Inverse reinforcement learning; Learning convergence; Process of learning; Reward function; Skill-based Learning; Reinforcement learning",,Conference Paper,"Final",,Scopus,2-s2.0-84959162902
"Shimosaka M., Nishi K., Sato J., Kataoka H.","7003853628;56723495100;57212913027;36720714800;","Predicting driving behavior using inverse reinforcement learning with multiple reward functions towards environmental diversity",2015,"IEEE Intelligent Vehicles Symposium, Proceedings","2015-August",, 7225745,"567","572",,12,"10.1109/IVS.2015.7225745","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84951162976&doi=10.1109%2fIVS.2015.7225745&partnerID=40&md5=d5ce3dfdb0b08ee5582ee780f2efb21d","Department of Mechano-Informatics, University of Tokyo, Tokyo, Japan","Shimosaka, M., Department of Mechano-Informatics, University of Tokyo, Tokyo, Japan; Nishi, K., Department of Mechano-Informatics, University of Tokyo, Tokyo, Japan; Sato, J., Department of Mechano-Informatics, University of Tokyo, Tokyo, Japan; Kataoka, H., Department of Mechano-Informatics, University of Tokyo, Tokyo, Japan",,"Automobile drivers; Bayesian networks; Behavioral research; Forecasting; Intelligent vehicle highway systems; Inverse problems; Reinforcement learning; Vehicles; Dirichlet process mixture; Driving behavior; Driving environment; Environmental diversities; Inverse reinforcement learning; Long-term prediction; Poor performance; Professional drivers; Advanced driver assistance systems",,Conference Paper,"Final",,Scopus,2-s2.0-84951162976
"Odom P., Natarajan S.","56050615500;57203254125;","Active advice seeking for inverse reinforcement learning",2015,"Proceedings of the National Conference on Artificial Intelligence","6",,,"4186","4187",,4,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84961233902&partnerID=40&md5=ec840c66e309c4b074a69463467208e8","School of Informatics and Computing, Indiana University Bloomington, United States","Odom, P., School of Informatics and Computing, Indiana University Bloomington, United States; Natarajan, S., School of Informatics and Computing, Indiana University Bloomington, United States",,"Artificial intelligence; Decision making; Demonstrations; Intelligent systems; Inverse problems; Active Learning; Human expert; Inverse reinforcement learning; Optimal decision making; Reinforcement learning",,Conference Paper,"Final",,Scopus,2-s2.0-84961233902
"Ermon S., Xue Y., Toth R., Dilkina B., Bernstein R., Damoulas T., Clark P., DeGloria S., Mude A., Barrett C., Gomes C.P.","35791579200;55216614000;55940320300;23008031500;36805567300;24175970800;56745331400;6603727016;16203313100;7201943246;7101707114;","Learning large-scale dynamic discrete choice models of spatio-temporal preferences with application to migratory pastoralism in east Africa",2015,"Proceedings of the National Conference on Artificial Intelligence","1",,,"644","650",,9,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84959537621&partnerID=40&md5=f9bbe4f3af1c39b07e8368091aa4ca16","Stanford University, United States; Cornell University, United States; University of Sydney, Australia; Georgia Tech, United States; NYU CUSP, United States; USDA Research Service, United States; International Livestock Research Institute, United States","Ermon, S., Stanford University, United States; Xue, Y., Cornell University, United States; Toth, R., University of Sydney, Australia; Dilkina, B., Georgia Tech, United States; Bernstein, R., Cornell University, United States; Damoulas, T., NYU CUSP, United States; Clark, P., USDA Research Service, United States; DeGloria, S., Cornell University, United States; Mude, A., International Livestock Research Institute, United States; Barrett, C., Cornell University, United States; Gomes, C.P., Cornell University, United States",,"Artificial intelligence; Climate change; Decision making; Economics; Inverse problems; Learning algorithms; Learning systems; Reinforcement learning; Statistics; Arid and semi-arid regions; Discrete choice models; Inverse reinforcement learning; Large-scale dynamics; Machine learning literature; Movement trajectories; Probabilistic approaches; Resource availability; Climate models",,Conference Paper,"Final",,Scopus,2-s2.0-84959537621
"Michini B., Walsh T.J., Agha-Mohammadi A.-A., How J.P.","36603096200;56388213200;54792649700;7006512768;","Bayesian Nonparametric Reward Learning From Demonstration",2015,"IEEE Transactions on Robotics","31","2", 7076638,"369","386",,19,"10.1109/TRO.2015.2405593","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027958292&doi=10.1109%2fTRO.2015.2405593&partnerID=40&md5=336016010dbf9ea167a354ded3a05581","Aerospace Controls Lab., Massachusetts Institute of Technology, CambridgeMA  02139, United States; Airware, San Francisco, CA  94103, United States; Kronos Incorporated, Chelmsford, MA  01824, United States; Laboratory for Information and Decision Systems, Massachusetts Institute of Technology, Cambridge, MA  02139, United States","Michini, B., Aerospace Controls Lab., Massachusetts Institute of Technology, CambridgeMA  02139, United States, Airware, San Francisco, CA  94103, United States; Walsh, T.J., Aerospace Controls Lab., Massachusetts Institute of Technology, CambridgeMA  02139, United States, Kronos Incorporated, Chelmsford, MA  01824, United States; Agha-Mohammadi, A.-A., Laboratory for Information and Decision Systems, Massachusetts Institute of Technology, Cambridge, MA  02139, United States; How, J.P., Aerospace Controls Lab., Massachusetts Institute of Technology, CambridgeMA  02139, United States","Demonstration; inverse reinforcement learning (IRL); reward learning","Demonstrations; Learning algorithms; Remote control; Attractive solutions; Computational advantages; Discrete state space; Gaussian Processes; Inverse reinforcement learning; Learning frameworks; Learning from demonstration; reward learning; Reinforcement learning",,Article,"Final",,Scopus,2-s2.0-85027958292
"Vasquez D., Yu Y., Kumar S., Laugier C.","7004245418;56308172600;55567173000;7007031683;","An open framework for human-like autonomous driving using inverse reinforcement learning",2015,"2014 IEEE Vehicle Power and Propulsion Conference, VPPC 2014",,, 7007013,"","",,2,"10.1109/VPPC.2014.7007013","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84934312155&doi=10.1109%2fVPPC.2014.7007013&partnerID=40&md5=54fde521cbbd5f0d1de346da42faee30","Inria Rhne-Alpes, France; Beijing University, China; IIIT Hyderabad, India","Vasquez, D., Inria Rhne-Alpes, France; Yu, Y., Beijing University, China; Kumar, S., IIIT Hyderabad, India; Laugier, C., Inria Rhne-Alpes, France","Autonomous Driving; Experimental Frameworks; Inverse Reinforcement Learning","Reinforcement learning; Robotics; Autonomous driving; Autonomous Vehicles; Driving assistance systems; Driving simulator; Evaluation metrics; Experimental Frameworks; Inverse reinforcement learning; Reinforce learning; Advanced driver assistance systems",,Conference Paper,"Final",,Scopus,2-s2.0-84934312155
"Bogert K., Doshi P.","36095803300;23008336000;","Multi-robot inverse reinforcement learning under occlusion with state transition estimation",2015,"Proceedings of the International Joint Conference on Autonomous Agents and Multiagent Systems, AAMAS","3",,,"1837","1838",,1,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84944679224&partnerID=40&md5=ad8ddedf7e8b772caf511e8b6827a9a0","THINC Lab, University of Georgia, Athens, GA  30602, United States","Bogert, K., THINC Lab, University of Georgia, Athens, GA  30602, United States; Doshi, P., THINC Lab, University of Georgia, Athens, GA  30602, United States","Inverse reinforcement; Machine learning; Multi-robot systems","Artificial intelligence; Autonomous agents; Industrial robots; Learning systems; Multi agent systems; Multipurpose robots; Probability; Robots; Stochastic systems; Inverse reinforcement learning; Multi-robot systems; Multiple robot; Robotic applications; State transitions; Stochastic transitions; Transition errors; Transition functions; Reinforcement learning",,Conference Paper,"Final",,Scopus,2-s2.0-84944679224
"Bogert K., Doshi P.","36095803300;23008336000;","Toward estimating others' transition models under occlusion for multi-robot IRL",2015,"IJCAI International Joint Conference on Artificial Intelligence","2015-January",,,"1867","1873",,3,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84949803311&partnerID=40&md5=28f198a24905ddd4a1cba71d05fb776e","THINC Lab., Department of Computer Science, University of Georgia, Athens, GA  30602, United States","Bogert, K., THINC Lab., Department of Computer Science, University of Georgia, Athens, GA  30602, United States; Doshi, P., THINC Lab., Department of Computer Science, University of Georgia, Athens, GA  30602, United States",,"Artificial intelligence; Industrial robots; Inverse problems; Multipurpose robots; Nonlinear programming; Probability; Reinforcement learning; Stochastic systems; Inverse reinforcement learning; Non-linear optimization; Robotic applications; Stochastic transitions; Transition errors; Transition functions; Transition model; Under-constrained; Robots",,Conference Paper,"Final",,Scopus,2-s2.0-84949803311
"Herman M., Fischer V., Gindele T., Burgard W.","56742755300;56742874600;24476503400;7003610380;","Inverse reinforcement learning of behavioral models for online-adapting navigation strategies",2015,"Proceedings - IEEE International Conference on Robotics and Automation","2015-June","June", 7139642,"3215","3222",,6,"10.1109/ICRA.2015.7139642","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84938262677&doi=10.1109%2fICRA.2015.7139642&partnerID=40&md5=3a92ffad8fa4945aa14ca0071ba9fb3c","Robert Bosch GmbH, Corporate Sector Research and Advance Engineering, Stuttgart, D-70442, Germany; University of Freiburg, Department of Computer Science, Freiburg, D-79110, Germany","Herman, M., Robert Bosch GmbH, Corporate Sector Research and Advance Engineering, Stuttgart, D-70442, Germany; Fischer, V., Robert Bosch GmbH, Corporate Sector Research and Advance Engineering, Stuttgart, D-70442, Germany; Gindele, T., Robert Bosch GmbH, Corporate Sector Research and Advance Engineering, Stuttgart, D-70442, Germany; Burgard, W., University of Freiburg, Department of Computer Science, Freiburg, D-79110, Germany",,"Economic and social effects; Inverse problems; Reinforcement learning; Robot programming; Robotics; Robots; Stochastic models; Stochastic systems; Adaptive navigation; Autonomous systems; Human demonstrations; Inverse reinforcement learning; Mobile autonomous; Navigation strategies; Social acceptability; Stochastic behavior; Behavioral research",,Conference Paper,"Final",,Scopus,2-s2.0-84938262677
"Nguyen Q.P., Low K.H., Jaillet P.","57189094420;7102180182;6701591623;","Inverse reinforcement learning with locally consistent reward functions",2015,"Advances in Neural Information Processing Systems","2015-January",,,"1747","1755",,8,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84965111532&partnerID=40&md5=cc7e0c181b86f6dbf7255fd4fb13c420","Dept. of Computer Science, National University of Singapore, Singapore, Singapore; Dept. of Electrical Engineering and Computer Science, Massachusetts Institute of Technology, United States","Nguyen, Q.P., Dept. of Computer Science, National University of Singapore, Singapore, Singapore; Low, K.H., Dept. of Computer Science, National University of Singapore, Singapore, Singapore; Jaillet, P., Dept. of Electrical Engineering and Computer Science, Massachusetts Institute of Technology, United States",,"Algorithms; Clustering algorithms; Information science; Inverse problems; Iterative methods; Maximum likelihood; Maximum principle; Stochastic models; Stochastic systems; Trajectories; Empirical evaluations; Expectation-maximization algorithms; Inverse reinforcement learning; Probabilistic graphical models; Real-world datasets; Reward function; State of the art; Stochastic transitions; Reinforcement learning",,Conference Paper,"Final",,Scopus,2-s2.0-84965111532
"Xie N., Zhao T., Tian F., Zhang X., Sugiyama M.","57203385327;53664758000;55186571600;57207320672;7402826969;","Stroke-based stylization learning and rendering with inverse reinforcement learning",2015,"IJCAI International Joint Conference on Artificial Intelligence","2015-January",,,"2531","2539",,3,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84949755630&partnerID=40&md5=e07d7bf827677784e159fc07839916fc","Tongji University, China; Tianjin University of Science and Technology, China; Bournemouth University, United Kingdom; Hiroshima Institute of Technology, Japan; University of Tokyo, Japan","Xie, N., Tongji University, China; Zhao, T., Tianjin University of Science and Technology, China; Tian, F., Bournemouth University, United Kingdom; Zhang, X., Hiroshima Institute of Technology, Japan; Sugiyama, M., University of Tokyo, Japan",,"Artificial intelligence; Computer graphics; Rendering (computer graphics); Brush stroke; Drawing styles; Inverse reinforcement learning; Non-Photorealistic Rendering; PhotoShop; Reinforcement learning",,Conference Paper,"Final",,Scopus,2-s2.0-84949755630
"Bloem M., Bambos N.","23570762400;7004840524;","Ground delay program analytics with behavioral cloning and inverse reinforcement learning",2015,"Journal of Aerospace Information Systems","12","3",,"299","313",,15,"10.2514/1.I010304","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84929144713&doi=10.2514%2f1.I010304&partnerID=40&md5=5fd387c1b7fda91b77b4c086e6f148c4","Systems Modeling and Optimization Branch, MS 210-15, NASA Ames Research Center, Moffett Field, CA  94035, United States; Departments of Management Science and Engineering and Electrical Engineering, Stanford University, Stanford, CA  94305, United States","Bloem, M., Systems Modeling and Optimization Branch, MS 210-15, NASA Ames Research Center, Moffett Field, CA  94035, United States; Bambos, N., Departments of Management Science and Engineering and Electrical Engineering, Stanford University, Stanford, CA  94305, United States",,"Air traffic control; Cloning; Decision making; Decision trees; Forecasting; Genetic engineering; Air traffics; Behavioral cloning; Ground delay programs; Historical data; Inverse reinforcement learning; Random forests; San Francisco International Airport; Reinforcement learning",,Article,"Final",,Scopus,2-s2.0-84929144713
"Bloem M., Bambos N.","23570762400;7004840524;","Ground delay program analytics with behavioral cloning and inverse reinforcement learning",2015,"AGIFORS 55th Annual Symposium: Analytics for Efficiency and Customer Centric Optimization",,,,"","",,,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84979088384&partnerID=40&md5=6608dd2266190fa16fb0833a8abf9ad3","Department of Management Science and Engineering, Stanford University, Stanford, CA  94305, United States; Department of Management Science and Engineering and Electrical Engineering, Stanford University, Stanford, CA  94305, United States","Bloem, M., Department of Management Science and Engineering, Stanford University, Stanford, CA  94305, United States; Bambos, N., Department of Management Science and Engineering and Electrical Engineering, Stanford University, Stanford, CA  94305, United States",,"Air traffic control; Cloning; Decision making; Decision trees; Efficiency; Forecasting; Genetic engineering; Air traffics; Behavioral cloning; Gain insight; Ground delay programs; Historical data; Inverse reinforcement learning; Random forests; San Francisco International Airport; Reinforcement learning",,Conference Paper,"Final",,Scopus,2-s2.0-84979088384
"Sakurai S., Oba S., Ishii S.","57033593500;7007003360;7403110142;","Inverse reinforcement learning based on behaviors of a learning agent",2015,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","9489",,,"724","732",,1,"10.1007/978-3-319-26532-2_80","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84952791706&doi=10.1007%2f978-3-319-26532-2_80&partnerID=40&md5=de6114776b69351ad6e77666fd9e80aa","Department of Systems Science, Graduate School of Informatics, Kyoto University, Kyoto, Japan","Sakurai, S., Department of Systems Science, Graduate School of Informatics, Kyoto University, Kyoto, Japan; Oba, S., Department of Systems Science, Graduate School of Informatics, Kyoto University, Kyoto, Japan; Ishii, S., Department of Systems Science, Graduate School of Informatics, Kyoto University, Kyoto, Japan","Apprenticeship learning; Inverse reinforcement learning; Reinforcement learning","Apprentices; Information science; Intelligent agents; Inverse problems; Markov processes; Apprenticeship learning; Appropriate designs; Inverse reinforcement learning; Learning strategy; Markov Decision Processes; Optimal policies; Reinforcement learning agent; Variable environment; Reinforcement learning","Sakurai, S.; Department of Systems Science, Graduate School of Informatics, Kyoto UniversityJapan; email: sakurai.shunsuke.35r@st.kyoto-u.ac.jp",Conference Paper,"Final",,Scopus,2-s2.0-84952791706
[No author name available],[No author id available],"AGIFORS 55th Annual Symposium: Analytics for Efficiency and Customer Centric Optimization",2015,"AGIFORS 55th Annual Symposium: Analytics for Efficiency and Customer Centric Optimization",,,,"","",715,,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84979085483&partnerID=40&md5=cfcd9832f101fa8d1b5c22870ff75aad",,"",,,,Conference Review,"Final",,Scopus,2-s2.0-84979085483
"Sezener C.E.","56728606400;","Inferring human values for safe AGI design",2015,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","9205",,,"152","155",,3,"10.1007/978-3-319-21365-1_16","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84952760405&doi=10.1007%2f978-3-319-21365-1_16&partnerID=40&md5=e6afb198afb83c70b196a149178212a0","Department of Computer Science, Ozyegin University, Istanbul, Turkey","Sezener, C.E., Department of Computer Science, Ozyegin University, Istanbul, Turkey","Friendly AI; Inverse reinforcement learning; Safe AGI; Value learning","Reinforcement learning; Human values; Inverse reinforcement learning; Safe AGI; Value learning; Behavioral research","Sezener, C.E.; Department of Computer Science, Ozyegin UniversityTurkey; email: eren.sezener@ozu.edu.tr",Conference Paper,"Final",,Scopus,2-s2.0-84952760405
"Nikolaidis S., Ramakrishnan R., Gu K., Shah J.","55635433000;56895701000;56895699700;19640414800;","Efficient Model Learning from Joint-Action Demonstrations for Human-Robot Collaborative Tasks",2015,"ACM/IEEE International Conference on Human-Robot Interaction","2015-March",,,"189","196",,56,"10.1145/2696454.2696455","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84943534743&doi=10.1145%2f2696454.2696455&partnerID=40&md5=d9fc83d3488fe6c3abdb4b7bd0d3377b","Computer Science and Artificial Intelligence Laboratory, Massachusetts Institute of Technology, Cambridge, MA  02139, United States","Nikolaidis, S., Computer Science and Artificial Intelligence Laboratory, Massachusetts Institute of Technology, Cambridge, MA  02139, United States; Ramakrishnan, R., Computer Science and Artificial Intelligence Laboratory, Massachusetts Institute of Technology, Cambridge, MA  02139, United States; Gu, K., Computer Science and Artificial Intelligence Laboratory, Massachusetts Institute of Technology, Cambridge, MA  02139, United States; Shah, J., Computer Science and Artificial Intelligence Laboratory, Massachusetts Institute of Technology, Cambridge, MA  02139, United States","human-robot collaboration; mixed observability markov decision process; model learning","Human computer interaction; Learning algorithms; Man machine systems; Markov processes; Observability; Reinforcement learning; Robots; Collaborative tasks; Human subject experiments; Human-robot collaboration; Inverse reinforcement learning; Markov Decision Processes; Mixed observabilities; Model learning; Observable variables; Human robot interaction",,Conference Paper,"Final",,Scopus,2-s2.0-84943534743
"Yang S.Y., Qiao Q., Beling P.A., Scherer W.T., Kirilenko A.A.","24802841600;35848809400;6603732790;7102162666;7005937861;","Gaussian process-based algorithmic trading strategy identification",2015,"Quantitative Finance","15","10",,"1683","1703",,11,"10.1080/14697688.2015.1011684","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84941803646&doi=10.1080%2f14697688.2015.1011684&partnerID=40&md5=5508d8be850793319ecd7524238c4336","Financial Engineering Program, Stevens Institute of Technology, 1 Castle Point on Hudson, Hoboken, NJ  03070, United States; Department of Systems and Information Engineering, The University of Virginia, Charlottesville, VA  22904, United States; MIT Sloan School of Management, 50 Memorial Drive, Cambridge, MA  02142, United States","Yang, S.Y., Financial Engineering Program, Stevens Institute of Technology, 1 Castle Point on Hudson, Hoboken, NJ  03070, United States; Qiao, Q., Department of Systems and Information Engineering, The University of Virginia, Charlottesville, VA  22904, United States; Beling, P.A., Department of Systems and Information Engineering, The University of Virginia, Charlottesville, VA  22904, United States; Scherer, W.T., Department of Systems and Information Engineering, The University of Virginia, Charlottesville, VA  22904, United States; Kirilenko, A.A., Department of Systems and Information Engineering, The University of Virginia, Charlottesville, VA  22904, United States, MIT Sloan School of Management, 50 Memorial Drive, Cambridge, MA  02142, United States","Algorithmic trading; Behavioural finance; Gaussian process; High-frequency trading; Inverse reinforcement learning; Markov decision process; Support vector machine",,"Yang, S.Y.; Financial Engineering Program, Stevens Institute of Technology, 1 Castle Point on Hudson, United States",Article,"Final",Open Access,Scopus,2-s2.0-84941803646
"Li D.C., He Y.Q., Fu F.","56650286700;23060003200;56650564700;","Nonlinear inverse reinforcement learning with mutual information and Gaussian process",2014,"2014 IEEE International Conference on Robotics and Biomimetics, IEEE ROBIO 2014",,, 7090537,"1445","1450",,,"10.1109/ROBIO.2014.7090537","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84949928482&doi=10.1109%2fROBIO.2014.7090537&partnerID=40&md5=2e92cbadd0125b09ea9dda575eaf9d96","Shenyang Institute of Automation, Chinese Academy of Sciences, Shenyang, China","Li, D.C., Shenyang Institute of Automation, Chinese Academy of Sciences, Shenyang, China; He, Y.Q., Shenyang Institute of Automation, Chinese Academy of Sciences, Shenyang, China; Fu, F., Shenyang Institute of Automation, Chinese Academy of Sciences, Shenyang, China",,"Algorithms; Biomimetics; Gaussian distribution; Gaussian noise (electronic); Knowledge acquisition; Learning algorithms; Learning systems; Adaptive modeling; Automatic relevance determination; Extreme learning machine; Gaussian Processes; Generalization capability; Inverse reinforcement learning; Mutual informations; Optimal subsets; Reinforcement learning",,Conference Paper,"Final",,Scopus,2-s2.0-84949928482
"Surana A., Srivastava K.","6602087409;24924034600;","Bayesian nonparametric inverse reinforcement learning for switched markov decision processes",2014,"Proceedings - 2014 13th International Conference on Machine Learning and Applications, ICMLA 2014",,, 7033090,"47","54",,9,"10.1109/ICMLA.2014.105","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84946685279&doi=10.1109%2fICMLA.2014.105&partnerID=40&md5=d0d708d4e61f8253d2f6ad5efaebc9c2","United Technologies Research Center, East Hartford, CT  06118, United States","Surana, A., United Technologies Research Center, East Hartford, CT  06118, United States; Srivastava, K., United Technologies Research Center, East Hartford, CT  06118, United States","Bayesian Nonparametrics; Inverse Reinforcement Learning; Markov Decision Processes","Artificial intelligence; Dynamical systems; Learning algorithms; Learning systems; Linear control systems; Markov processes; Monte Carlo methods; Bayesian nonparametrics; Complex agent; Inverse reinforcement learning; Linear dynamical systems; Markov chain Monte Carlo method; Markov Decision Processes; Non-parametric; Sticky hierarchical dirichlet process; Reinforcement learning",,Conference Paper,"Final",,Scopus,2-s2.0-84946685279
[No author name available],[No author id available],"Proceedings - 2014 13th International Conference on Machine Learning and Applications, ICMLA 2014",2014,"Proceedings - 2014 13th International Conference on Machine Learning and Applications, ICMLA 2014",,,,"","",643,,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84949087653&partnerID=40&md5=de15f2f689464558fba5b0b86e5b4d15",,"",,,,Conference Review,"Final",,Scopus,2-s2.0-84949087653
"Kim D., Breslin C., Tsiakoulis P., Gašić M., Henderson M., Young S.","56304109400;18041542300;14018573600;26631804000;56360767500;7404515229;","Inverse reinforcement learning for micro-turn management",2014,"Proceedings of the Annual Conference of the International Speech Communication Association, INTERSPEECH",,,,"328","332",,9,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84910078105&partnerID=40&md5=1c54ea57e3b7cc9fb30094cd269b7781","Department of Engineering, University of Cambridge, Cambridge, United Kingdom","Kim, D., Department of Engineering, University of Cambridge, Cambridge, United Kingdom; Breslin, C., Department of Engineering, University of Cambridge, Cambridge, United Kingdom; Tsiakoulis, P., Department of Engineering, University of Cambridge, Cambridge, United Kingdom; Gašić, M., Department of Engineering, University of Cambridge, Cambridge, United Kingdom; Henderson, M., Department of Engineering, University of Cambridge, Cambridge, United Kingdom; Young, S., Department of Engineering, University of Cambridge, Cambridge, United Kingdom","Dialogue management; Inverse reinforcement learning; Markov decision processes; Spoken dialogue systems","Behavioral research; Markov processes; Speech communication; Speech processing; Decision-theoretic; Dialogue management; Human-human interactions; Inverse reinforcement learning; Markov Decision Processes; Natural interactions; Reward function; Spoken dialogue system; Reinforcement learning","Kim, D.; Department of Engineering, University of CambridgeUnited Kingdom",Conference Paper,"Final",,Scopus,2-s2.0-84910078105
"Rojas-Barahona L.M., Cerisara C.","26653813700;6506781029;","Bayesian inverse reinforcement learning for modeling conversational agents in a virtual environment",2014,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","8403 LNCS","PART 1",,"503","514",,1,"10.1007/978-3-642-54906-9_41","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84958522870&doi=10.1007%2f978-3-642-54906-9_41&partnerID=40&md5=c7a815b00d10fae40ceb1ae2acfb9721","Université de Lorraine/LORIA, Nancy, France; CNRS/LORIA, Nancy, France","Rojas-Barahona, L.M., Université de Lorraine/LORIA, Nancy, France; Cerisara, C., CNRS/LORIA, Nancy, France",,"Bayesian networks; Behavioral research; Computational linguistics; Text processing; Virtual reality; Baseline systems; Bayesian; Bayesian approaches; Conversational agents; Dialogue manager; Inverse reinforcement learning; Optimal decisions; Serious games; Reinforcement learning",,Conference Paper,"Final",,Scopus,2-s2.0-84958522870
"Bloem M., Bambos N.","23570762400;7004840524;","Ground delay program analytics with behavioral cloning and inverse reinforcement learning",2014,"AIAA AVIATION 2014 -14th AIAA Aviation Technology, Integration, and Operations Conference",,,,"","",,8,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84907388882&partnerID=40&md5=a2a848590094da70dea62ea56ce227dc","NASA Ames Research Center, Systems Modeling and Optimization Branch, MS 210-15, Moffett Field, CA, 94035, United States; Stanford University, Stanford, CA, 94305, United States","Bloem, M., NASA Ames Research Center, Systems Modeling and Optimization Branch, MS 210-15, Moffett Field, CA, 94035, United States; Bambos, N., Stanford University, Stanford, CA, 94305, United States",,"Air traffic control; Aviation; Cloning; Decision trees; Forecasting; Air traffics; Behavioral cloning; Gain insight; Ground delay programs; Historical data; Inverse reinforcement learning; Random forests; San Francisco International Airport; Reinforcement learning",,Conference Paper,"Final",,Scopus,2-s2.0-84907388882
"Calinon S., Bruno D., Malekzadeh M.S., Nanayakkara T., Caldwell D.G.","6507248533;56018693500;56019474800;6508336274;7202685497;","Human-robot skills transfer interfaces for a flexible surgical robot",2014,"Computer Methods and Programs in Biomedicine","116","2",,"81","96",,25,"10.1016/j.cmpb.2013.12.015","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84903192398&doi=10.1016%2fj.cmpb.2013.12.015&partnerID=40&md5=928d826f81dc853819cf4a14e1c201b9","Department of Advanced Robotics, Istituto Italiano di Tecnologia (IIT), Via Morego 30, 16163 Genova, Italy; Department of Informatics, King's College London, Strand, London, WC2R 2LS, United Kingdom","Calinon, S., Department of Advanced Robotics, Istituto Italiano di Tecnologia (IIT), Via Morego 30, 16163 Genova, Italy; Bruno, D., Department of Advanced Robotics, Istituto Italiano di Tecnologia (IIT), Via Morego 30, 16163 Genova, Italy; Malekzadeh, M.S., Department of Advanced Robotics, Istituto Italiano di Tecnologia (IIT), Via Morego 30, 16163 Genova, Italy; Nanayakkara, T., Department of Informatics, King's College London, Strand, London, WC2R 2LS, United Kingdom; Caldwell, D.G., Department of Advanced Robotics, Istituto Italiano di Tecnologia (IIT), Via Morego 30, 16163 Genova, Italy","Inverse reinforcement learning; Learning from demonstration; Robot-assisted surgery; Skills transfer; Soft robotics; Stochastic optimization","Demonstrations; Optimization; Reinforcement learning; Robots; Surgery; Inverse reinforcement learning; Learning from demonstration; Robot-assisted surgery; Skills transfer; Soft robotics; Stochastic optimizations; Surgical equipment; article; human; kinematics; laparoscopic surgery; reinforcement; reward; robot assisted surgery; robotics; simulation; skill; surgical equipment; surgical technique; algorithm; artificial intelligence; biomechanics; computer interface; computer simulation; devices; image quality; motor performance; robotic surgical procedure; robotics; task performance; Algorithms; Artificial Intelligence; Biomechanical Phenomena; Computer Simulation; Humans; Motor Skills; Phantoms, Imaging; Robotic Surgical Procedures; Robotics; Task Performance and Analysis; User-Computer Interface","Calinon, S.; Department of Advanced Robotics, Istituto Italiano di Tecnologia (IIT), Via Morego 30, 16163 Genova, Italy; email: sylvain.calinon@iit.it",Article,"Final",,Scopus,2-s2.0-84903192398
"Bloem M., Bambos N.","23570762400;7004840524;","Infinite time horizon maximum causal entropy inverse reinforcement learning",2014,"Proceedings of the IEEE Conference on Decision and Control","2015-February","February", 7040156,"4911","4916",,17,"10.1109/CDC.2014.7040156","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988268663&doi=10.1109%2fCDC.2014.7040156&partnerID=40&md5=b1d4cd7ceff3278c960c86b7c0da39f5","Aviation Systems Division, NASA Ames Research Center, Moffett Field, CA  94035, United States; Departments of Electrical Engineering and Management Science and Engineering, Stanford University, Stanford, CA  94305, United States","Bloem, M., Aviation Systems Division, NASA Ames Research Center, Moffett Field, CA  94035, United States; Bambos, N., Departments of Electrical Engineering and Management Science and Engineering, Stanford University, Stanford, CA  94305, United States",,,,Conference Paper,"Final",,Scopus,2-s2.0-84988268663
"Pérez-Higueras N., Ramón-Vigo R., Caballero F., Merino L.","56414879200;56414842900;8956620100;7003946345;","Robot local navigation with learned social cost functions",2014,"ICINCO 2014 - Proceedings of the 11th International Conference on Informatics in Control, Automation and Robotics","2",,,"618","625",,6,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84910024548&partnerID=40&md5=20f5ae5cc8e1d4a21934265b84306782","School of Engineering, Pablo de Olavide University, Crta. Utrera km 1, Seville, Spain; Dpt. of System Engineering and Automation, University of Seville, Camino de los Descubrimientos s/n, Seville, Spain","Pérez-Higueras, N., School of Engineering, Pablo de Olavide University, Crta. Utrera km 1, Seville, Spain; Ramón-Vigo, R., School of Engineering, Pablo de Olavide University, Crta. Utrera km 1, Seville, Spain; Caballero, F., Dpt. of System Engineering and Automation, University of Seville, Camino de los Descubrimientos s/n, Seville, Spain; Merino, L., School of Engineering, Pablo de Olavide University, Crta. Utrera km 1, Seville, Spain","Inverse Reinforcement Learning; Learning from Demonstrations; Robot Navigation; Social Robots","Cost functions; Costs; Human robot interaction; Navigation systems; Reinforcement learning; Robotics; Telecommunication networks; Human interactions; Human social interactions; Inverse reinforcement learning; Learning from demonstration; Robot acceptances; Robot navigation; Robot navigation system; Social robots; Robots","Pérez-Higueras, N.; School of Engineering, Pablo de Olavide University, Crta. Utrera km 1, Spain",Conference Paper,"Final",,Scopus,2-s2.0-84910024548
"Shimosaka M., Kaneko T., Nishi K.","7003853628;55421217800;56723495100;","Modeling risk anticipation and defensive driving on residential roads with inverse reinforcement learning",2014,"2014 17th IEEE International Conference on Intelligent Transportation Systems, ITSC 2014",,, 6957937,"1694","1700",,13,"10.1109/ITSC.2014.6957937","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84937150950&doi=10.1109%2fITSC.2014.6957937&partnerID=40&md5=8a4c58b7b574c3618864ffdadec91a53","Department of Mechano-Informatics, Graduate School of Information Science and Technology, University of Tokyo, Tokyo, 113-8656, Japan","Shimosaka, M., Department of Mechano-Informatics, Graduate School of Information Science and Technology, University of Tokyo, Tokyo, 113-8656, Japan; Kaneko, T., Department of Mechano-Informatics, Graduate School of Information Science and Technology, University of Tokyo, Tokyo, 113-8656, Japan; Nishi, K., Department of Mechano-Informatics, Graduate School of Information Science and Technology, University of Tokyo, Tokyo, 113-8656, Japan",,"Accidents; Active safety systems; Behavioral research; Hidden Markov models; Housing; Intelligent systems; Inverse problems; Learning algorithms; Markov processes; Motion planning; Safety engineering; Transportation; Feature descriptors; Inverse reinforcement learning; Long-term prediction; Markov Decision Processes; Maximum entropy Markov model; Pedestrian detection; Physical limitations; Reducing traffic accidents; Reinforcement learning",,Conference Paper,"Final",,Scopus,2-s2.0-84937150950
"Das S., Lavoie A.","55476999400;55295620500;","The effects of feedback on human behavior in social media: An inverse reinforcement learning model",2014,"13th International Conference on Autonomous Agents and Multiagent Systems, AAMAS 2014","1",,,"653","660",,9,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84911426890&partnerID=40&md5=9410899641a02cca87887f0e3b436adc","Washington University, St. Louis, United States","Das, S., Washington University, St. Louis, United States; Lavoie, A., Washington University, St. Louis, United States","Multi-agent learning; Social media; Social simulation",,,Conference Paper,"Final",,Scopus,2-s2.0-84911426890
"Bogert K., Doshi P.","36095803300;23008336000;","Multi-robot inverse reinforcement learning under occlusion with interactions",2014,"13th International Conference on Autonomous Agents and Multiagent Systems, AAMAS 2014","1",,,"173","180",,8,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84911432424&partnerID=40&md5=c339192e4d608b2bd420cedecf1eb5ca","Computer Science Department, University of Georgia, United States","Bogert, K., Computer Science Department, University of Georgia, United States; Doshi, P., Computer Science Department, University of Georgia, United States","Inverse reinforcement; Machine learning; Multi-robot systems; Patrolling",,,Conference Paper,"Final",,Scopus,2-s2.0-84911432424
"Pynadath D.V., Rosenbloom P.S., Marsella S.C.","6602713215;6603896745;6603739353;","Reinforcement learning for adaptive theory of mind in the sigma cognitive architecture",2014,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","8598 LNAI",,,"143","154",,6,"10.1007/978-3-319-09274-4_14","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84905844071&doi=10.1007%2f978-3-319-09274-4_14&partnerID=40&md5=f18f885d0e0a4a9568a8dbe910f00cc7","Institute for Creative Technologies, University of Southern California, Los Angeles, CA, United States; Department of Computer Science, University of Southern California, Los Angeles, CA, United States; Northeastern University, Boston, MA, United States","Pynadath, D.V., Institute for Creative Technologies, University of Southern California, Los Angeles, CA, United States; Rosenbloom, P.S., Institute for Creative Technologies, University of Southern California, Los Angeles, CA, United States, Department of Computer Science, University of Southern California, Los Angeles, CA, United States; Marsella, S.C., Northeastern University, Boston, MA, United States",,"Behavioral research; Cognitive systems; Knowledge management; Multi agent systems; Cognitive architectures; Explicit modeling; Gradient descent algorithms; Human intelligence; Inverse reinforcement learning; Multi-agent learning; Multi-agent reinforcement learning; Social interactions; Reinforcement learning",,Conference Paper,"Final",,Scopus,2-s2.0-84905844071
"Ondrúška P., Posner I.","56308174300;23010022000;","The route not taken: Driver-centric estimation of electric vehicle range",2014,"Proceedings International Conference on Automated Planning and Scheduling, ICAPS","2014-January","January",,"413","420",,17,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84933050809&partnerID=40&md5=46b4fa6e96f048605ef4708bb7d219f6","Mobile Robotics Group, University of Oxford, United Kingdom","Ondrúška, P., Mobile Robotics Group, University of Oxford, United Kingdom; Posner, I., Mobile Robotics Group, University of Oxford, United Kingdom",,"Behavioral research; Decision making; Electric vehicles; Energy utilization; Forecasting; Markov processes; Reinforcement learning; Scheduling; Energy prediction; Inverse reinforcement learning; Markov Decision Processes; Policy evaluation; Real trajectories; Relative errors; Route preferences; Sequential decision making; Energy policy",,Conference Paper,"Final",,Scopus,2-s2.0-84933050809
"Chinaei H., Chaib-draa B.","23388191000;7004238886;","Dialogue POMDP components (Part II): learning the reward function",2014,"International Journal of Speech Technology","17","4",,"325","340",,1,"10.1007/s10772-014-9224-x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84911971999&doi=10.1007%2fs10772-014-9224-x&partnerID=40&md5=66a204a6a06477a304ca8bdc1ba89095","Department of Computer Science, Laval University, Québec, Canada","Chinaei, H., Department of Computer Science, Laval University, Québec, Canada; Chaib-draa, B., Department of Computer Science, Laval University, Québec, Canada","Healthcare dialogue management; Inverse reinforcement learning; Partially observable Markov decision processes (POMDP)","Approximation algorithms; Behavioral research; Health care; Learning algorithms; Markov processes; Speech processing; Dialogue management; Dialogue systems; Intelligent wheelchair; Inverse reinforcement learning; Linear approximations; Markov Decision Processes; Partially observable Markov decision process; Transition model; Reinforcement learning","Chaib-draa, B.; Department of Computer Science, Laval UniversityCanada",Article,"Final",,Scopus,2-s2.0-84911971999
"Jetchev N., Toussaint M.","14034161200;7006246144;","Discovering relevant task spaces using inverse feedback control",2014,"Autonomous Robots","37","2",,"169","189",,4,"10.1007/s10514-014-9384-1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84902287582&doi=10.1007%2fs10514-014-9384-1&partnerID=40&md5=acaa3d6a9248ed2d398b72262d330c52","FU Berlin, Berlin, Germany; University of Stuttgart, Stuttgart, Germany","Jetchev, N., FU Berlin, Berlin, Germany; Toussaint, M., University of Stuttgart, Stuttgart, Germany","Imitation learning; Inverse reinforcement learning; Machine learning and robotics; Motion rate control; Robot motion generation; Task spaces for motion","Artificial intelligence; Reinforcement learning; Robotics; Imitation learning; Inverse reinforcement learning; Motion rate control; Robot motion generations; Task space; Feedback control","Jetchev, N.; FU Berlin, Berlin, Germany; email: nikjetchev@gmail.com",Article,"Final",,Scopus,2-s2.0-84902287582
"Oswald S., Kretzschmar H., Burgard W., Stachniss C.","56640890200;34168016900;7003610380;6507517732;","Learning to give route directions from human demonstrations",2014,"Proceedings - IEEE International Conference on Robotics and Automation",,, 6907334,"3303","3308",,6,"10.1109/ICRA.2014.6907334","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84929223719&doi=10.1109%2fICRA.2014.6907334&partnerID=40&md5=57f88279f8012446c263a065049955e6","Department of Computer Science, University of Freiburg, Freiburg, 79110, Germany; University of Bonn, Institute of Geodesy and Geoinformation, Bonn, 53115, Germany","Oswald, S., Department of Computer Science, University of Freiburg, Freiburg, 79110, Germany; Kretzschmar, H., Department of Computer Science, University of Freiburg, Freiburg, 79110, Germany; Burgard, W., Department of Computer Science, University of Freiburg, Freiburg, 79110, Germany; Stachniss, C., Department of Computer Science, University of Freiburg, Freiburg, 79110, Germany, University of Bonn, Institute of Geodesy and Geoinformation, Bonn, 53115, Germany",,,,Conference Paper,"Final",,Scopus,2-s2.0-84929223719
"Piot B., Geist M., Pietquin O.","55697434100;25929145100;16040586900;","Boosted and reward-regularized classification for apprenticeship learning",2014,"13th International Conference on Autonomous Agents and Multiagent Systems, AAMAS 2014","2",,,"1249","1256",,6,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84911437588&partnerID=40&md5=8afbd7ea74fd484a395c5ef67f6268b7","Supelec, MaLIS Research Group, France; Georgia Tech., CNRS UMI 2958, France; University Lille 1, France; LIFL (UMR 8022 CNRS/Lille 1), SequeL Team, France","Piot, B., Supelec, MaLIS Research Group, France, Georgia Tech., CNRS UMI 2958, France; Geist, M., Supelec, MaLIS Research Group, France, Georgia Tech., CNRS UMI 2958, France; Pietquin, O., University Lille 1, France, LIFL (UMR 8022 CNRS/Lille 1), SequeL Team, France","Boosting; Inverse reinforcement learning; Large margin methods; Learning from demonstrations",,,Conference Paper,"Final",,Scopus,2-s2.0-84911437588
"Osogami T., Raymond R.","10042517800;23393816800;","Map matching with inverse reinforcement learning",2013,"IJCAI International Joint Conference on Artificial Intelligence",,,,"2547","2553",,21,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84896061924&partnerID=40&md5=530c6fc9da49dc759f450d2de9582ebb","IBM Research-Tokyo, 5-6-52 Toyosu, Koto-ku, Tokyo, Japan","Osogami, T., IBM Research-Tokyo, 5-6-52 Toyosu, Koto-ku, Tokyo, Japan; Raymond, R., IBM Research-Tokyo, 5-6-52 Toyosu, Koto-ku, Tokyo, Japan",,"Inverse reinforcement learning; Map matching; Numerical experiments; Road segments; State-of-the-art approach; Transition probabilities; Travel distance; Artificial intelligence; Hidden Markov models; Reinforcement learning; Roads and streets; Probability","IBM Research-Tokyo, 5-6-52 Toyosu, Koto-ku, Tokyo, Japan",Conference Paper,"Final",,Scopus,2-s2.0-84896061924
"Tanwani A.K., Billard A.","27868177900;7006389948;","Transfer in inverse reinforcement learning for multiple strategies",2013,"IEEE International Conference on Intelligent Robots and Systems",,, 6696817,"3244","3250",,6,"10.1109/IROS.2013.6696817","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893761567&doi=10.1109%2fIROS.2013.6696817&partnerID=40&md5=00abbe6cac2b01e6e31d6d03539b3922","Learning Algorithms and Systems Laboratory (LASA), Ecole Polytechnique Federale de Lausanne, Switzerland","Tanwani, A.K., Learning Algorithms and Systems Laboratory (LASA), Ecole Polytechnique Federale de Lausanne, Switzerland; Billard, A., Learning Algorithms and Systems Laboratory (LASA), Ecole Polytechnique Federale de Lausanne, Switzerland",,"Inverse reinforcement learning; Linear combinations; Multiple strategy; Optimal policies; Playing Strategies; Reward function; Robot arms; Sequential task; Intelligent robots; Reinforcement learning; Set theory; Optimization","Learning Algorithms and Systems Laboratory (LASA), Ecole Polytechnique Federale de LausanneSwitzerland",Conference Paper,"Final",,Scopus,2-s2.0-84893761567
"Tossou A.C.Y., Dimitrakakis C.","55936518300;55886345900;","Probabilistic inverse reinforcement learning in unknown environments",2013,"Uncertainty in Artificial Intelligence - Proceedings of the 29th Conference, UAI 2013",,,,"635","643",,2,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84888166253&partnerID=40&md5=3e86bcb775de98bd76f2ca7ca5f1fecd","EPAC, Abomey-Calavi, Benin; EPFL, Lausanne, Switzerland","Tossou, A.C.Y., EPAC, Abomey-Calavi, Benin; Dimitrakakis, C., EPFL, Lausanne, Switzerland",,"Agent preferences; Bayesian inference; Convex optimisation; Inverse reinforcement learning; Learning by demonstration; Maximum a posteriori estimation; Probabilistic approaches; Probabilistic models; Artificial intelligence; Bayesian networks; Inference engines; Reinforcement learning","EPAC, Abomey-Calavi, Benin",Conference Paper,"Final",,Scopus,2-s2.0-84888166253
"Kalakrishnan M., Pastor P., Righetti L., Schaal S.","24467757900;57197413733;22981433900;7007171558;","Learning objective functions for manipulation",2013,"Proceedings - IEEE International Conference on Robotics and Automation",,, 6630743,"1331","1336",,45,"10.1109/ICRA.2013.6630743","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84887294616&doi=10.1109%2fICRA.2013.6630743&partnerID=40&md5=576de078e46efcf4d167d1433d576c7f","CLMC Lab, University of Southern California, Los Angeles CA 90089, United States; Max Planck Institute for Intelligent Systems, Tübingen, 72076, Germany","Kalakrishnan, M., CLMC Lab, University of Southern California, Los Angeles CA 90089, United States; Pastor, P., CLMC Lab, University of Southern California, Los Angeles CA 90089, United States; Righetti, L., CLMC Lab, University of Southern California, Los Angeles CA 90089, United States, Max Planck Institute for Intelligent Systems, Tübingen, 72076, Germany; Schaal, S., CLMC Lab, University of Southern California, Los Angeles CA 90089, United States, Max Planck Institute for Intelligent Systems, Tübingen, 72076, Germany",,"Continuous state-action spaces; Convex objectives; Grasping and manipulation; High-dimensional; Inverse reinforcement learning; Learning objective functions; Redundancy resolution; Robotic manipulation; Algorithms; Cost functions; Inverse kinematics; Optimization; Reinforcement learning; Robotics","CLMC Lab, University of Southern California, Los Angeles CA 90089, United States",Conference Paper,"Final",,Scopus,2-s2.0-84887294616
"Michini B., Cutler M., How J.P.","36603096200;55540436700;7006512768;","Scalable reward learning from demonstration",2013,"Proceedings - IEEE International Conference on Robotics and Automation",,, 6630592,"303","308",,11,"10.1109/ICRA.2013.6630592","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84887309045&doi=10.1109%2fICRA.2013.6630592&partnerID=40&md5=11485c155db550b1d94b7d3010f9593c","Aerospace Controls Laboratory, Massachusetts Institute of Technology, United States","Michini, B., Aerospace Controls Laboratory, Massachusetts Institute of Technology, United States; Cutler, M., Aerospace Controls Laboratory, Massachusetts Institute of Technology, United States; How, J.P., Aerospace Controls Laboratory, Massachusetts Institute of Technology, United States",,"Action spaces; Discretizations; Inverse reinforcement learning; Key modifications; Learning from demonstration; Markov Decision Processes; Non-parametric; Optimal value functions; Learning algorithms; Markov processes; Optimal systems; Reinforcement learning; Robotics","Aerospace Controls Laboratory, Massachusetts Institute of TechnologyUnited States",Conference Paper,"Final",,Scopus,2-s2.0-84887309045
"Englert P., Paraschos A., Peters J., Deisenroth M.P.","55855227200;55336427400;35248912800;24922891000;","Model-based imitation learning by probabilistic trajectory matching",2013,"Proceedings - IEEE International Conference on Robotics and Automation",,, 6630832,"1922","1927",,20,"10.1109/ICRA.2013.6630832","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84887311623&doi=10.1109%2fICRA.2013.6630832&partnerID=40&md5=02a762475414abb5e7f4eca24a590d4d","Dept. of Computer Science, Technische Universität Darmstadt, Germany; Max Planck Institute for Intelligent Systems, Tübingen, Germany","Englert, P., Dept. of Computer Science, Technische Universität Darmstadt, Germany; Paraschos, A., Dept. of Computer Science, Technische Universität Darmstadt, Germany; Peters, J., Dept. of Computer Science, Technische Universität Darmstadt, Germany, Max Planck Institute for Intelligent Systems, Tübingen, Germany; Deisenroth, M.P., Dept. of Computer Science, Technische Universität Darmstadt, Germany",,"Behavioral cloning; Function learning; Imitation learning; Inverse reinforcement learning; Kullback Leibler divergence; Model-based reinforcement learning; Non-trivial tasks; Trajectory matching; Cost functions; Probability distributions; Reinforcement learning; Robots; Trajectories; Robotics","Dept. of Computer Science, Technische Universität DarmstadtGermany",Conference Paper,"Final",,Scopus,2-s2.0-84887311623
[No author name available],[No author id available],"Machine Learning and Knowledge Discovery in Databases - European Conference, ECML PKDD 2013, Proceedings",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","8188 LNAI","PART 1",,"","",2063,,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84886531190&partnerID=40&md5=105f8e158f5aba4a9175359d407bb8f9",,"",,,,Conference Review,"Final",,Scopus,2-s2.0-84886531190
[No author name available],[No author id available],"Machine Learning and Knowledge Discovery in Databases - European Conference, ECML PKDD 2013, Proceedings",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","8190 LNAI","PART 3",,"","",2063,,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84886474154&partnerID=40&md5=7605c5ec9d5a425c9506ad3bf511b50a",,"",,,,Conference Review,"Final",,Scopus,2-s2.0-84886474154
[No author name available],[No author id available],"Machine Learning and Knowledge Discovery in Databases - European Conference, ECML PKDD 2013, Proceedings",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","8189 LNAI","PART 2",,"","",2063,,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84886451399&partnerID=40&md5=6c1f4b95ef2f206e6f61966047991f4a",,"",,,,Conference Review,"Final",,Scopus,2-s2.0-84886451399
"Klein E., Piot B., Geist M., Pietquin O.","55234823100;55697434100;25929145100;16040586900;","A cascaded supervised learning approach to inverse reinforcement learning",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","8188 LNAI","PART 1",,"1","16",,17,"10.1007/978-3-642-40988-2_1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84886548664&doi=10.1007%2f978-3-642-40988-2_1&partnerID=40&md5=48491f0d52609e1d78c2f9971bd28de4","ABC Team, LORIA-CNRS, France; Supélec, IMS-MaLIS Research Group, France; UMI 2958 (GeorgiaTech-CNRS), France","Klein, E., ABC Team, LORIA-CNRS, France, Supélec, IMS-MaLIS Research Group, France; Piot, B., Supélec, IMS-MaLIS Research Group, France, UMI 2958 (GeorgiaTech-CNRS), France; Geist, M., Supélec, IMS-MaLIS Research Group, France; Pietquin, O., Supélec, IMS-MaLIS Research Group, France, UMI 2958 (GeorgiaTech-CNRS), France",,"Driving simulator; Inverse reinforcement learning; Markov Decision Processes; Near-optimal; Reward function; Score function; State-of-the-art approach; Supervised learning approaches; Learning algorithms; Markov processes; Optimization; Reinforcement learning; Supervised learning; Inverse problems","ABC Team, LORIA-CNRSFrance",Conference Paper,"Final",,Scopus,2-s2.0-84886548664
"Qiao Q., Beling P.A.","35848809400;6603732790;","Recognition of agents based on observation of their sequential behavior",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","8188 LNAI","PART 1",,"33","48",,2,"10.1007/978-3-642-40988-2_3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84886474556&doi=10.1007%2f978-3-642-40988-2_3&partnerID=40&md5=c5e9a03405d8e3590bd486e8bd0062d0","Department of Systems Engineering, University of Virginia, VA, United States","Qiao, Q., Department of Systems Engineering, University of Virginia, VA, United States; Beling, P.A., Department of Systems Engineering, University of Virginia, VA, United States",,"Classification models; Empirical - comparisons; Inverse reinforcement learning; Learning scenarios; Markov Decision Processes; Navigation problem; Optimal stopping problem; Sequential decisions; Learning algorithms; Markov processes; Optimization; Reinforcement learning","Department of Systems Engineering, University of Virginia, VA, United States",Conference Paper,"Final",Open Access,Scopus,2-s2.0-84886474556
"Klein E., Piot B., Geist M., Pietquin O.","55234823100;55697434100;25929145100;16040586900;","Structured classification for inverse reinforcement learning [Classification structurée pour l'apprentissage par renforcement inverse]",2013,"Revue d'Intelligence Artificielle","27","2",,"155","169",,,"10.3166/RIA.27.155-169","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84885946141&doi=10.3166%2fRIA.27.155-169&partnerID=40&md5=b30444dfc2d0b47907adcfb6c1b01ea9","LORIA Équipe ABC, Nancy, France; Supélec Groupe de Recherche IMS-MaLIS, Metz, France; UMI 2958 (GeorgiaTech-CNRS), Metz, France","Klein, E., LORIA Équipe ABC, Nancy, France, Supélec Groupe de Recherche IMS-MaLIS, Metz, France; Piot, B., Supélec Groupe de Recherche IMS-MaLIS, Metz, France, UMI 2958 (GeorgiaTech-CNRS), Metz, France; Geist, M., Supélec Groupe de Recherche IMS-MaLIS, Metz, France; Pietquin, O., Supélec Groupe de Recherche IMS-MaLIS, Metz, France, UMI 2958 (GeorgiaTech-CNRS), Metz, France","Inverse reinforcement learning; Reinforcement learning","Car driving; Classification structure; Inverse reinforcement learning; Near-optimal; Reward function; Score function; Algorithms; Optimization; Reinforcement learning; Inverse problems","LORIA Équipe ABC, Nancy, France",Article,"Final",,Scopus,2-s2.0-84885946141
"Tao Z., Chen Z., Li Y.","55968518700;55967386500;55719284400;","Sensitivity-based inverse reinforcement learning",2013,"Chinese Control Conference, CCC",,, 6639909,"2856","2861",,1,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84890488073&partnerID=40&md5=9683f22b9ea73b1bd683b37b2dc6bbdc","Harbin Institute of Technology, Shenzhen Graduate School, Harbin 518055, China","Tao, Z., Harbin Institute of Technology, Shenzhen Graduate School, Harbin 518055, China; Chen, Z., Harbin Institute of Technology, Shenzhen Graduate School, Harbin 518055, China; Li, Y., Harbin Institute of Technology, Shenzhen Graduate School, Harbin 518055, China","Inverse Reinforcement Learning; Performance sensitivity; Reward function","Markov processes; Site selection; Inverse reinforcement learning; Markov Decision Processes; Optimal control policy; Optimization theory; Performance sensitivity; Potential rewards; Reward function; Unified approach; Reinforcement learning",,Conference Paper,"Final",,Scopus,2-s2.0-84890488073
"Englert P., Paraschos A., Deisenroth M.P., Peters J.","55855227200;55336427400;24922891000;35248912800;","Probabilistic model-based imitation learning",2013,"Adaptive Behavior","21","5",,"388","403",,26,"10.1177/1059712313491614","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84884155630&doi=10.1177%2f1059712313491614&partnerID=40&md5=d6ac74c3cd95313e1b1aa5c432cf8a1f","Department of Computer Science, Technische Universität Darmstadt, Germany; Max Planck Institute for Intelligent Systems, Tübingen, Germany","Englert, P., Department of Computer Science, Technische Universität Darmstadt, Germany; Paraschos, A., Department of Computer Science, Technische Universität Darmstadt, Germany; Deisenroth, M.P., Department of Computer Science, Technische Universität Darmstadt, Germany; Peters, J., Department of Computer Science, Technische Universität Darmstadt, Germany, Max Planck Institute for Intelligent Systems, Tübingen, Germany",,,"Englert, P.; Department of Computer Science, Technische Universität DarmstadtGermany; email: englert@ias.tu-darmstadt.de",Article,"Final",,Scopus,2-s2.0-84884155630
[No author name available],[No author id available],"Proceedings of the 2nd Workshop on Machine Learning for Interactive Systems: Bridging the Gap Between Perception, Action and Communication, MLIS 2013 - 23rd IJCAI 2013",2013,"ACM International Conference Proceeding Series",,,,"","",81,,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84883008795&partnerID=40&md5=7407a17bb4138e08cfae3b85c4445026",,"",,,,Conference Review,"Final",,Scopus,2-s2.0-84883008795
"Pietquin O.","16040586900;","Inverse reinforcement learning for interactive systems",2013,"ACM International Conference Proceeding Series",,,,"71","75",,2,"10.1145/2493525.2493529","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84883034035&doi=10.1145%2f2493525.2493529&partnerID=40&md5=00e70e1c2c9b2ac1d5885debdd12c24f","SUPELEC - UMI 2958 (GeorgiaTech-CNRS), 2 Rue Edouard Belin, 57070 Metz, France","Pietquin, O., SUPELEC - UMI 2958 (GeorgiaTech-CNRS), 2 Rue Edouard Belin, 57070 Metz, France","Human-machine interaction; Reinforcement learning","Human activity recognition; Human machine interaction; Interaction managers; Interactive system; Inverse reinforcement learning; Machine learning techniques; Natural language generation; Sequential decision making; Algorithms; Artificial intelligence; Communication; Human computer interaction; Reinforcement learning","Pietquin, O.; SUPELEC - UMI 2958 (GeorgiaTech-CNRS), 2 Rue Edouard Belin, 57070 Metz, France; email: olivier.pietquin@supelec.fr",Conference Paper,"Final",,Scopus,2-s2.0-84883034035
"Iturrate I., Omedes J., Montesano L.","57194101260;55567220200;6602849539;","Shared control of a robot using EEG-based feedback Signals",2013,"ACM International Conference Proceeding Series",,,,"45","50",,4,"10.1145/2493525.2493533","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84882932193&doi=10.1145%2f2493525.2493533&partnerID=40&md5=fe837c5b62fab17f55d05e02a4a1a7ed","I3A, DIIS, EINA, University of Zaragoza, Zaragoza, Spain","Iturrate, I., I3A, DIIS, EINA, University of Zaragoza, Zaragoza, Spain; Omedes, J., I3A, DIIS, EINA, University of Zaragoza, Zaragoza, Spain; Montesano, L., I3A, DIIS, EINA, University of Zaragoza, Zaragoza, Spain",,"Binary information; Continuous state; Feedback signal; Graphical interface; Human observations; Inverse reinforcement learning; Multiple modalities; Robot operations; Artificial intelligence; Communication; Reinforcement learning; Robots","I3A, DIIS, EINA, University of Zaragoza, Zaragoza, Spain",Conference Paper,"Final",,Scopus,2-s2.0-84882932193
"Li Y., Zhu Y., Yang F., Jia Q.","36835429600;8855113900;56408791400;47761289100;","Inverse reinforcement learning based optimal schedule generation approach for carrier aircraft on flight deck",2013,"Guofang Keji Daxue Xuebao/Journal of National University of Defense Technology","35","4",,"171","175",,5,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84886303335&partnerID=40&md5=b98228ae550b28d8d9c62029f36b91f0","College of Information and System and Management, National University of Defense Technology, Changsha 410073, China","Li, Y., College of Information and System and Management, National University of Defense Technology, Changsha 410073, China; Zhu, Y., College of Information and System and Management, National University of Defense Technology, Changsha 410073, China; Yang, F., College of Information and System and Management, National University of Defense Technology, Changsha 410073, China; Jia, Q., College of Information and System and Management, National University of Defense Technology, Changsha 410073, China","Aircraft scheduling on flight deck; Inverse reinforcement learning; Optimal schedule generation; Reinforcement learning","Aircraft operations; Aircraft scheduling; Flight decks; Inverse reinforcement learning; Markov Decision Processes; Optimal policies; Optimal schedule; Scheduling optimization; Decision making; Markov processes; Optimization; Reinforcement learning; Scheduling; Aircraft","Zhu, Y.; College of Information and System and Management, National University of Defense Technology, Changsha 410073, China; email: nudtzyf@hotmail.com",Article,"Final",,Scopus,2-s2.0-84886303335
"Murray W.R., Harrison P., Singliar T.","57202909075;23094014700;15043013000;","Interpreting spatiotemporal expressions from english to fuzzy logic",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","8035",,,"226","233",,,"10.1007/978-3-642-39617-5_21","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880982771&doi=10.1007%2f978-3-642-39617-5_21&partnerID=40&md5=7836a29c16a2f842bc87f6d06d0b4bcd","Boeing Research and Technology, PO Box 3707, Seattle, WA 98124, United States","Murray, W.R., Boeing Research and Technology, PO Box 3707, Seattle, WA 98124, United States; Harrison, P., Boeing Research and Technology, PO Box 3707, Seattle, WA 98124, United States; Singliar, T., Boeing Research and Technology, PO Box 3707, Seattle, WA 98124, United States","anomaly detection; controlled natural language; CPL; fuzzy logic; GPSG parsing; inverse reinforcement learning; spatiotemporal reasoning","Computer circuits; Computer programming languages; Context free grammars; Learning algorithms; Learning systems; Reinforcement learning; Syntactics; Web crawler; Anomaly detection; Controlled natural language; GPSG parsing; Inverse reinforcement learning; Spatio-temporal reasoning; Fuzzy logic",,Conference Paper,"Final",,Scopus,2-s2.0-84880982771
"Dragan A.D., Srinivasa S.S.","55193779100;6602084313;","A policy-blending formalism for shared control",2013,"International Journal of Robotics Research","32","7",,"790","805",,100,"10.1177/0278364913490324","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84879894559&doi=10.1177%2f0278364913490324&partnerID=40&md5=80d1d854125bbaade4cd3aff819dc319","Robotics Institute, Carnegie Mellon University, United States","Dragan, A.D., Robotics Institute, Carnegie Mellon University, United States; Srinivasa, S.S., Robotics Institute, Carnegie Mellon University, United States","arbitration; human-robot collaboration; intent prediction; shared control; sliding autonomy; teleoperation","arbitration; Human-robot collaboration; Inverse reinforcement learning; Prediction problem; Robotic manipulators; Shared control; Simplifying assumptions; Sliding autonomy; Blending; Forecasting; Inverse problems; Reinforcement learning; Remote control; Robots; Behavioral research","Dragan, A.D.; Robotics Institute, Carnegie Mellon UniversityUnited States; email: adragan@cs.cmu.edu",Conference Paper,"Final",,Scopus,2-s2.0-84879894559
"Boularias A., Chaib-draa B.","23090275300;7004238886;","Apprenticeship learning with few examples",2013,"Neurocomputing","104",,,"83","96",,3,"10.1016/j.neucom.2012.11.002","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873703578&doi=10.1016%2fj.neucom.2012.11.002&partnerID=40&md5=8b2c44c096ea60e6286e597a604461d4","Max Planck Institute for Intelligent Systems, Spemannstraße 41, 72076 Tuebingen, Germany; Computer Science and Software Engineering Department, Laval University, Quebec G1V 0A6, Canada","Boularias, A., Max Planck Institute for Intelligent Systems, Spemannstraße 41, 72076 Tuebingen, Germany; Chaib-draa, B., Computer Science and Software Engineering Department, Laval University, Quebec G1V 0A6, Canada","Bootstrapping; Imitation learning; Inverse reinforcement learning; Transfer learning","Apprenticeship learning; AS graph; Bootstrapping; Empirical averages; Imitation learning; Inverse reinforcement learning; Linear combinations; Near-optimal; New approaches; Optimal policies; Reward function; Simulated robot; State space; Transfer learning; Transfer technique; Value functions; Dynamics; Learning algorithms; Optimization; Reinforcement learning; Apprentices; algorithm; analytical error; apprenticeship learning; article; bootstrapping; dynamics; imitation; learning; priority journal; reinforcement; reward; robotics; simulation; statistical analysis","Boularias, A.; Max Planck Institute for Intelligent Systems, Spemannstraße 41, 72076 Tuebingen, Germany; email: abdeslam.boularias@tuebingen.mpg.de",Article,"Final",,Scopus,2-s2.0-84873703578
"Ziebart B.D., Bagnell J.A., Dey A.K.","13608719300;6506127486;7101701731;","The principle of maximum causal entropy for estimating interacting processes",2013,"IEEE Transactions on Information Theory","59","4", 6479340,"1966","1980",,20,"10.1109/TIT.2012.2234824","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84896690184&doi=10.1109%2fTIT.2012.2234824&partnerID=40&md5=319b27f0a8c54dd29611d5c16d18a24b","Department of Computer Science, University of Illinois at Chicago, Chicago, IL 60607, United States; School of Computer Science, Carnegie Mellon University, Pittsburgh, PA 15213, United States","Ziebart, B.D., Department of Computer Science, University of Illinois at Chicago, Chicago, IL 60607, United States; Bagnell, J.A., School of Computer Science, Carnegie Mellon University, Pittsburgh, PA 15213, United States; Dey, A.K., School of Computer Science, Carnegie Mellon University, Pittsburgh, PA 15213, United States","Causal entropy; Correlated equilibrium (CE); Directed information; Inverse optimal control; Inverse reinforcement learning; Maximum entropy; Statistical estimation","Computer games; Entropy; Maximum entropy methods; Maximum principle; Probability distributions; Reinforcement learning; Correlated equilibria; Directed information; Inverse reinforcement learning; Inverse-optimal control; Statistical estimation; Estimation",,Article,"Final",,Scopus,2-s2.0-84896690184
"Sugiyama H., Meguro T., Minami Y.","56965179800;36632749100;24428386300;","Preference-learning based inverse reinforcement learning for dialog control",2012,"13th Annual Conference of the International Speech Communication Association 2012, INTERSPEECH 2012","1",,,"222","225",,8,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878412864&partnerID=40&md5=387fa005a872733b7eaabfec1556ca53","NTT Communication Science Laboratories, Kyoto, Japan","Sugiyama, H., NTT Communication Science Laboratories, Kyoto, Japan; Meguro, T., NTT Communication Science Laboratories, Kyoto, Japan; Minami, Y., NTT Communication Science Laboratories, Kyoto, Japan",,"Competitive algorithms; Dialog controls; Dialog systems; Inverse reinforcement learning; Reward function; Algorithms; Reinforcement learning","NTT Communication Science Laboratories, Kyoto, Japan",Conference Paper,"Final",,Scopus,2-s2.0-84878412864
"Reddy T.S., Gopikrishna V., Zaruba G., Huber M.","55557204500;35322408300;6602515225;7202671760;","Inverse reinforcement learning for decentralized non-cooperative multiagent systems",2012,"Conference Proceedings - IEEE International Conference on Systems, Man and Cybernetics",,, 6378020,"1930","1935",,5,"10.1109/ICSMC.2012.6378020","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84872422561&doi=10.1109%2fICSMC.2012.6378020&partnerID=40&md5=dc588a31fe74bea836cbc64a063d757c","Computer Science Department, University of Texas at Arlington, Arlington, TX, United States","Reddy, T.S., Computer Science Department, University of Texas at Arlington, Arlington, TX, United States; Gopikrishna, V., Computer Science Department, University of Texas at Arlington, Arlington, TX, United States; Zaruba, G., Computer Science Department, University of Texas at Arlington, Arlington, TX, United States; Huber, M., Computer Science Department, University of Texas at Arlington, Arlington, TX, United States","Game Theory; General-Sum Stochastic Games; Inverse Reinfocement Learning; Multiagent Systems; Nash equilibrium","Distributed solutions; Inverse Reinfocement Learning; Inverse reinforcement learning; Multi-agent problems; Multi-agent setting; Nash equilibria; Non-cooperative; Optimal policies; Reward function; Single-agent; Stochastic game; Cybernetics; Game theory; Reinforcement learning; Multi agent systems","Reddy, T.S.; Computer Science Department, University of Texas at Arlington, Arlington, TX, United States; email: sudhamsh@uta.edu",Conference Paper,"Final",,Scopus,2-s2.0-84872422561
"Klein E., Geist M., Piot B., Pietquin O.","55234823100;25929145100;55697434100;16040586900;","Inverse reinforcement learning through structured classification",2012,"Advances in Neural Information Processing Systems","2",,,"1007","1015",,29,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84877756695&partnerID=40&md5=27f1bed54aa7e7a1fd17e16183d6398c","LORIA, Team ABC, Nancy, France; Supélec, IMS-MaLIS Research Group, Metz, France; UMI 2958 (GeorgiaTech-CNRS), Metz, France","Klein, E., LORIA, Team ABC, Nancy, France, Supélec, IMS-MaLIS Research Group, Metz, France; Geist, M., Supélec, IMS-MaLIS Research Group, Metz, France; Piot, B., Supélec, IMS-MaLIS Research Group, Metz, France, UMI 2958 (GeorgiaTech-CNRS), Metz, France; Pietquin, O., Supélec, IMS-MaLIS Research Group, Metz, France, UMI 2958 (GeorgiaTech-CNRS), Metz, France",,"Car driving; Inverse reinforcement learning; Multi-class classifier; Near-optimal; Reward function; Score function; Algorithms; Optimization; Reinforcement learning; Inverse problems","LORIA, Team ABC, Nancy, France",Conference Paper,"Final",,Scopus,2-s2.0-84877756695
"Singliar T., Margineantu D.D.","15043013000;9738431900;","Scalable inverse reinforcement learning via instructed feature construction",2012,"AAAI Workshop - Technical Report","WS-12-11",,,"33","35",,,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84875583424&partnerID=40&md5=061580a7cddf361b2384a6a52270e7ae","Boeing Research and Technology, M/C 7L-44, P.O. Box 3707, Seattle, WA 98124-2207, United States","Singliar, T., Boeing Research and Technology, M/C 7L-44, P.O. Box 3707, Seattle, WA 98124-2207, United States; Margineantu, D.D., Boeing Research and Technology, M/C 7L-44, P.O. Box 3707, Seattle, WA 98124-2207, United States",,"Basis functions; Feature construction; Inverse reinforcement learning; Limited rationality; Reward function; Traditional models; Value function approximation; Value functions; Stochastic models; Reinforcement learning","Singliar, T.; Boeing Research and Technology, M/C 7L-44, P.O. Box 3707, Seattle, WA 98124-2207, United States; email: tomas.singliar@boeing.com",Conference Paper,"Final",,Scopus,2-s2.0-84875583424
"MacGlashan J., Babeş-Vroman M., Winner K., Gao R., Adjogah R., DesJardins M., Littman M., Muresan S.","21934008200;52263115900;57204815305;55635873200;55635986700;10239052700;7006510438;57206143958;","Learning to interpret natural language instructions",2012,"AAAI Workshop - Technical Report","WS-12-07",,,"18","24",,1,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84875609319&partnerID=40&md5=b5eb86a0d028a936a679982f702f51fb","Department of Computer Science and Electrical Engineering, University of Maryland, Baltimore County, MD, United States; Computer Science Department, Rutgers University, United States; School of Communication and Information, Rutgers University, United States","MacGlashan, J., Department of Computer Science and Electrical Engineering, University of Maryland, Baltimore County, MD, United States; Babeş-Vroman, M., Computer Science Department, Rutgers University, United States; Winner, K., Department of Computer Science and Electrical Engineering, University of Maryland, Baltimore County, MD, United States; Gao, R., Computer Science Department, Rutgers University, United States; Adjogah, R., Department of Computer Science and Electrical Engineering, University of Maryland, Baltimore County, MD, United States; DesJardins, M., Department of Computer Science and Electrical Engineering, University of Maryland, Baltimore County, MD, United States; Littman, M., Computer Science Department, Rutgers University, United States; Muresan, S., School of Communication and Information, Rutgers University, United States",,"Artificial agents; Domain specific; Goal definitions; Inverse reinforcement learning; Natural languages; Reward function; Semantic parsing; Three component; Reinforcement learning; Semantics; Natural language processing systems","MacGlashan, J.; Department of Computer Science and Electrical Engineering, University of Maryland, Baltimore County, MD, United States",Conference Paper,"Final",,Scopus,2-s2.0-84875609319
"Vogel A., Ramachandran D., Gupta R., Raux A.","36945583200;56247655000;56537374400;14018300800;","Improving hybrid vehicle fuel efficiency using inverse reinforcement learning",2012,"Proceedings of the National Conference on Artificial Intelligence","1",,,"384","390",,15,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84868272761&partnerID=40&md5=de0776a2574cfbc32a41ebf907744570","Stanford University, United States; Honda Research Institute (USA) Inc., United States","Vogel, A., Stanford University, United States; Ramachandran, D., Honda Research Institute (USA) Inc., United States; Gupta, R., Honda Research Institute (USA) Inc., United States; Raux, A., Honda Research Institute (USA) Inc., United States",,"Battery power; Driver behavior; Fuel efficiency; Fuel savings; Hardware modifications; Hybrid controls; Inverse reinforcement learning; Optimal mixes; Power efficiency; Power requirement; Prediction systems; Artificial intelligence; Fuel economy; Fuels; Hybrid vehicles; Land vehicle propulsion; Optimization; Reinforcement learning; Efficiency","Vogel, A.; Stanford UniversityUnited States; email: av@cs.stanford.edu",Conference Paper,"Final",,Scopus,2-s2.0-84868272761
"Levine S., Koltun V.","35731728100;7004337914;","Continuous inverse optimal control with locally optimal examples",2012,"Proceedings of the 29th International Conference on Machine Learning, ICML 2012","1",,,"41","48",,65,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867123291&partnerID=40&md5=737595bfb1fe409ba7278928432d0b82","Computer Science Department, Stanford University, Stanford, CA 94305, United States","Levine, S., Computer Science Department, Stanford University, Stanford, CA 94305, United States; Koltun, V., Computer Science Department, Stanford University, Stanford, CA 94305, United States",,"Continuous domain; Inverse reinforcement learning; Inverse-optimal control; Local approximation; Local optimality; Markov Decision Processes; Optimal policies; Reward function; Inverse problems; Learning algorithms; Markov processes; Reinforcement learning; Optimization","Levine, S.; Computer Science Department, Stanford University, Stanford, CA 94305, United States; email: SVLEVINE@STANFORD.EDU",Conference Paper,"Final",,Scopus,2-s2.0-84867123291
"Akrour R., Schoenauer M., Sebag M.","49860946100;57198279938;57193617732;","APRIL: Active preference learning-based reinforcement learning",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","7524 LNAI","PART 2",,"116","131",,19,"10.1007/978-3-642-33486-3_8","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84866867863&doi=10.1007%2f978-3-642-33486-3_8&partnerID=40&md5=d1bdf7ba19bfc13cda6d2d77faa79e16","TAO, CNRS - INRIA - LRI, Université Paris-Sud, Orsay Cedex F-91405, France","Akrour, R., TAO, CNRS - INRIA - LRI, Université Paris-Sud, Orsay Cedex F-91405, France; Schoenauer, M., TAO, CNRS - INRIA - LRI, Université Paris-Sud, Orsay Cedex F-91405, France; Sebag, M., TAO, CNRS - INRIA - LRI, Université Paris-Sud, Orsay Cedex F-91405, France","interactive optimization; preference learning; reinforcement learning; robotics","Direct policy search; Human expert; Interactive optimization; Inverse reinforcement learning; Preference learning; Prior knowledge; Ranking queries; Reward function; RL framework; Swarm robotics; Robotics; Reinforcement learning","Akrour, R.; TAO, CNRS - INRIA - LRI, Université Paris-Sud, Orsay Cedex F-91405, France; email: Riad.Akrour@lri.fr",Conference Paper,"Final",Open Access,Scopus,2-s2.0-84866867863
"Klein E., Geist M., Pietquin O.","55234823100;25929145100;16040586900;","Batch, off-policy and model-free apprenticeship learning",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","7188 LNAI",,,"285","296",,4,"10.1007/978-3-642-29946-9_28","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84861679549&doi=10.1007%2f978-3-642-29946-9_28&partnerID=40&md5=ceaa3029aa9e912551823633d4e99cfb","Supélec, IMS Research Group, France; UMI 2958, GeorgiaTech-CNRS, France; Equipe ABC, LORIA-CNRS, France","Klein, E., Supélec, IMS Research Group, France, Equipe ABC, LORIA-CNRS, France; Geist, M., Supélec, IMS Research Group, France; Pietquin, O., Supélec, IMS Research Group, France, UMI 2958, GeorgiaTech-CNRS, France",,"Apprenticeship learning; Associated feature; Generative model; Inverse reinforcement learning; Learning control; Model free; Monte Carlo Simulation; Parameterized; Reward function; Temporal differences; Utility functions; Monte Carlo methods; Reinforcement learning; Apprentices","Klein, E.; Supélec, IMS Research GroupFrance; email: Edouard.Klein@supelec.fr",Conference Paper,"Final",,Scopus,2-s2.0-84861679549
"Chandramohan S., Geist M., Lefèvre F., Pietquin O.","55817552546;25929145100;56273617700;16040586900;","Behavior specific user simulation in spoken dialogue systems",2012,"Proceedings of 10th ITG Symposium on Speech Communication",,, 6309603,"","",,3,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84883047874&partnerID=40&md5=66394fa6e9a17b0af1a0d768a3194d9a","Supelec, IMS - MaLIS Research Group, France; UMI 2958 (CNRS - GeorgiaTech), France; Université d'Avignon et des Pays de Vaucluse, LIA-CERI, France","Chandramohan, S., Supelec, IMS - MaLIS Research Group, France, Université d'Avignon et des Pays de Vaucluse, LIA-CERI, France; Geist, M., Supelec, IMS - MaLIS Research Group, France; Lefèvre, F., Université d'Avignon et des Pays de Vaucluse, LIA-CERI, France; Pietquin, O., Supelec, IMS - MaLIS Research Group, France, UMI 2958 (CNRS - GeorgiaTech), France",,"Inverse problems; Linguistics; Reinforcement learning; Speech communication; Speech processing; Data requirements; Data-driven methods; Inverse reinforcement learning; Policy optimization; Spoken dialogue system; Spoken languages; State of the art; User simulation; Behavioral research",,Conference Paper,"Final",,Scopus,2-s2.0-84883047874
"Aghasadeghi N., Bretl T.","52763227100;6506270751;","Maximum entropy inverse reinforcement learning in continuous state spaces with path integrals",2011,"IEEE International Conference on Intelligent Robots and Systems",,, 6048804,"1561","1566",,22,"10.1109/IROS.2011.6048804","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84455160661&doi=10.1109%2fIROS.2011.6048804&partnerID=40&md5=a5ef44890dac4e25b0e0a2390ef7b425","Department of Electrical and Computer Engineering, University of Illinois at Urbana-Champaign, Urbana, IL 61801, United States; Department of Aerospace Engineering, University of Illinois at Urbana-Champaign, Urbana, IL 61801, United States","Aghasadeghi, N., Department of Electrical and Computer Engineering, University of Illinois at Urbana-Champaign, Urbana, IL 61801, United States; Bretl, T., Department of Aerospace Engineering, University of Illinois at Urbana-Champaign, Urbana, IL 61801, United States",,"Action spaces; Basis functions; Continuous state; Continuous State Space; Continuous time; Current estimates; Empirical evidence; Finite set; Input costs; Input noise; Inverse reinforcement learning; Maximum entropy; Maximum entropy distribution; Optimal control policy; Path integral; Sample path; Continuous time systems; Cost functions; Costs; Entropy; Intelligent robots; Inverse problems; Maximum likelihood estimation; Optimization; Reinforcement; Reinforcement learning; Robotics; Cost benefit analysis","Aghasadeghi, N.; Department of Electrical and Computer Engineering, University of Illinois at Urbana-Champaign, Urbana, IL 61801, United States; email: aghasad1@illinois.edu",Conference Paper,"Final",,Scopus,2-s2.0-84455160661
"Levine S., Popović Z., Koltun V.","35731728100;35176314800;7004337914;","Nonlinear inverse reinforcement learning with Gaussian processes",2011,"Advances in Neural Information Processing Systems 24: 25th Annual Conference on Neural Information Processing Systems 2011, NIPS 2011",,,,"","",9,92,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84860621067&partnerID=40&md5=63ebcead252f16eebde04c4b67a565cd","Stanford University, United States; University of Washington, United States","Levine, S., Stanford University, United States; Popović, Z., University of Washington, United States; Koltun, V., Stanford University, United States",,"Complex behavior; Gaussian Processes; Inverse reinforcement learning; Linear combinations; Markov Decision Processes; Nonlinear functions; Probabilistic algorithm; Reward function; Gaussian distribution; Gaussian noise (electronic); Learning algorithms; Markov processes; Reinforcement learning","Levine, S.; Stanford UniversityUnited States; email: svlevine@cs.stanford.edu",Conference Paper,"Final",,Scopus,2-s2.0-84860621067
"Daume III H.","8911764800;","Beyond structured prediction: Inverse reinforcement learning",2011,"ACL HLT 2011 - 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, Proceedings of Student Session",,,,"","",26,,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84859064813&partnerID=40&md5=5b1eaa1f013bdd655ab5cf054f1317ed","Computer Science, University of Maryland, United States","Daume III, H., Computer Science, University of Maryland, United States",,"Complete solutions; Inverse reinforcement learning; Reward function; Structured prediction; Computational linguistics; Reinforcement learning","Daume III, H.; Computer Science, University of MarylandUnited States; email: me@hal3.name",Conference Paper,"Final",,Scopus,2-s2.0-84859064813
"Boularias A., Kober J., Peters J.","23090275300;22940629000;35248912800;","Relative entropy Inverse Reinforcement Learning",2011,"Journal of Machine Learning Research","15",,,"182","189",,61,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84862293297&partnerID=40&md5=6bb3401274a67b9a5b08b1157570752a","Max-Planck Institute for Intelligent Systems, 72076 Tübingen, Germany","Boularias, A., Max-Planck Institute for Intelligent Systems, 72076 Tübingen, Germany; Kober, J., Max-Planck Institute for Intelligent Systems, 72076 Tübingen, Germany; Peters, J., Max-Planck Institute for Intelligent Systems, 72076 Tübingen, Germany",,"Empirical distributions; Imitation learning; Inverse reinforcement learning; Markov Decision Processes; Model free; Optimal policies; Relative entropy; Reward function; State space; Stochastic gradient descent; Algorithms; Artificial intelligence; Entropy; Markov processes; Reinforcement learning; Mathematical models","Boularias, A.; Max-Planck Institute for Intelligent Systems, 72076 Tübingen, Germany; email: abdeslam.boularias@tuebingen.mpg.de",Conference Paper,"Final",,Scopus,2-s2.0-84862293297
"Chandramohan S., Geist M., Lefèvre F., Pietquin O.","55817552546;25929145100;56273617700;16040586900;","User simulation in dialogue systems using Inverse Reinforcement Learning",2011,"Proceedings of the Annual Conference of the International Speech Communication Association, INTERSPEECH",,,,"1025","1028",,41,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84865717612&partnerID=40&md5=72da518c50fcb34bb90d80939ccd3640","Supelec - Metz Campus, IMS Research Group, France; UMI 2958 (CNRS - GeorgiaTech), France; Université D'Avignon et des Pays de Vaucluse, LIA-CERI, France","Chandramohan, S., Supelec - Metz Campus, IMS Research Group, France, Université D'Avignon et des Pays de Vaucluse, LIA-CERI, France; Geist, M., Supelec - Metz Campus, IMS Research Group, France; Lefèvre, F., Université D'Avignon et des Pays de Vaucluse, LIA-CERI, France; Pietquin, O., Supelec - Metz Campus, IMS Research Group, France, UMI 2958 (CNRS - GeorgiaTech), France","Inverse Reinforcement Learning; Spoken Dialogue Systems; User simulation","Dialogue systems; Human users; Imitation learning; Inverse reinforcement learning; Man-machine interface; Natural languages; Spoken dialogue system; Synthetic data; User simulation; Reinforcement learning; Speech processing","Chandramohan, S.; Supelec - Metz Campus, IMS Research GroupFrance; email: senthilkumar.chandramohan@supelec.fr",Conference Paper,"Final",,Scopus,2-s2.0-84865717612
"Michini B., How J.P.","36603096200;7006512768;","A human-interactive course of action planner for aircraft carrier deck operations",2011,"AIAA Infotech at Aerospace Conference and Exhibit 2011",,,,"","",,17,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880848742&partnerID=40&md5=efd23a574d3a6b1116c61b1b59bcde64","Aerospace Controls Laboratory, Massachusetts Institute of Technology, MIT, Cambridge, MA 02139, United States; Aerospace Controls Laboratory, Laboratory for Information and Decision Systems, Massachusetts Institute of Technology, MIT, Cambridge, MA 02139, United States","Michini, B., Aerospace Controls Laboratory, Massachusetts Institute of Technology, MIT, Cambridge, MA 02139, United States; How, J.P., Aerospace Controls Laboratory, Laboratory for Information and Decision Systems, Massachusetts Institute of Technology, MIT, Cambridge, MA 02139, United States",,"Course of action; Emergency scenario; Inverse reinforcement learning; Normal operations; Optimal policies; Planning problem; Time-critical scheduling; Uncertain environments; Aircraft; Aircraft carriers; Artificial intelligence; Decision support systems; Optimization; Personnel training; Scheduling; Taxicabs; Curricula","Aerospace Controls Laboratory, Massachusetts Institute of Technology, MIT, Cambridge, MA 02139, United States",Conference Paper,"Final",,Scopus,2-s2.0-84880848742
"Riordan B., Brimi S., Schurr N., Freeman J., Ganberg G., Cooke N.J., Rima N.","36469192100;55343995500;8511831000;7403529943;36600400000;7102096954;36026010500;","Inferring user intent with Bayesian inverse planning: Making sense of multi-UAS mission management",2011,"20th Annual Conference on Behavior Representation in Modeling and Simulation 2011, BRiMS 2011",,,,"49","56",,6,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84865361469&partnerID=40&md5=bacac8df2ed1fac6e12e209d4eb8ca01","Aptima, Inc., 12 Gill Street, Woburn, MA 01801, United States; Cognitive Engineering Research Institute, 5810 South Sossaman Road, Mesa, AZ 85212, United States","Riordan, B., Aptima, Inc., 12 Gill Street, Woburn, MA 01801, United States; Brimi, S., Aptima, Inc., 12 Gill Street, Woburn, MA 01801, United States; Schurr, N., Aptima, Inc., 12 Gill Street, Woburn, MA 01801, United States; Freeman, J., Aptima, Inc., 12 Gill Street, Woburn, MA 01801, United States; Ganberg, G., Aptima, Inc., 12 Gill Street, Woburn, MA 01801, United States; Cooke, N.J., Cognitive Engineering Research Institute, 5810 South Sossaman Road, Mesa, AZ 85212, United States; Rima, N., Cognitive Engineering Research Institute, 5810 South Sossaman Road, Mesa, AZ 85212, United States","Bayesian inference; Intent; Inverse reinforcement learning; Markov decision processes; Unmanned aerial systems","Bayesian inference; Intent; Inverse reinforcement learning; Markov Decision Processes; Unmanned aerial systems; Bayesian networks; Computer simulation; Markov processes; Reinforcement learning; User interfaces; Inference engines","Riordan, B.; Aptima, Inc., 12 Gill Street, Woburn, MA 01801, United States; email: briordan@aptima.com",Conference Paper,"Final",,Scopus,2-s2.0-84865361469
[No author name available],[No author id available],"Advances in Neural Information Processing Systems 24: 25th Annual Conference on Neural Information Processing Systems 2011, NIPS 2011",2011,"Advances in Neural Information Processing Systems 24: 25th Annual Conference on Neural Information Processing Systems 2011, NIPS 2011",,,,"","",2718,,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84860637780&partnerID=40&md5=f5dd06d12eb758b76907887b36017998",,"",,,,Conference Review,"Final",,Scopus,2-s2.0-84860637780
"Qiao Q., Beling P.A.","35848809400;6603732790;","Inverse reinforcement learning with Gaussian process",2011,"Proceedings of the American Control Conference",,, 5990948,"113","118",,14,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-80053156567&partnerID=40&md5=e9585553ce328e45a88d480881cadfd8","Department of Systems and Information Engineering, University of Virginia, Charlottesville, VA 22904, United States","Qiao, Q., Department of Systems and Information Engineering, University of Virginia, Charlottesville, VA 22904, United States; Beling, P.A., Department of Systems and Information Engineering, University of Virginia, Charlottesville, VA 22904, United States",,"Apprenticeship learning; Bayesian inference; Gaussian process models; Gaussian Processes; Infinite state space; Inverse reinforcement learning; Inverse-optimal control; Maximum a posteriori estimation; Numerical problems; Preference graph; Quadratic programs; Real-world application; Reward function; Bayesian networks; Convex optimization; Gaussian distribution; Gaussian noise (electronic); Inference engines; Numerical methods; Optimization; Reinforcement; Reinforcement learning; Learning algorithms","Qiao, Q.; Department of Systems and Information Engineering, University of Virginia, Charlottesville, VA 22904, United States; email: qq2r@virginia.edu",Conference Paper,"Final",,Scopus,2-s2.0-80053156567
[No author name available],[No author id available],"Machine Learning and Knowledge Discovery in Databases - European Conference, ECML PKDD 2011, Proceedings",2011,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","6913 LNAI","PART 3",,"","",1981,,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-80052420294&partnerID=40&md5=1c3daa149d8195ccbd48e3988880bf48",,"",,,,Conference Review,"Final",,Scopus,2-s2.0-80052420294
[No author name available],[No author id available],"Machine Learning and Knowledge Discovery in Databases - European Conference, ECML PKDD 2011, Proceedings",2011,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","6911 LNAI","PART 1",,"","",1981,,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-80052408801&partnerID=40&md5=56e0e50a8c641d81c3c796c3e942bafd",,"",,,,Conference Review,"Final",,Scopus,2-s2.0-80052408801
[No author name available],[No author id available],"Machine Learning and Knowledge Discovery in Databases - European Conference, ECML PKDD 2011, Proceedings",2011,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","6912 LNAI","PART 2",,"","",1981,,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-80052392387&partnerID=40&md5=eb5d9df83a0723c33dc281479e2f5e39",,"",,,,Conference Review,"Final",,Scopus,2-s2.0-80052392387
"Choi J., Kim K.-E.","36835669400;15053383400;","Inverse reinforcement learning in partially observable environments",2011,"Journal of Machine Learning Research","12",,,"691","730",,31,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-79955875655&partnerID=40&md5=02bd7a0269869bebe7c1cff2c4b935a5","Department of Computer Science, Korea Advanced Institute of Science and Technology, Daejeon 305-701, South Korea","Choi, J., Department of Computer Science, Korea Advanced Institute of Science and Technology, Daejeon 305-701, South Korea; Kim, K.-E., Department of Computer Science, Korea Advanced Institute of Science and Technology, Daejeon 305-701, South Korea","Inverse optimization; Inverse reinforcement learning; Linear programming; Partially observable Markov decision process; Quadratically constrained programming","Constrained programming; Ill posed; Inverse optimization; Inverse reinforcement learning; Markov Decision Processes; Partially observable environments; Partially observable Markov decision process; Realistic scenario; Reward function; Algorithms; Computer programming; Markov processes; Reinforcement learning; Inverse problems","Choi, J.; Department of Computer Science, Korea Advanced Institute of Science and Technology, Daejeon 305-701, South Korea; email: JDCHOI@AI.KAIST.AC.KR",Article,"Final",,Scopus,2-s2.0-79955875655
"Jin Z.-J., Qian H., Chen S.-Y., Zhu M.-L.","25652807500;57203653311;14035228600;7402909050;","Convergence analysis of an incremental approach to online inverse reinforcement learning",2011,"Journal of Zhejiang University: Science C","12","1",,"17","24",,1,"10.1631/jzus.C1010010","https://www.scopus.com/inward/record.uri?eid=2-s2.0-79951719816&doi=10.1631%2fjzus.C1010010&partnerID=40&md5=cc39d3500916ff197567172b04581342","School of Computer Science and Technology, Zhejiang University, Hangzhou 310027, China","Jin, Z.-J., School of Computer Science and Technology, Zhejiang University, Hangzhou 310027, China; Qian, H., School of Computer Science and Technology, Zhejiang University, Hangzhou 310027, China; Chen, S.-Y., School of Computer Science and Technology, Zhejiang University, Hangzhou 310027, China; Zhu, M.-L., School of Computer Science and Technology, Zhejiang University, Hangzhou 310027, China","Incremental approach; Inverse reinforcement learning; Markov decision process; Online learning; Reward recovering","Incremental approach; Inverse reinforcement learning; Markov Decision Processes; Online learning; Reward recovering; Markov processes; Recovery; Reinforcement learning; E-learning","Qian, H.; School of Computer Science and Technology, Zhejiang University, Hangzhou 310027, China; email: qianhui@zju.edu.cn",Article,"Final",,Scopus,2-s2.0-79951719816
"Boularias A., Chaib-Draa B.","23090275300;7004238886;","Bootstrapping apprenticeship learning",2010,"Advances in Neural Information Processing Systems 23: 24th Annual Conference on Neural Information Processing Systems 2010, NIPS 2010",,,,"","",9,4,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84860621667&partnerID=40&md5=741e6f4a48735d37a0683dbf1d919fd6","Department of Empirical Inference, Max-Planck Institute for Biological Cybernetics, 72076 Tübingen, Germany; Department of Computer Science, Laval University, QC G1V 0A6, Canada","Boularias, A., Department of Empirical Inference, Max-Planck Institute for Biological Cybernetics, 72076 Tübingen, Germany; Chaib-Draa, B., Department of Computer Science, Laval University, QC G1V 0A6, Canada",,"Apprenticeship learning; Highly sensitive; Inverse reinforcement learning; Linear combinations; Monte-Carlo estimation; State space; Utility functions; Apprentices; Reinforcement learning; Demonstrations","Boularias, A.; Department of Empirical Inference, Max-Planck Institute for Biological Cybernetics, 72076 Tübingen, Germany; email: abdeslam.boularias@tuebingen.mpg.de",Conference Paper,"Final",,Scopus,2-s2.0-84860621667
"Melo F.S., Lopes M.","35569256100;8607501600;","Learning from demonstration using MDP induced metrics",2010,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","6322 LNAI","PART 2",,"385","401",,7,"10.1007/978-3-642-15883-4_25","https://www.scopus.com/inward/record.uri?eid=2-s2.0-78049399307&doi=10.1007%2f978-3-642-15883-4_25&partnerID=40&md5=a8bffc83a889f0783baee7a7bafb6ade","INESC-ID/Instituto Superior Técnico, TagusPark - Edifício IST, 2780-990 Porto Salvo, Portugal; University of Plymouth PL4 8AA, Plymouth, Devon, United Kingdom","Melo, F.S., INESC-ID/Instituto Superior Técnico, TagusPark - Edifício IST, 2780-990 Porto Salvo, Portugal; Lopes, M., University of Plymouth PL4 8AA, Plymouth, Devon, United Kingdom",,"Computational costs; Generalization performance; Inverse reinforcement learning; Kernel based approach; Learning from demonstration; Optimal policies; State-space; Supervised learning methods; Learning systems; Demonstrations","Melo, F. S.; INESC-ID/Instituto Superior Técnico, TagusPark - Edifício IST, 2780-990 Porto Salvo, Portugal",Conference Paper,"Final",,Scopus,2-s2.0-78049399307
"Silver D., Bagnell J.A., Stentz A.","7202151417;6506127486;7006771602;","Learning from demonstration for autonomous navigation in complex unstructured terrain",2010,"International Journal of Robotics Research","29","12",,"1565","1592",,50,"10.1177/0278364910369715","https://www.scopus.com/inward/record.uri?eid=2-s2.0-77957947591&doi=10.1177%2f0278364910369715&partnerID=40&md5=1731feb8d6fe182bda5d9a780b829754","Carnegie Mellon University, 10 Fourtieth Street, Pittsburgh, PA 15201, United States","Silver, D., Carnegie Mellon University, 10 Fourtieth Street, Pittsburgh, PA 15201, United States; Bagnell, J.A., Carnegie Mellon University, 10 Fourtieth Street, Pittsburgh, PA 15201, United States; Stentz, A., Carnegie Mellon University, 10 Fourtieth Street, Pittsburgh, PA 15201, United States","autonomous navigation; Field robotics; imitation learning; inverse reinforcement learning; learning from demonstration; mobile robotics","Autonomous navigation; Field robotics; Imitation learning; Inverse reinforcement learning; Learning from demonstration; Mobile robotic; Demonstrations; Landforms; Machine design; Navigation systems; Online systems; Reinforcement learning; Robot programming; Robotics; Robots; Navigation","Silver, D.; Carnegie Mellon University, 10 Fourtieth Street, Pittsburgh, PA 15201, United States; email: dsilver@ri.cmu.edu",Conference Paper,"Final",,Scopus,2-s2.0-77957947591
"Dvijotham K., Todorov E.","36470045000;6701358567;","Inverse optimal control with linearly-solvable MDPs",2010,"ICML 2010 - Proceedings, 27th International Conference on Machine Learning",,,,"335","342",,60,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-77956518472&partnerID=40&md5=1c32392f7bcdb30cb34020ef9ba57540","Computer Science and Engineering and Applied Mathematics, University of Washington, Seattle - 98105, United States","Dvijotham, K., Computer Science and Engineering and Applied Mathematics, University of Washington, Seattle - 98105, United States; Todorov, E., Computer Science and Engineering and Applied Mathematics, University of Washington, Seattle - 98105, United States",,"Continuous State Space; Control engineering; Control policy; Density estimation; Forward problem; Inverse algorithm; Inverse reinforcement learning; Inverse-optimal control; Log likelihood; Maximum entropy; Nonlinear problems; Orders of magnitude; Special properties; Unconstrained optimization; Value functions; Control; Learning algorithms; Learning systems; Linearization; Maximum likelihood estimation; Optimization; Data reduction","Dvijotham, K.; Computer Science and Engineering and Applied Mathematics, University of Washington, Seattle - 98105, United States; email: DVIJ@CS.WASHINGTON.EDU",Conference Paper,"Final",,Scopus,2-s2.0-77956518472
"Lee S.J., Popović Z.","57192515790;35176314800;","Learning behavior styles with inverse reinforcement learning",2010,"ACM Transactions on Graphics","29","4", 122,"","",,22,"10.1145/1778765.1778859","https://www.scopus.com/inward/record.uri?eid=2-s2.0-77956417638&doi=10.1145%2f1778765.1778859&partnerID=40&md5=e0e912385ee21466ac8e162af9717c79","University of Washington, United States","Lee, S.J., University of Washington, United States; Popović, Z., University of Washington, United States","Apprenticeship learning; Data driven animation; Human animation; Inverse reinforcement learning; Optimal control","Apprenticeship learning; Data-driven animation; Human animation; Inverse reinforcement learning; Optimal controls; Apprentices; Control; Learning algorithms; Optimization; Reinforcement learning; Animation","Lee, S. J.; University of WashingtonUnited States",Article,"Final",,Scopus,2-s2.0-77956417638
"Lee S.J., Popović Z.","57192515790;35176314800;","Learning behavior styles with inverse reinforcement learning",2010,"ACM SIGGRAPH 2010 Papers, SIGGRAPH 2010",,, 122,"","",,2,"10.1145/1778765.1778859","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029591362&doi=10.1145%2f1778765.1778859&partnerID=40&md5=633985d4e53cd0131f25e3d2a752ab74","University of Washington, United States","Lee, S.J., University of Washington, United States; Popović, Z., University of Washington, United States","Apprenticeship learning; Data driven animation; Human animation; Inverse reinforcement learning; Optimal control","Animation; Apprentices; Computer graphics; Interactive computer graphics; Inverse problems; Learning algorithms; Apprenticeship learning; Data-driven animation; Human animation; Inverse reinforcement learning; Optimal controls; Reinforcement learning",,Conference Paper,"Final",,Scopus,2-s2.0-85029591362
