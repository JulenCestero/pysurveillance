Authors,Author(s) ID,Title,Year,Source title,Volume,Issue,Art. No.,Page start,Page end,Page count,Cited by,DOI,Link,Affiliations,Authors with affiliations,Author Keywords,Index Keywords,Document Type,Publication Stage,Access Type,Source,EID
"Lin J.-L., Hwang K.-S., Shi H., Pan W.","23389644200;7402426737;12752051900;57198514328;","An ensemble method for inverse reinforcement learning",2020,"Information Sciences","512",,,"518","532",,,"10.1016/j.ins.2019.09.066","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073051334&doi=10.1016%2fj.ins.2019.09.066&partnerID=40&md5=8822d30b633ab7bd73b01f99436a6edb","Department of Information Management, Shih-Hsin University Taipei11678, Taiwan; Department of Electrical Engineering, National Sun Yat-sen University, Kaohsiung, 80424, Taiwan; School of Computer Science, Northwestern Polytechnical University, Xi'an, 710129, China","Lin, J.-L., Department of Information Management, Shih-Hsin University Taipei11678, Taiwan; Hwang, K.-S., Department of Electrical Engineering, National Sun Yat-sen University, Kaohsiung, 80424, Taiwan; Shi, H., School of Computer Science, Northwestern Polytechnical University, Xi'an, 710129, China; Pan, W., School of Computer Science, Northwestern Polytechnical University, Xi'an, 710129, China","Apprentice learning; Boosting classifier; Inverse reinforcement learning; Q-learning","Adaptive boosting; Inverse problems; Machine learning; Robot applications; Accurate inference; AdaBoost algorithm; Boosting classifiers; Classification methods; Individual features; Inverse reinforcement learning; Parametric functions; Q-learning; Reinforcement learning",Article,"Final",,Scopus,2-s2.0-85073051334
"Shou Z., Di X., Ye J., Zhu H., Zhang H., Hampshire R.","57203962496;36987391400;57214080792;57212490055;57202297163;57203505770;","Optimal passenger-seeking policies on E-hailing platforms using Markov decision process and imitation learning",2020,"Transportation Research Part C: Emerging Technologies","111",,,"91","113",,,"10.1016/j.trc.2019.12.005","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076866879&doi=10.1016%2fj.trc.2019.12.005&partnerID=40&md5=de4ff1de56dcbb64192f6b9cb83c54ba","Department of Civil Engineering and Engineering Mechanics, Columbia University, United States; Data Science Institute, Columbia University, United States; Didi Chuxing Inc., Beijing, China; National Maglev Transportation Engineering R&D Center, Tongji University, Shanghai, China; University of Michigan Transportation Research Institute, University of Michigan, Ann Arbor, United States; Ford School of Public Policy, University of Michigan, Ann Arbor, United States","Shou, Z., Department of Civil Engineering and Engineering Mechanics, Columbia University, United States; Di, X., Department of Civil Engineering and Engineering Mechanics, Columbia University, United States, Data Science Institute, Columbia University, United States; Ye, J., Didi Chuxing Inc., Beijing, China; Zhu, H., Didi Chuxing Inc., Beijing, China; Zhang, H., National Maglev Transportation Engineering R&D Center, Tongji University, Shanghai, China; Hampshire, R., University of Michigan Transportation Research Institute, University of Michigan, Ann Arbor, United States, Ford School of Public Policy, University of Michigan, Ann Arbor, United States","E-hailing; Imitation learning; Markov Decision Process (MDP)","Behavioral research; Driver training; Earnings; Intelligent systems; Markov processes; Monte Carlo methods; Optimization; Roads and streets; Taxicabs; Traffic congestion; Dynamic adjustment; Imitation learning; Inverse reinforcement learning; Markov Decision Processes; Optimal sequential; Supply demand ratio; Transportation network; Vehicle-miles traveled; Reinforcement learning; competition (economics); computer simulation; decision analysis; learning; Markov chain; Monte Carlo analysis; numerical model; taxi transport; transportation policy",Article,"Final",,Scopus,2-s2.0-85076866879
"Hitomi K., Matsui K., Rivas A., Corchado J.M.","57209749083;56239039500;57195304326;7006360842;","Development of a dangerous driving suppression system using inverse reinforcement learning and blockchain",2020,"Advances in Intelligent Systems and Computing","1003",,,"3","9",,,"10.1007/978-3-030-23887-2_1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85068624697&doi=10.1007%2f978-3-030-23887-2_1&partnerID=40&md5=782181af9063b264dab444a5c630e966","Faculty of Robotics and Design, Osaka Institute of Technology, Osaka, Japan; BISITE Digital Innovation Hub, University of Salamanca, Salamanca, Spain","Hitomi, K., Faculty of Robotics and Design, Osaka Institute of Technology, Osaka, Japan; Matsui, K., Faculty of Robotics and Design, Osaka Institute of Technology, Osaka, Japan; Rivas, A., BISITE Digital Innovation Hub, University of Salamanca, Salamanca, Spain; Corchado, J.M., BISITE Digital Innovation Hub, University of Salamanca, Salamanca, Spain","Blockchain; Inverse reinforcement learning; Safe driving","Accidents; Blockchain; Cameras; Distributed computer systems; Machine learning; Competitive spirit; Dangerous drivings; Inverse reinforcement learning; Reckless driving; Safe driving; Significance levels; Suppression systems; System development; Reinforcement learning",Conference Paper,"Final",,Scopus,2-s2.0-85068624697
"Yusukc N., Sachiyo A.","57213189390;57213188460;","Bavesian inverse reinforcement learning for demonstrations of an expert in multiple dynamics: Toward estimation of transferable reward",2020,"Transactions of the Japanese Society for Artificial Intelligence","35","1",,"","",,,"10.1527/tjsai.G-J73","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077677208&doi=10.1527%2ftjsai.G-J73&partnerID=40&md5=44fa9fa4fdcd2aa091ba951e2cf4703b","Department of Urban Environment Systems, Graduate School of Science and Engineering, Chiba University, Japan; Department of Urban Environment Systems, Graduate School of Engineering, Chiba University, Japan","Yusukc, N., Department of Urban Environment Systems, Graduate School of Science and Engineering, Chiba University, Japan; Sachiyo, A., Department of Urban Environment Systems, Graduate School of Engineering, Chiba University, Japan","Bayesian inference; Inverse reinforcement learning; Markov decision processes; Reinforcement learning","Bayesian networks; Demonstrations; Inference engines; Inverse problems; Learning algorithms; Machine learning; Markov processes; Monte Carlo methods; Bayesian inference; Driving tasks; Inverse reinforcement learning; Learning problem; Markov chain Monte Carlo method; Markov Decision Processes; Objective functions; Reward function; Reinforcement learning",Article,"Final",Open Access,Scopus,2-s2.0-85077677208
"Li Y., Huang W.","56193387600;57211978130;","Imitation learning from human-generated spatial-temporal data",2019,"Proceedings of the 3rd ACM SIGSPATIAL International Workshop on AI for Geographic Knowledge Discovery, GeoAI 2019",,,,"9","10",,,"10.1145/3356471.3365229","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075586727&doi=10.1145%2f3356471.3365229&partnerID=40&md5=48816d3a30696563291964389c156b21","Worcester Polytechnic Institute, United States","Li, Y., Worcester Polytechnic Institute, United States; Huang, W., Worcester Polytechnic Institute, United States","Human-generated spatial-temporal data; Imitation learning; Inverse reinforcement learning","Decision making; Learning algorithms; Decision-making strategies; Human decision making; Imitation learning; Inverse reinforcement learning; Research challenges; Spatial temporals; Spatial-temporal data; State of the art; Reinforcement learning",Conference Paper,"Final",Open Access,Scopus,2-s2.0-85075586727
"Hidaka K., Hayakawa K., Nishi T., Usui T., Yamamoto T.","57195234027;15047971700;57205550400;57211070410;55703202800;","Generating pedestrian walking behavior considering detour and pause in the path under space-time constraints",2019,"Transportation Research Part C: Emerging Technologies","108",,,"115","129",,,"10.1016/j.trc.2019.09.005","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072558066&doi=10.1016%2fj.trc.2019.09.005&partnerID=40&md5=7fc436530a474ca4c79d672095d38f88","Toyota Central R&D Labs., Inc., 41-1, Yokomichi, Nagakute, Aichi, Japan; Toyota Central R&D Labs., Inc., Koraku Mori Building 10F, 1-4-14 Koraku, Bunkyo-ku, Tokyo, Japan; University of Human Environments, 6-2, Kamisanbonmatsu, Motojuku-cho, Okazaki, Japan; Nagoya University Institute of Materials and Systems for Sustainability, Furo-cho, Chikusa-ku, Nagoya, Japan","Hidaka, K., Toyota Central R&D Labs., Inc., 41-1, Yokomichi, Nagakute, Aichi, Japan; Hayakawa, K., Toyota Central R&D Labs., Inc., Koraku Mori Building 10F, 1-4-14 Koraku, Bunkyo-ku, Tokyo, Japan; Nishi, T., Toyota Central R&D Labs., Inc., Koraku Mori Building 10F, 1-4-14 Koraku, Bunkyo-ku, Tokyo, Japan; Usui, T., University of Human Environments, 6-2, Kamisanbonmatsu, Motojuku-cho, Okazaki, Japan; Yamamoto, T., Nagoya University Institute of Materials and Systems for Sustainability, Furo-cho, Chikusa-ku, Nagoya, Japan","Inverse reinforcement learning; Pedestrian behavior modeling; Space-time constraints; Trajectory generation","Inverse problems; Machine learning; Stochastic systems; Trajectories; Goal-oriented behavior; Inverse reinforcement learning; Location-aware technology; Pedestrian behavior model; Pedestrian trajectories; Points of Interest(POI); Space time; Trajectory generation; Reinforcement learning; inverse analysis; learning; pedestrian; public space; spatiotemporal analysis; urban transport; walking",Article,"Final",,Scopus,2-s2.0-85072558066
"Rivera-Villicana J., Zambetta F., Harland J., Berry M.","56415008400;6507963260;7005796007;13006225500;","Exploring apprenticeship learning for player modelling in interactive narratives",2019,"CHI PLAY 2019 - Extended Abstracts of the Annual Symposium on Computer-Human Interaction in Play",,,,"645","652",,,"10.1145/3341215.3356314","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074810571&doi=10.1145%2f3341215.3356314&partnerID=40&md5=b59d41d47bde855b478470d8d907dab4","Applied Artificial Intelligence Institute (A2I2, Deakin University, Burwood, VIC, Australia; School of Science, RMIT University, Melbourne, VIC, Australia; School of Media and Communication, RMIT University, Melbourne, VIC, Australia","Rivera-Villicana, J., Applied Artificial Intelligence Institute (A2I2, Deakin University, Burwood, VIC, Australia, School of Science, RMIT University, Melbourne, VIC, Australia; Zambetta, F., School of Science, RMIT University, Melbourne, VIC, Australia; Harland, J., School of Science, RMIT University, Melbourne, VIC, Australia; Berry, M., School of Media and Communication, RMIT University, Melbourne, VIC, Australia","Anchorhead; Apprenticeship Learning; Interactive Narratives; Inverse Reinforcement Learning; Player Modelling","Apprentices; Human computer interaction; Interactive computer systems; Action sequences; Anchorhead; Apprenticeship learning; Interactive fiction; Interactive narrative; Inverse reinforcement learning; Receding horizon; Reward function; Reinforcement learning",Conference Paper,"Final",Open Access,Scopus,2-s2.0-85074810571
"Sun R., Hu S., Zhao H., Moze M., Aioun F., Guillemard F.","57212482840;57212480046;35723299400;15925903800;57190282434;11138805900;","Human-like Highway Trajectory Modeling based on Inverse Reinforcement Learning",2019,"2019 IEEE Intelligent Transportation Systems Conference, ITSC 2019",,, 8916970,"1482","1489",,,"10.1109/ITSC.2019.8916970","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076817726&doi=10.1109%2fITSC.2019.8916970&partnerID=40&md5=0b1c3910668462c02e960091c577bb64","Peking University, Key Lab of Machine Perception, Beijing, China; Groupe PSA, Velizy, France","Sun, R., Peking University, Key Lab of Machine Perception, Beijing, China; Hu, S., Peking University, Key Lab of Machine Perception, Beijing, China; Zhao, H., Peking University, Key Lab of Machine Perception, Beijing, China; Moze, M., Groupe PSA, Velizy, France; Aioun, F., Groupe PSA, Velizy, France; Guillemard, F., Groupe PSA, Velizy, France",,"Autonomous vehicles; Behavioral research; Deep learning; Intelligent systems; Intelligent vehicle highway systems; Machine learning; Mapping; Reinforcement learning; Trajectories; Autonomous driving; Behavior patterns; Cutting edge technology; Experimental validations; Human-like trajectory; Inverse reinforcement learning; State-of-the-art performance; Trajectory modeling; Inverse problems",Conference Paper,"Final",,Scopus,2-s2.0-85076817726
"Lin X., Adams S.C., Beling P.A.","57211939661;56954203500;6603732790;","Multi-agent inverse reinforcement learning for certain general-sum stochastic games",2019,"Journal of Artificial Intelligence Research","66",,,"473","502",,,"10.1613/jair.1.11541","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075421977&doi=10.1613%2fjair.1.11541&partnerID=40&md5=827863ba9b393daba2a36fc860008d66","Data Science, MassMutual Financial Group, 470 Atlantic Ave., Boston, MA  02210, United States; University of Virginia, Charlottesville, VA  22904, United States","Lin, X., Data Science, MassMutual Financial Group, 470 Atlantic Ave., Boston, MA  02210, United States, University of Virginia, Charlottesville, VA  22904, United States; Adams, S.C., University of Virginia, Charlottesville, VA  22904, United States; Beling, P.A., University of Virginia, Charlottesville, VA  22904, United States",,"Computation theory; Convex optimization; Game theory; Linear programming; Machine learning; Multi agent systems; Reinforcement learning; Stochastic systems; Characteristic set; Convex optimization problems; Cooperative strategy; Correlated equilibria; General-sum stochastic games; Inverse reinforcement learning; Linear programming problem; Solution uniqueness; Inverse problems",Article,"Final",Open Access,Scopus,2-s2.0-85075421977
"Xin L., Li S.E., Wang P., Cao W., Nie B., Chan C.-Y., Cheng B.","55343655300;15065803700;56193114700;57212480181;56896875900;7404814589;36611317200;","Accelerated Inverse Reinforcement Learning with Randomly Pre-sampled Policies for Autonomous Driving Reward Design",2019,"2019 IEEE Intelligent Transportation Systems Conference, ITSC 2019",,, 8916952,"2757","2764",,,"10.1109/ITSC.2019.8916952","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076813251&doi=10.1109%2fITSC.2019.8916952&partnerID=40&md5=cdaf83297d647c8db8c194bdb46f7ed6","Tsinghua University, State Key Laboratory of Automotive Safety and Energy, School of Vehicle and Mobility, Beijing, 100084, China; California PATH, University of California, Berkeley, Richmond, CA  94804, United States","Xin, L., Tsinghua University, State Key Laboratory of Automotive Safety and Energy, School of Vehicle and Mobility, Beijing, 100084, China; Li, S.E., Tsinghua University, State Key Laboratory of Automotive Safety and Energy, School of Vehicle and Mobility, Beijing, 100084, China; Wang, P., California PATH, University of California, Berkeley, Richmond, CA  94804, United States; Cao, W., Tsinghua University, State Key Laboratory of Automotive Safety and Energy, School of Vehicle and Mobility, Beijing, 100084, China; Nie, B., Tsinghua University, State Key Laboratory of Automotive Safety and Energy, School of Vehicle and Mobility, Beijing, 100084, China; Chan, C.-Y., California PATH, University of California, Berkeley, Richmond, CA  94804, United States; Cheng, B., Tsinghua University, State Key Laboratory of Automotive Safety and Energy, School of Vehicle and Mobility, Beijing, 100084, China","Autonomous driving; Inverse reinforcement learning; Maximum entropy; Pre-sampled policy; Reward design","Autonomous vehicles; Efficiency; Intelligent systems; Intelligent vehicle highway systems; Inverse problems; Machine learning; Maximum entropy methods; Trajectories; Autonomous driving; Candidate trajectories; Improving learning; Inverse reinforcement learning; Optimal policies; Optimal trajectories; Reward function; Simulated driving; Reinforcement learning",Conference Paper,"Final",,Scopus,2-s2.0-85076813251
"Xu K., Zeng Y., Zhang Q., Yin Q., Sun L., Xiao K.","56828524300;57209278546;56520586200;23096547400;57209284133;56419405500;","Online probabilistic goal recognition and its application in dynamic shortest-path local network interdiction",2019,"Engineering Applications of Artificial Intelligence","85",,,"57","71",,,"10.1016/j.engappai.2019.05.009","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85067169848&doi=10.1016%2fj.engappai.2019.05.009&partnerID=40&md5=88544050dbd7a3f858f31b6957bab2a3","College of Systems Engineering, National University of Defense Technology, Changsha, China; Joint Service College, National Defense University, Beijing, China; Science and Technology on Information Systems Engineering Laboratory, College of System Engineering, National University of Defense Technology, Changsha, China","Xu, K., College of Systems Engineering, National University of Defense Technology, Changsha, China; Zeng, Y., College of Systems Engineering, National University of Defense Technology, Changsha, China; Zhang, Q., College of Systems Engineering, National University of Defense Technology, Changsha, China; Yin, Q., College of Systems Engineering, National University of Defense Technology, Changsha, China; Sun, L., Joint Service College, National Defense University, Beijing, China; Xiao, K., Science and Technology on Information Systems Engineering Laboratory, College of System Engineering, National University of Defense Technology, Changsha, China","Behavior modeling; Goal recognition; Network interdiction","Graph theory; Inverse problems; Probability distributions; Reinforcement learning; Behavior model; Dynamic shortest path; Effectiveness of knowledge; Goal recognition; Inverse reinforcement learning; Network interdiction; Network interdiction problems; Probabilistic distribution; Heuristic methods",Article,"Final",,Scopus,2-s2.0-85067169848
"Self R., Harlan M., Kamalapurkar R.","57209273510;57211021537;55210138200;","Online inverse reinforcement learning for nonlinear systems",2019,"CCTA 2019 - 3rd IEEE Conference on Control Technology and Applications",,, 8920458,"296","301",,,"10.1109/CCTA.2019.8920458","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077788456&doi=10.1109%2fCCTA.2019.8920458&partnerID=40&md5=01d76d0016785e22e56072bea9e4fe04","Oklahoma State University, School of Mechanical and Aerospace Engineering, Still-Water, OK, United States","Self, R., Oklahoma State University, School of Mechanical and Aerospace Engineering, Still-Water, OK, United States; Harlan, M., Oklahoma State University, School of Mechanical and Aerospace Engineering, Still-Water, OK, United States; Kamalapurkar, R., Oklahoma State University, School of Mechanical and Aerospace Engineering, Still-Water, OK, United States",,"Machine learning; Nonlinear systems; Online systems; Reinforcement learning; Input trajectory; Inverse reinforcement learning; Reward function; Theoretical guarantees; Unknown values; E-learning",Conference Paper,"Final",,Scopus,2-s2.0-85077788456
"Ahlberg S., Dimarogonas D.V.","57211204477;6506281602;","Human-in-the-loop control synthesis for multi-agent systems under hard and soft metric interval temporal logic specifications∗",2019,"IEEE International Conference on Automation Science and Engineering","2019-August",, 8842954,"788","793",,,"10.1109/COASE.2019.8842954","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072955571&doi=10.1109%2fCOASE.2019.8842954&partnerID=40&md5=897b7dc4962c14d4c76b9233b734410b","Division of Decision and Control Systems, KTH Royal Institute of Technology, Sweden","Ahlberg, S., Division of Decision and Control Systems, KTH Royal Institute of Technology, Sweden; Dimarogonas, D.V., Division of Decision and Control Systems, KTH Royal Institute of Technology, Sweden",,"Control system synthesis; Reinforcement learning; Temporal logic; Control synthesis; Hard and soft constraints; Human-in-the-loop control; Interval temporal logic; Inverse reinforcement learning; Mixed initiative; Optimal paths; Soft constraint; Multi agent systems",Conference Paper,"Final",,Scopus,2-s2.0-85072955571
"Pauna A., Bica I., Pop F., Castiglione A.","55360454800;36343237100;22836395600;16027997000;","On the rewards of self-adaptive IoT honeypots",2019,"Annales des Telecommunications/Annals of Telecommunications","74","7-8",,"501","515",,,"10.1007/s12243-018-0695-7","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059537479&doi=10.1007%2fs12243-018-0695-7&partnerID=40&md5=1dfaadfd843d037fe93c6cd53acf0945","Faculty of Military Electronic and Information Systems, Military Technical Academy, Bucharest, Romania; Faculty of Automatic Control and Computers, “Politehnica” University of Bucharest, Bucharest, Romania; National Institute for Research and Development in Informatics (ICI), Bucharest, Romania; Department of Computer Science, University of Salerno, Fisciano, SA, Italy","Pauna, A., Faculty of Military Electronic and Information Systems, Military Technical Academy, Bucharest, Romania; Bica, I., Faculty of Military Electronic and Information Systems, Military Technical Academy, Bucharest, Romania; Pop, F., Faculty of Automatic Control and Computers, “Politehnica” University of Bucharest, Bucharest, Romania, National Institute for Research and Development in Informatics (ICI), Bucharest, Romania; Castiglione, A., Department of Computer Science, University of Salerno, Fisciano, SA, Italy","Honeypot systems; Internet of things; Inverse reinforcement learning; Neural network; Reinforcement learning; Self-adaptive honeypot systems","Learning algorithms; Network security; Neural networks; Reinforcement learning; Risk assessment; Apprenticeship learning; Honeypots; Inverse reinforcement learning; Iot devices; Key Issues; Optimal reward; Reward function; Self-Adaptive; Internet of things",Article,"Final",,Scopus,2-s2.0-85059537479
"Gao X., Zhao X., Tan M.","57211203891;55648474000;14034568900;","Modeling socially normative navigation behaviors from demonstrations with inverse reinforcement learning",2019,"IEEE International Conference on Automation Science and Engineering","2019-August",, 8843123,"1333","1340",,,"10.1109/COASE.2019.8843123","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072957301&doi=10.1109%2fCOASE.2019.8843123&partnerID=40&md5=a9f596cf82e9e64e691001686d791779","State Key Laboratory of Management and Control for Complex Systems, Institute of Automation, Chinese Academy of Sciences, Beijing, 100190, China; University of Chinese Academy of Sciences, Beijing, 100049, China","Gao, X., State Key Laboratory of Management and Control for Complex Systems, Institute of Automation, Chinese Academy of Sciences, Beijing, 100190, China, University of Chinese Academy of Sciences, Beijing, 100049, China; Zhao, X., State Key Laboratory of Management and Control for Complex Systems, Institute of Automation, Chinese Academy of Sciences, Beijing, 100190, China; Tan, M., State Key Laboratory of Management and Control for Complex Systems, Institute of Automation, Chinese Academy of Sciences, Beijing, 100190, China",,"Air navigation; Behavioral research; Cost functions; Demonstrations; Inverse problems; Learning algorithms; Machine learning; Navigation; Robots; Trajectories; Discrete distribution; Dynamic obstacles; Human navigation; Inverse reinforcement learning; Motion planners; Navigation behavior; Social navigation; State-of-the-art methods; Reinforcement learning",Conference Paper,"Final",,Scopus,2-s2.0-85072957301
"Ashida K., Kato T., Hotta K., Oka K.","57204919943;57208750954;7202730545;7201489853;","Multiple tracking and machine learning reveal dopamine modulation for area-restricted foraging behaviors via velocity change in Caenorhabditis elegans",2019,"Neuroscience Letters","706",,,"68","74",,1,"10.1016/j.neulet.2019.05.011","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065714065&doi=10.1016%2fj.neulet.2019.05.011&partnerID=40&md5=9ad5c6c0707c68bcea841a485614818a","Department of Bioscience and Informatics, Faculty of Science and Technology, Keio University, Yokohama, 223-8522, Japan; Graduate Institute of Medicine, College of Medicine, Kaohsiung Medical University, Kaohsiung City, 80708, Taiwan; Waseda Research Institute for Science and Engineering, Waseda University, 2-2 Wakamatsucho, Shinjuku, Tokyo  162-8480, Japan","Ashida, K., Department of Bioscience and Informatics, Faculty of Science and Technology, Keio University, Yokohama, 223-8522, Japan; Kato, T., Department of Bioscience and Informatics, Faculty of Science and Technology, Keio University, Yokohama, 223-8522, Japan; Hotta, K., Department of Bioscience and Informatics, Faculty of Science and Technology, Keio University, Yokohama, 223-8522, Japan; Oka, K., Department of Bioscience and Informatics, Faculty of Science and Technology, Keio University, Yokohama, 223-8522, Japan, Graduate Institute of Medicine, College of Medicine, Kaohsiung Medical University, Kaohsiung City, 80708, Taiwan, Waseda Research Institute for Science and Engineering, Waseda University, 2-2 Wakamatsucho, Shinjuku, Tokyo  162-8480, Japan","Area-restricted search; Behavioral assay; Dopamine; Foraging; Inverse reinforcement learning; Machine learning","dopamine; dopamine; Article; Caenorhabditis elegans; controlled study; dopamine metabolism; foraging behavior; machine learning; nonhuman; priority journal; reinforcement; velocity; animal; appetitive behavior; Caenorhabditis elegans; drug effect; transgenic animal; Animals; Animals, Genetically Modified; Appetitive Behavior; Caenorhabditis elegans; Dopamine; Machine Learning; Reinforcement, Psychology",Article,"Final",,Scopus,2-s2.0-85065714065
"Oh M.-H., Iyengar G.","56939849000;7005191853;","Sequential anomaly detection using inverse reinforcement learning",2019,"Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining",,,,"1480","1490",,,"10.1145/3292500.3330932","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071146603&doi=10.1145%2f3292500.3330932&partnerID=40&md5=528a1e6194f6c36b5b0d7bad8675655f","Columbia University, New York, NY, United States","Oh, M.-H., Columbia University, New York, NY, United States; Iyengar, G., Columbia University, New York, NY, United States","Anomaly detection; Bootstrap; Inverse reinforcement learning; Neural networks; Outlier detection; Spatial-temporal data","Bayesian networks; Data mining; Decision making; Inverse problems; Machine learning; Neural networks; Reinforcement learning; Safety engineering; Anomaly detection methods; Application scenario; Automatic detection systems; Bootstrap; Critical environment; Decision making agents; Inverse reinforcement learning; Spatial-temporal data; Anomaly detection",Conference Paper,"Final",,Scopus,2-s2.0-85071146603
"Piao S., Huang Y., Liu H.","57211741412;57211277745;35272617500;","Online multi-modal imitation learning via lifelong intention encoding",2019,"2019 4th IEEE International Conference on Advanced Robotics and Mechatronics, ICARM 2019",,, 8833960,"786","792",,,"10.1109/ICARM.2019.8833960","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073232592&doi=10.1109%2fICARM.2019.8833960&partnerID=40&md5=a6d7d90c008261e5828d799e4d65519a","School of Computer Science and Technology, Harbin Institute of Technology, Harbin, China; Department of Computer Science and Technology, Tsinghua University, Beijing, China","Piao, S., School of Computer Science and Technology, Harbin Institute of Technology, Harbin, China; Huang, Y., School of Computer Science and Technology, Harbin Institute of Technology, Harbin, China; Liu, H., Department of Computer Science and Technology, Tsinghua University, Beijing, China",,"E-learning; Encoding (symbols); Inverse problems; Machine learning; Robotics; Signal encoding; Imitation learning; Inverse reinforcement learning; Multi-modal; Reward function; Robotic manipulation; Sparse coding; Reinforcement learning",Conference Paper,"Final",,Scopus,2-s2.0-85073232592
"Imani M., Braga-Neto U.M.","57189031055;6603390822;","Control of gene regulatory networks using bayesian inverse reinforcement learning",2019,"IEEE/ACM Transactions on Computational Biology and Bioinformatics","16","4", 3370654,"1250","1261",,5,"10.1109/TCBB.2018.2830357","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046001548&doi=10.1109%2fTCBB.2018.2830357&partnerID=40&md5=6da59589d88f5f109056eebaf9f54ed5","Department of Electrical and Computer Engineering, Texas A and M University, College Station, TX  77843, United States","Imani, M., Department of Electrical and Computer Engineering, Texas A and M University, College Station, TX  77843, United States; Braga-Neto, U.M., Department of Electrical and Computer Engineering, Texas A and M University, College Station, TX  77843, United States","Bayesian inverse reinforcement learning; Boolean Kalman smoother; Gene regulatory networks; Melanoma; P53-MDM2; Partially-observed Boolean dynamical system; Q-learning","Bayesian networks; Boolean functions; Cost functions; Costs; Dermatology; Dynamical systems; Gene expression; Inverse problems; Learning algorithms; Oncology; State feedback; Boolean dynamical systems; Gene regulatory networks; Inverse reinforcement learning; Kalman smoother; Melanoma; p53-MDM2; Q-learning; Reinforcement learning",Article,"Final",,Scopus,2-s2.0-85046001548
"Zhuansun S., Yang J.-A., Liu H.","57202022207;9737323800;56125442700;","Apprenticeship learning in cognitive jamming",2019,"Optimal Control Applications and Methods","40","4",,"647","658",,,"10.1002/oca.2502","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063507637&doi=10.1002%2foca.2502&partnerID=40&md5=4df1fe33eb13cf892375d0e3a61546d0","Electronic Countermeasure Institute, National University of Defense Technology, Hefei, China; Key Laboratory of Electronic Restriction, Hefei, China","Zhuansun, S., Electronic Countermeasure Institute, National University of Defense Technology, Hefei, China, Key Laboratory of Electronic Restriction, Hefei, China; Yang, J.-A., Electronic Countermeasure Institute, National University of Defense Technology, Hefei, China, Key Laboratory of Electronic Restriction, Hefei, China; Liu, H., Electronic Countermeasure Institute, National University of Defense Technology, Hefei, China, Key Laboratory of Electronic Restriction, Hefei, China","cognitive jamming; inverse reinforcement learning; jamming strategy; Markov decision process; reinforcement learning","Apprentices; Electronic warfare; Inverse problems; Jamming; Learning algorithms; Machine learning; Markov processes; Transmitters; Apprenticeship learning; Changing environment; Cognitive jamming; Complete information; Inverse reinforcement learning; Jamming strategies; Markov Decision Processes; Number of iterations; Reinforcement learning",Article,"Final",,Scopus,2-s2.0-85063507637
"Noothigattu R., Bouneffouf D., Mattei N., Chandra R., Madan P., Varshney K.R., Campbell M., Singh M., Rossi F.","57191410942;55210872800;35234406800;57211758567;57211752662;14069407800;7403371273;55443750200;56066648900;","Teaching AI agents ethical values using reinforcement learning and policy orchestration",2019,"IBM Journal of Research and Development","63","4-5", 8827920,"","",,1,"10.1147/JRD.2019.2940428","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074918441&doi=10.1147%2fJRD.2019.2940428&partnerID=40&md5=5d489dd9ae188985211978aa0b8dff6f","IBM Research, Cambridge, MA  02142, United States","Noothigattu, R., IBM Research, Cambridge, MA  02142, United States; Bouneffouf, D., IBM Research, Cambridge, MA  02142, United States; Mattei, N., IBM Research, Cambridge, MA  02142, United States; Chandra, R., IBM Research, Cambridge, MA  02142, United States; Madan, P., IBM Research, Cambridge, MA  02142, United States; Varshney, K.R., IBM Research, Cambridge, MA  02142, United States; Campbell, M., IBM Research, Cambridge, MA  02142, United States; Singh, M., IBM Research, Cambridge, MA  02142, United States; Rossi, F., IBM Research, Cambridge, MA  02142, United States",,"Machine learning; Reinforcement learning; Constraint-based; Contextual bandits; Cyber physicals; Ethical values; Implicit constraints; Inverse reinforcement learning; Pac-man; Time step; Autonomous agents",Article,"Final",,Scopus,2-s2.0-85074918441
"Kim K., Kim S., Lee C., Ko S.","57209973294;57209981341;57209978509;37049022800;","Poster: Modeling exploration/exploitation decisions through mobile sensing for understanding mechanisms of addiction",2019,"MobiSys 2019 - Proceedings of the 17th Annual International Conference on Mobile Systems, Applications, and Services",,,,"512","513",,,"10.1145/3307334.3328599","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069165274&doi=10.1145%2f3307334.3328599&partnerID=40&md5=85c7467a0ff678ab1bea14f74921e41b","School of Electrical and Computer Engineering, UNIST Ulsan, South Korea","Kim, K., School of Electrical and Computer Engineering, UNIST Ulsan, South Korea; Kim, S., School of Electrical and Computer Engineering, UNIST Ulsan, South Korea; Lee, C., School of Electrical and Computer Engineering, UNIST Ulsan, South Korea; Ko, S., School of Electrical and Computer Engineering, UNIST Ulsan, South Korea",,"Disease control; Game theory; Neurology; Reinforcement learning; Brain disease; Cost-efficient; Exploration/exploitation; Higher learning; Inverse reinforcement learning; Literature models; Loss of control; Mobile sensing; Inverse problems",Conference Paper,"Final",,Scopus,2-s2.0-85069165274
"Sun L., Zhan W., Chan C.-Y., Tomizuka M.","56007948700;57193013312;7404814589;35567325500;","Behavior planning of autonomous cars with social perception",2019,"IEEE Intelligent Vehicles Symposium, Proceedings","2019-June",, 8814223,"207","213",,1,"10.1109/IVS.2019.8814223","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072284431&doi=10.1109%2fIVS.2019.8814223&partnerID=40&md5=4216b2808dab454d1154403749e92e4f","Department of Mechanical Engineering, University of California, Berkeley, CA  94720, United States; California PATH, University of California, Berkeley, CA  94720, United States","Sun, L., Department of Mechanical Engineering, University of California, Berkeley, CA  94720, United States; Zhan, W., Department of Mechanical Engineering, University of California, Berkeley, CA  94720, United States; Chan, C.-Y., California PATH, University of California, Berkeley, CA  94720, United States; Tomizuka, M., Department of Mechanical Engineering, University of California, Berkeley, CA  94720, United States",,"Cost functions; Decision making; Intelligent vehicle highway systems; Model predictive control; Predictive control systems; Reinforcement learning; Roads and streets; Sensor networks; Behavior planning; Distributed sensor; Dynamic environments; Individual behavior; Inverse reinforcement learning; Probabilistic planning; Probabilistic prediction; Sensor limitations; Autonomous vehicles",Conference Paper,"Final",,Scopus,2-s2.0-85072284431
"Hussein M., Begum M., Petrik M.","57212629145;16548811500;16425736700;","Inverse reinforcement learning of interaction dynamics from demonstrations",2019,"Proceedings - IEEE International Conference on Robotics and Automation","2019-May",, 8793867,"2267","2274",,,"10.1109/ICRA.2019.8793867","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071451734&doi=10.1109%2fICRA.2019.8793867&partnerID=40&md5=28f363cc061f42a93caa54e2ac9a34b6","Cognitive Assistive Robotics Lab, University of New Hampshire, United States; Department of Computer Science, University of New Hampshire, United States","Hussein, M., Cognitive Assistive Robotics Lab, University of New Hampshire, United States; Begum, M., Cognitive Assistive Robotics Lab, University of New Hampshire, United States; Petrik, M., Department of Computer Science, University of New Hampshire, United States",,,Conference Paper,"Final",,Scopus,2-s2.0-85071451734
"You C., Lu J., Filev D., Tsiotras P.","57191623509;7601561346;7004005522;7006580129;","Advanced planning for autonomous vehicles using reinforcement learning and deep inverse reinforcement learning",2019,"Robotics and Autonomous Systems","114",,,"1","18",,7,"10.1016/j.robot.2019.01.003","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060531902&doi=10.1016%2fj.robot.2019.01.003&partnerID=40&md5=49dcd8d86ea9d1835065e136374f100a","School of Aerospace Engineering, Georgia Institute of Technology, Atlanta, GA  30332-0150, United States; Research & Advanced Engineering, Ford Motor Company, Dearborn, MI  48121, United States; School of Aerospace Engineering and the Institute for Robotics & Intelligent Machines, Georgia Institute of Technology, Atlanta, GA  30332-0150, United States","You, C., School of Aerospace Engineering, Georgia Institute of Technology, Atlanta, GA  30332-0150, United States; Lu, J., Research & Advanced Engineering, Ford Motor Company, Dearborn, MI  48121, United States; Filev, D., Research & Advanced Engineering, Ford Motor Company, Dearborn, MI  48121, United States; Tsiotras, P., School of Aerospace Engineering and the Institute for Robotics & Intelligent Machines, Georgia Institute of Technology, Atlanta, GA  30332-0150, United States","Autonomous vehicle; Deep neural-network; Inverse reinforcement learning; Maximum entropy; Path planning; Reinforcement learning","Autonomous vehicles; Behavioral research; Deep neural networks; Entropy; Highway traffic control; Intelligent systems; Intelligent vehicle highway systems; Inverse problems; Learning algorithms; Machine learning; Markov processes; Maximum entropy methods; Maximum principle; Motion planning; Stochastic models; Stochastic systems; Traffic congestion; Driving strategy; Intelligent transportation systems; Inverse reinforcement learning; Markov Decision Processes; Maximum entropy principle; Planning problem; Reinforcement learning techniques; Simulated results; Reinforcement learning",Article,"Final",Open Access,Scopus,2-s2.0-85060531902
"Pflueger M., Agha A., Sukhatme G.S.","56743089900;57207943863;7003730870;","Rover-IRL: Inverse reinforcement learning with soft value iteration networks for planetary rover path planning",2019,"IEEE Robotics and Automation Letters","4","2", 8629318,"1387","1394",,3,"10.1109/LRA.2019.2895892","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063311999&doi=10.1109%2fLRA.2019.2895892&partnerID=40&md5=bc1d700a61f497b5b54ba6436b882ba9","Department of Computer Science, University of Southern California, Los Angeles, CA  90089, United States; Jet Propulsion Laboratory, California Institute of Technology, Pasadena, CA  91109, United States","Pflueger, M., Department of Computer Science, University of Southern California, Los Angeles, CA  90089, United States; Agha, A., Jet Propulsion Laboratory, California Institute of Technology, Pasadena, CA  91109, United States; Sukhatme, G.S., Department of Computer Science, University of Southern California, Los Angeles, CA  90089, United States","deep learning in robotics and automation; learning from demonstration; Space robotics and automation","Approximation algorithms; Convolution; Deep learning; Inverse problems; Iterative methods; Machine learning; Motion planning; Neural networks; Planetary landers; Probability distributions; Robotics; Convolutional networks; Convolutional neural network; Internal structure; Inverse reinforcement learning; Learning from demonstration; Path planning problems; Proposed architectures; Space robotics; Reinforcement learning",Article,"Final",,Scopus,2-s2.0-85063311999
"Wang W., Li R., Chen Y., Diekel Z.M., Jia Y.","57201554862;57201552259;57201551043;57201554266;57203623083;","Facilitating Human-Robot Collaborative Tasks by Teaching-Learning-Collaboration from Human Demonstrations",2019,"IEEE Transactions on Automation Science and Engineering","16","2", 8403899,"640","653",,6,"10.1109/TASE.2018.2840345","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049471588&doi=10.1109%2fTASE.2018.2840345&partnerID=40&md5=caf1f464ddd4d405d2c4013adae6e7a8","Department of Automotive Engineering, Clemson University, Greenville, SC  29607, United States","Wang, W., Department of Automotive Engineering, Clemson University, Greenville, SC  29607, United States; Li, R., Department of Automotive Engineering, Clemson University, Greenville, SC  29607, United States; Chen, Y., Department of Automotive Engineering, Clemson University, Greenville, SC  29607, United States; Diekel, Z.M., Department of Automotive Engineering, Clemson University, Greenville, SC  29607, United States; Jia, Y., Department of Automotive Engineering, Clemson University, Greenville, SC  29607, United States","Human-centered manufacture; human-robot collaboration; learning from demonstrations; maximum entropy inverse reinforcement learning (MaxEnt-IRL); natural language","Demonstrations; Entropy; Ergonomics; Intelligent robots; Knowledge management; Learning algorithms; Manufacture; Collaboration process; Collaborative assembly; Human demonstrations; Human-robot collaboration; Intelligent Manufacturing; Inverse reinforcement learning; Learning from demonstration; Natural languages; Reinforcement learning",Article,"Final",,Scopus,2-s2.0-85049471588
"Brooks C., Szafir D.","57188741380;50362161600;","Balanced Information Gathering and Goal-Oriented Actions in Shared Autonomy",2019,"ACM/IEEE International Conference on Human-Robot Interaction","2019-March",, 8673192,"85","94",,,"10.1109/HRI.2019.8673192","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064003646&doi=10.1109%2fHRI.2019.8673192&partnerID=40&md5=424a322c45313ab54b0ea0c382b4b911","Department of Computer Science, University of Colorado Boulder, Boulder, United States; Department of Computer Science, ATLAS Institute, University of Colorado Boulder, Boulder, United States","Brooks, C., Department of Computer Science, University of Colorado Boulder, Boulder, United States; Szafir, D., Department of Computer Science, ATLAS Institute, University of Colorado Boulder, Boulder, United States","active learning; assistive teleoperation; human-robot teaming; inverse reinforcement learning; Shared autonomy","Artificial intelligence; Control theory; Degrees of freedom (mechanics); Economic and social effects; Man machine systems; Manipulators; Reinforcement learning; Remote control; Active Learning; Assistive; Human robots; Inverse reinforcement learning; Shared autonomy; Human robot interaction",Conference Paper,"Final",,Scopus,2-s2.0-85064003646
"Krishnan S., Garg A., Liaw R., Thananjeyan B., Miller L., Pokorny F.T., Goldberg K.","56272628500;55561356700;57195417297;57195425489;57192208784;55314672100;35453491100;","SWIRL: A sequential windowed inverse reinforcement learning algorithm for robot tasks with delayed rewards",2019,"International Journal of Robotics Research","38","2-3",,"126","145",,1,"10.1177/0278364918784350","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052190280&doi=10.1177%2f0278364918784350&partnerID=40&md5=34ff0bf996ea99be0cac565416bdc71c","AUTOLAB, University of California, Berkeley, United States; Stanford University, United States; RPL/CSC, KTH Royal Institute of Technology, Sweden","Krishnan, S., AUTOLAB, University of California, Berkeley, United States; Garg, A., AUTOLAB, University of California, Berkeley, United States, Stanford University, United States; Liaw, R., AUTOLAB, University of California, Berkeley, United States; Thananjeyan, B., AUTOLAB, University of California, Berkeley, United States; Miller, L., AUTOLAB, University of California, Berkeley, United States; Pokorny, F.T., RPL/CSC, KTH Royal Institute of Technology, Sweden; Goldberg, K., AUTOLAB, University of California, Berkeley, United States","inverse reinforcement learning; learning from demonstrations; medical robots and systems; Reinforcement learning","Cloning; Deformation; Demonstrations; Learning algorithms; Robotic surgery; Robots; Surgery; Autonomous exploration; Behavioral cloning; Da vinci surgical robots; Inverse reinforcement learning; Learning from demonstration; Medical robots and systems; Physical experiments; Transition conditions; Reinforcement learning",Article,"Final",,Scopus,2-s2.0-85052190280
"Huang S.H., Held D., Abbeel P., Dragan A.D.","56422734600;35955893400;8269962600;55193779100;","Enabling robots to communicate their objectives",2019,"Autonomous Robots","43","2",,"309","326",,5,"10.1007/s10514-018-9771-0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048686549&doi=10.1007%2fs10514-018-9771-0&partnerID=40&md5=98b721ed244e1992157cfc865a3f201c","University of California, Berkeley, Berkeley, CA, United States; Carnegie Mellon University, Pittsburgh, PA, United States","Huang, S.H., University of California, Berkeley, Berkeley, CA, United States; Held, D., Carnegie Mellon University, Pittsburgh, PA, United States; Abbeel, P., University of California, Berkeley, Berkeley, CA, United States; Dragan, A.D., University of California, Berkeley, Berkeley, CA, United States","Explainable artificial intelligence; Human-robot interaction; Inverse reinforcement learning; Transparency","Cognitive systems; Human computer interaction; Intelligent robots; Reinforcement learning; Transparency; Appropriate models; Candidate models; Human learning; Inverse reinforcement learning; Mental model; Objective functions; Robot behavior; Robot model; Human robot interaction",Article,"Final",,Scopus,2-s2.0-85048686549
"Pan W., Qu R., Hwang K.-S., Lin H.-S.","57198514328;57207731019;7402426737;57208084484;","An Ensemble Fuzzy Approach for Inverse Reinforcement Learning",2019,"International Journal of Fuzzy Systems","21","1",,"95","103",,2,"10.1007/s40815-018-0535-y","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063725863&doi=10.1007%2fs40815-018-0535-y&partnerID=40&md5=ec720eef2adc7b168db34d223539130c","School of Computer Science, Northwestern Polytechnical University, Xi’an, China; Department of Electrical Engineering, National Sun Yat-Sen University, Kaohsiung, Taiwan","Pan, W., School of Computer Science, Northwestern Polytechnical University, Xi’an, China; Qu, R., School of Computer Science, Northwestern Polytechnical University, Xi’an, China; Hwang, K.-S., Department of Electrical Engineering, National Sun Yat-Sen University, Kaohsiung, Taiwan; Lin, H.-S., Department of Electrical Engineering, National Sun Yat-Sen University, Kaohsiung, Taiwan","Fuzzy; Inverse reinforcement learning; Reward feature","Demonstrations; Information theory; Inverse problems; Machine learning; Maximum likelihood; Apprenticeship learning; Fuzzy; Information-theoretic methods; Inverse reinforcement learning; Linear combinations; Reward feature; Sampling efficiency; Simulation demonstrate; Reinforcement learning",Article,"Final",,Scopus,2-s2.0-85063725863
"Zhang K., Yu Y.","57208822390;57210071874;","Methodologies for Imitation Learning via Inverse Reinforcement Learning: A Review [基于逆强化学习的示教学习方法综述]",2019,"Jisuanji Yanjiu yu Fazhan/Computer Research and Development","56","2",,"254","261",,,"10.7544/issn1000-1239.2019.20170578","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065870040&doi=10.7544%2fissn1000-1239.2019.20170578&partnerID=40&md5=3635cf1183521045d24b1351d4cec616","State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, 210023, China","Zhang, K., State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, 210023, China; Yu, Y., State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, 210023, China","Imitation learning; Inverse reinforcement learning; Markov decision process; Multi-step decision problem; Reinforcement learning","Behavioral research; Decision making; Inverse problems; Learning algorithms; Machine learning; Markov processes; Autonomous robotic systems; Decision problems; Imitation learning; Inverse reinforcement learning; Markov Decision Processes; Reinforcement learning approach; Reinforcement learning method; Sequential decision making; Reinforcement learning",Review,"Final",,Scopus,2-s2.0-85065870040
"Peysakhovich A.","55258497600;","Reinforcement learning and inverse reinforcement learning with system 1 and system 2",2019,"AIES 2019 - Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society",,,,"409","415",,,"10.1145/3306618.3314259","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85070585523&doi=10.1145%2f3306618.3314259&partnerID=40&md5=1378a3a0e54b12f4d90637873a59b823","Facebook AI Research","Peysakhovich, A., Facebook AI Research","Behavioral economics; Dual-system models; Inverse reinforcement learning; Reinforcement learning","Decision making; Economics; Inverse problems; Machine learning; Optimization; Philosophical aspects; Rational functions; Applications of AI; Behavioral economics; Competing models; Decision makers; Dual system; Inverse reinforcement learning; Markov decision problem; Reward function; Reinforcement learning",Conference Paper,"Final",,Scopus,2-s2.0-85070585523
"Scobee D.R.R., Rubies Royo V., Tomlin C.J., Sastry S.S.","55355309200;57207112216;56751013300;56111601700;","Haptic Assistance via Inverse Reinforcement Learning",2019,"Proceedings - 2018 IEEE International Conference on Systems, Man, and Cybernetics, SMC 2018",,, 8616258,"1510","1517",,,"10.1109/SMC.2018.00262","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062208365&doi=10.1109%2fSMC.2018.00262&partnerID=40&md5=22d49c40e7cdfed28db0dcdde5206feb","Department of Electrical Engineering and Computer Science, University of California, Berkeley, United States","Scobee, D.R.R., Department of Electrical Engineering and Computer Science, University of California, Berkeley, United States; Rubies Royo, V., Department of Electrical Engineering and Computer Science, University of California, Berkeley, United States; Tomlin, C.J., Department of Electrical Engineering and Computer Science, University of California, Berkeley, United States; Sastry, S.S., Department of Electrical Engineering and Computer Science, University of California, Berkeley, United States",,"Autonomous agents; Feedback; Haptic interfaces; Inverse problems; Machine learning; Remote control; Assistive; Control interfaces; Controlled system; Haptic assistance; Haptic feedbacks; Human users; Inverse reinforcement learning; User study; Reinforcement learning",Conference Paper,"Final",,Scopus,2-s2.0-85062208365
"Inga J., Eitel M., Flad M., Hohmann S.","57188985251;57207116884;6603246805;56027574000;","Evaluating Human Behavior in Manual and Shared Control via Inverse Optimization",2019,"Proceedings - 2018 IEEE International Conference on Systems, Man, and Cybernetics, SMC 2018",,, 8616457,"2699","2704",,2,"10.1109/SMC.2018.00461","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062238522&doi=10.1109%2fSMC.2018.00461&partnerID=40&md5=92327a6f4f3e3d4226005c4789426a93","Institute of Control Systems (IRS), Karlsruhe Institute of Technology (KIT), Karlsruhe, Germany","Inga, J., Institute of Control Systems (IRS), Karlsruhe Institute of Technology (KIT), Karlsruhe, Germany; Eitel, M., Institute of Control Systems (IRS), Karlsruhe Institute of Technology (KIT), Karlsruhe, Germany; Flad, M., Institute of Control Systems (IRS), Karlsruhe Institute of Technology (KIT), Karlsruhe, Germany; Hohmann, S., Institute of Control Systems (IRS), Karlsruhe Institute of Technology (KIT), Karlsruhe, Germany","Human Behavior Identification; Inverse Optimal Control; Inverse Reinforcement Learning; Shared Control","Cost functions; Cybernetics; Function evaluation; Inverse problems; Manual control; Reinforcement learning; Human behavior modeling; Human behaviors; Human machine interaction; Inverse reinforcement learning; Inverse-optimal control; Optimal control theory; Shared control; Shared-control systems; Behavioral research",Conference Paper,"Final",,Scopus,2-s2.0-85062238522
"Sama K., Morales Y., Akai N., Takeuchi E., Takeda K.","57202967306;24450513700;55671618700;15726701000;7404334995;","Learning How to Drive in Blind Intersections from Human Data",2019,"Proceedings - 2018 IEEE International Conference on Systems, Man, and Cybernetics, SMC 2018",,, 8616059,"317","324",,,"10.1109/SMC.2018.00064","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062214828&doi=10.1109%2fSMC.2018.00064&partnerID=40&md5=e4a38d9d84d37d02a1145412bfd00900","Institute of Innovation for Future Society, Nagoya University, Nagoya, 464-8603, Japan; Graduate School of Information Science, Nagoya University, Nagoya, 463-8603, Japan; Takeda Lab, Graduate School of Information Science, Nagoya University, Nagoya, 463-8603, Japan","Sama, K., Institute of Innovation for Future Society, Nagoya University, Nagoya, 464-8603, Japan; Morales, Y., Institute of Innovation for Future Society, Nagoya University, Nagoya, 464-8603, Japan; Akai, N., Institute of Innovation for Future Society, Nagoya University, Nagoya, 464-8603, Japan; Takeuchi, E., Graduate School of Information Science, Nagoya University, Nagoya, 463-8603, Japan; Takeda, K., Takeda Lab, Graduate School of Information Science, Nagoya University, Nagoya, 463-8603, Japan","autonomous driving; clustering; learning","Autonomous vehicles; Cybernetics; Digital storage; Drive-in facilities; Driver training; Reinforcement learning; Autonomous driving; Blind intersection; clustering; Different class; Inverse reinforcement learning; learning; Residential areas; Safe driving; Traffic control",Conference Paper,"Final",,Scopus,2-s2.0-85062214828
"Gaurav S., Ziebart B.","57195415451;13608719300;","Discriminatively learning inverse optimal control models for predicting human intentions",2019,"Proceedings of the International Joint Conference on Autonomous Agents and Multiagent Systems, AAMAS","3",,,"1368","1376",,1,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076919757&partnerID=40&md5=33dc7d96d62868d9d3af8a3ae368b146","University of Illinois at Chicago, Chicago, IL, United States","Gaurav, S., University of Illinois at Chicago, Chicago, IL, United States; Ziebart, B., University of Illinois at Chicago, Chicago, IL, United States","Goal prediction; Intent prediction; Inverse reinforcement learning; Maximum likelihood estimation","Autonomous agents; Computer aided design; Forecasting; Inverse problems; Machine learning; Multi agent systems; Reinforcement learning; Robots; Bayesian reasoning; Human intentions; Human robots; Inverse reinforcement learning; Inverse-optimal control; Partial sequences; Pointing tasks; Maximum likelihood estimation",Conference Paper,"Final",,Scopus,2-s2.0-85076919757
"Tang H., Wang A., Yang X.","25936833700;57211373973;57212206219;","Improved Bayesian inverse reinforcement learning based on demonstration and feedback",2019,"International Journal of Wireless and Mobile Computing","17","4",,"361","366",,,"10.1504/IJWMC.2019.103113","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073695652&doi=10.1504%2fIJWMC.2019.103113&partnerID=40&md5=41fb3bde9356838c586c13181c2df624","School of Information, Beijing Wuzi University, Beijing, 101149, China","Tang, H., School of Information, Beijing Wuzi University, Beijing, 101149, China; Wang, A., School of Information, Beijing Wuzi University, Beijing, 101149, China; Yang, X., School of Information, Beijing Wuzi University, Beijing, 101149, China","Bayesian rule; Demonstration and feedback; Inverse reinforcement learning; IRLDF algorithm","Demonstrations; Image enhancement; Inverse problems; Iterative methods; Learning algorithms; Machine learning; Bayesian; Bayesian rule; Interactive learning methods; Inverse reinforcement learning; Reward function; Traditional reinforcements; Reinforcement learning",Article,"Final",,Scopus,2-s2.0-85073695652
"Chen X.-L., Cao L., Xu Z.-X., Lai J., Li C.-X.","57192470874;57198528477;57194948633;57208789168;56103739800;","A Study of Continuous Maximum Entropy Deep Inverse Reinforcement Learning",2019,"Mathematical Problems in Engineering","2019",, 4834516,"","",,,"10.1155/2019/4834516","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065661758&doi=10.1155%2f2019%2f4834516&partnerID=40&md5=ec9b791f267d629157e927d83f2e0afb","Command and Control Engineering College, Army Engineering University, No. 1, Hai Fu Road, Guang Hua Road, Qin Huai District, Nanjing City, Jiangsu Province, 210007, China","Chen, X.-L., Command and Control Engineering College, Army Engineering University, No. 1, Hai Fu Road, Guang Hua Road, Qin Huai District, Nanjing City, Jiangsu Province, 210007, China; Cao, L., Command and Control Engineering College, Army Engineering University, No. 1, Hai Fu Road, Guang Hua Road, Qin Huai District, Nanjing City, Jiangsu Province, 210007, China; Xu, Z.-X., Command and Control Engineering College, Army Engineering University, No. 1, Hai Fu Road, Guang Hua Road, Qin Huai District, Nanjing City, Jiangsu Province, 210007, China; Lai, J., Command and Control Engineering College, Army Engineering University, No. 1, Hai Fu Road, Guang Hua Road, Qin Huai District, Nanjing City, Jiangsu Province, 210007, China; Li, C.-X., Command and Control Engineering College, Army Engineering University, No. 1, Hai Fu Road, Guang Hua Road, Qin Huai District, Nanjing City, Jiangsu Province, 210007, China",,"Demonstrations; Entropy; Learning algorithms; Machine learning; Reinforcement learning; Classical control; Continuous actions; Continuous maxima; Continuous State Space; Environment modeling; Inverse reinforcement learning; Optimal policies; Training process; Deep learning",Article,"Final",Open Access,Scopus,2-s2.0-85065661758
"Qureshi A.H., Yip M.C., Boots B.","55849495300;35868173800;10239214300;","Adversarial imitation via variational inverse reinforcement learning",2019,"7th International Conference on Learning Representations, ICLR 2019",,,,"","",,2,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071163274&partnerID=40&md5=ab8d39b5cc1125692a5b71a496439ebb","Department of Electrical and Computer Engineering, University of California San Diego, San Diego, CA  92093, United States; College of Computing, Georgia Institute of Technology, Atlanta, GA  30332, United States","Qureshi, A.H., Department of Electrical and Computer Engineering, University of California San Diego, San Diego, CA  92093, United States; Yip, M.C., Department of Electrical and Computer Engineering, University of California San Diego, San Diego, CA  92093, United States; Boots, B., College of Computing, Georgia Institute of Technology, Atlanta, GA  30332, United States",,"Inverse problems; Learning algorithms; Machine learning; Maximum entropy methods; Adversarial learning; Adversarial networks; High-dimensional; Information maximization; Inverse reinforcement learning; State of the art; Training and testing; Transfer learning; Reinforcement learning",Conference Paper,"Final",,Scopus,2-s2.0-85071163274
"Fu J., Korattikara A., Levine S., Guadarrama S.","57211527716;55208184500;35731728100;6506759870;","From language to goals: Inverse reinforcement learning for vision-based instruction following",2019,"7th International Conference on Learning Representations, ICLR 2019",,,,"","",,3,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071164474&partnerID=40&md5=e0380a9e6cd86be3dfb064a3c2891a9d","Google AI","Fu, J., Google AI; Korattikara, A., Google AI; Levine, S., Google AI; Guadarrama, S., Google AI",,"Deep neural networks; Inverse problems; Learning algorithms; Natural language processing systems; Reinforcement learning; Visual languages; Autonomous machines; Control problems; Grounding language; High-dimensional; Inverse reinforcement learning; Natural languages; Poor performance; Visual environments; Machine learning",Conference Paper,"Final",,Scopus,2-s2.0-85071164474
"Wei E., Wicke D., Luke S.","57190427295;56682590400;8931990100;","Multiagent adversarial inverse reinforcement learning",2019,"Proceedings of the International Joint Conference on Autonomous Agents and Multiagent Systems, AAMAS","4",,,"2265","2266",,,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077052760&partnerID=40&md5=a2caeb9dbda089e0e81b072602ddd235","George Mason University, Fairfax, VA, United States","Wei, E., George Mason University, Fairfax, VA, United States; Wicke, D., George Mason University, Fairfax, VA, United States; Luke, S., George Mason University, Fairfax, VA, United States","Adversarial learning; Deep reinforcement learning; Multiagent","Autonomous agents; Deep learning; Game theory; Inverse problems; Machine learning; Multi agent systems; Actor critic models; Adversarial learning; Coordinated behavior; Imitation learning; Inverse reinforcement learning; Multiagent; Overgeneralization; State-of-the-art approach; Reinforcement learning",Conference Paper,"Final",,Scopus,2-s2.0-85077052760
"Jarboui F., Gruson-Daniel C., Durmus A., Rocchisani V., Goulet Ebongue S.-H., Depoux A., Kirschenmann W., Perchet V.","57195918243;57208837447;56653659200;57195916372;57201896400;35868034100;35179800100;36020384800;","Markov decision process for MOOC users behavioral inference",2019,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","11475 LNCS",,,"70","80",,,"10.1007/978-3-030-19875-6_9","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065913448&doi=10.1007%2f978-3-030-19875-6_9&partnerID=40&md5=7ae40be25ba5d08eccaa70a6bebf151e","ANEO, Boulogne Billancourt, France; CMLA, École normale supérieur Paris Saclay, Université Paris Saclay, Paris, France; Centre Virchow-Villermé for Public Health Paris-Berlin, Université Sorbonne Paris-Cité, Paris, France; GRIPIC - EA 1498, Sorbonne Université, Paris, France; DRISS (Digital Research in Science & Society), Paris, France","Jarboui, F., ANEO, Boulogne Billancourt, France, CMLA, École normale supérieur Paris Saclay, Université Paris Saclay, Paris, France; Gruson-Daniel, C., Centre Virchow-Villermé for Public Health Paris-Berlin, Université Sorbonne Paris-Cité, Paris, France, DRISS (Digital Research in Science & Society), Paris, France; Durmus, A., CMLA, École normale supérieur Paris Saclay, Université Paris Saclay, Paris, France; Rocchisani, V., ANEO, Boulogne Billancourt, France; Goulet Ebongue, S.-H., Centre Virchow-Villermé for Public Health Paris-Berlin, Université Sorbonne Paris-Cité, Paris, France; Depoux, A., Centre Virchow-Villermé for Public Health Paris-Berlin, Université Sorbonne Paris-Cité, Paris, France, GRIPIC - EA 1498, Sorbonne Université, Paris, France; Kirschenmann, W., ANEO, Boulogne Billancourt, France; Perchet, V., CMLA, École normale supérieur Paris Saclay, Université Paris Saclay, Paris, France","Inverse Reinforcement Learning; Learning analytics; Markov Decision Process; User behaviour studies","Behavioral research; Education computing; Markov processes; Reinforcement learning; Inverse reinforcement learning; Learning analytics; Learning process; Log data; Markov Decision Processes; Massive open online course; User behaviour; E-learning",Conference Paper,"Final",,Scopus,2-s2.0-85065913448
"Alvarez N., Noda I.","55441230700;14719940800;","Inverse Reinforcement Learning for Agents Behavior in a Crowd Simulator",2019,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","11422 LNAI",,,"81","95",,,"10.1007/978-3-030-20937-7_6","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85066123482&doi=10.1007%2f978-3-030-20937-7_6&partnerID=40&md5=fc2f1fcadb6ca53730db7e4dcd44e09a","The National Institute of Advanced Industrial Science and Technology (AIST), Tokyo, Japan","Alvarez, N., The National Institute of Advanced Industrial Science and Technology (AIST), Tokyo, Japan; Noda, I., The National Institute of Advanced Industrial Science and Technology (AIST), Tokyo, Japan","Inverse reinforcement learning; Multi-agent systems; Pedestrian simulation","Behavioral research; Inverse problems; Machine learning; Reinforcement learning; Behavior model; Behavioral agents; Crowd behavior; Inverse reinforcement learning; ITS applications; Machine learning techniques; Pedestrian simulation; Working models; Multi agent systems",Conference Paper,"Final",,Scopus,2-s2.0-85066123482
"Ghasemipour S.K.S., Gu S., Zemel R.","57210570038;57192162618;7004912699;","Understanding the relation between maximum-entropy inverse reinforcement learning and behaviour cloning",2019,"Deep Generative Models for Highly Structured Data, DGS@ICLR 2019 Workshop",,,,"","",,,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071179615&partnerID=40&md5=5843354afcf90e0204f7bfe54e31cf6f","University of Toronto, Vector Institute, Canada; Google Brain, United States","Ghasemipour, S.K.S., University of Toronto, Vector Institute, Canada; Gu, S., Google Brain, United States; Zemel, R., University of Toronto, Vector Institute, Canada",,"Clone cells; Cloning; Decision making; Inverse problems; Machine learning; Bc methods; Continuous control; Control policy; Inverse reinforcement learning; State of the art; Reinforcement learning",Conference Paper,"Final",,Scopus,2-s2.0-85071179615
"Sun W., Yan D., Huang J., Sun C.","55726575000;57205265138;57211576777;54986916400;","Small-scale moving target detection in aerial image by deep inverse reinforcement learning",2019,"Soft Computing",,,,"","",,,"10.1007/s00500-019-04404-6","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074504736&doi=10.1007%2fs00500-019-04404-6&partnerID=40&md5=63f00eb84086cb814a6ccd4a7528ff70","School of Aerospace Science and Technology, Xidian University, Xi’an, 710071, China; Qian Xuesen Laboratory of Space Technology, China Academy of Space Technology, Beijing, 100094, China","Sun, W., School of Aerospace Science and Technology, Xidian University, Xi’an, 710071, China; Yan, D., School of Aerospace Science and Technology, Xidian University, Xi’an, 710071, China; Huang, J., School of Aerospace Science and Technology, Xidian University, Xi’an, 710071, China; Sun, C., Qian Xuesen Laboratory of Space Technology, China Academy of Space Technology, Beijing, 100094, China","Aerial image; Deep inverse reinforcement; Small-scale target detection","Antennas; Convolution; Image processing; Inverse problems; Machine learning; Network layers; Reinforcement learning; Aerial images; Candidate target; Fully connected networks; Inverse reinforcement learning; Moving target detection; Moving targets detections; Network modeling; Small scale; Deep learning",Article,"Article in Press",,Scopus,2-s2.0-85074504736
"Nakata Y., Arai S.","57208792335;14057611500;","Estimating consistent reward of expert in multiple dynamics via linear programming inverse reinforcement learning",2019,"Transactions of the Japanese Society for Artificial Intelligence","34","6", B-J23,"","",,,"10.1527/tjsai.B-J23","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074440530&doi=10.1527%2ftjsai.B-J23&partnerID=40&md5=e7a2a6b21d75d3e4986941b6714f197b","Department of Urban Environment Systems, Graduate School of Science and Engineering, Chiba University, Japan; Department of Urban Environment Systems, Graduate School of Engineering, Chiba University, Japan","Nakata, Y., Department of Urban Environment Systems, Graduate School of Science and Engineering, Chiba University, Japan; Arai, S., Department of Urban Environment Systems, Graduate School of Engineering, Chiba University, Japan","Inverse reinforcement learning; Linear programming","Decision making; Inverse problems; Linear programming; Machine learning; Inverse reinforcement learning; Linear programming problem; Objective functions; Reward function; Reinforcement learning",Article,"Final",Open Access,Scopus,2-s2.0-85074440530
"Arora S., Doshi P., Banerjee B.","57191058282;23008336000;7102466013;","Online inverse reinforcement learning under occlusion",2019,"Proceedings of the International Joint Conference on Autonomous Agents and Multiagent Systems, AAMAS","2",,,"1170","1178",,,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076970248&partnerID=40&md5=8bcf7cd30e16e078847989d605604e62","Dept of Computer Science, THINC Lab, University of Georgia, Athens, GA, United States; School of Computing Sciences and Computer Engineering, University of Southern Mississippi, Hattiesburg, MS, United States","Arora, S., Dept of Computer Science, THINC Lab, University of Georgia, Athens, GA, United States; Doshi, P., Dept of Computer Science, THINC Lab, University of Georgia, Athens, GA, United States; Banerjee, B., School of Computing Sciences and Computer Engineering, University of Southern Mississippi, Hattiesburg, MS, United States","Inverse reinforcement learning; Online learning; Reinforcement learning; Robot learning; Robotics",,Conference Paper,"Final",,Scopus,2-s2.0-85076970248
"Kamalaruban P., Devidze R., Cevher V., Singla A.","56045266800;57211757094;6506127004;24766779100;","Interactive teaching algorithms for inverse reinforcement learning",2019,"IJCAI International Joint Conference on Artificial Intelligence","2019-August",,,"2692","2700",,,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074928541&partnerID=40&md5=81714afc88954b0585b961814cd85cfe","LIONS, EPFL, Switzerland; Max Planck Institute for Software Systems (MPI-SWS), Germany","Kamalaruban, P., LIONS, EPFL, Switzerland; Devidze, R., Max Planck Institute for Software Systems (MPI-SWS), Germany; Cevher, V., LIONS, EPFL, Switzerland; Singla, A., Max Planck Institute for Software Systems (MPI-SWS), Germany",,,Conference Paper,"Final",,Scopus,2-s2.0-85074928541
"Nam C., Walker P., Li H., Lewis M., Sycara K.","35590319700;54785528900;57205670207;57200329926;7006431929;","Models of Trust in Human Control of Swarms With Varied Levels of Autonomy",2019,"IEEE Transactions on Human-Machine Systems",,,,"","",,,"10.1109/THMS.2019.2896845","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062147993&doi=10.1109%2fTHMS.2019.2896845&partnerID=40&md5=ae12b41bc2efe1a8db189c585f7bc718","Center for Robotics Research, Korea Institute of Science and Technology, Seoul 02792, South Korea (e-mail: cjnam@kist.re.kr).; Smart Information Flow Technologies, Minneapolis, MN 55401 USA (e-mail: pmwalk@gmail.com).; School of Information Science, University of Pittsburgh, Pittsburgh, PA 15260 USA (e-mail: huao.li@pitt.edu).; School of Information Science, University of Pittsburgh, Pittsburgh, PA 15260 USA (e-mail: mlewis@sis.pitt.edu).; Robotics Institute, Carnegie Mellon University, Pittsburgh, PA 15213 USA (e-mail: katia@cs.cmu.edu).","Nam, C., Center for Robotics Research, Korea Institute of Science and Technology, Seoul 02792, South Korea (e-mail: cjnam@kist.re.kr).; Walker, P., Smart Information Flow Technologies, Minneapolis, MN 55401 USA (e-mail: pmwalk@gmail.com).; Li, H., School of Information Science, University of Pittsburgh, Pittsburgh, PA 15260 USA (e-mail: huao.li@pitt.edu).; Lewis, M., School of Information Science, University of Pittsburgh, Pittsburgh, PA 15260 USA (e-mail: mlewis@sis.pitt.edu).; Sycara, K., Robotics Institute, Carnegie Mellon University, Pittsburgh, PA 15213 USA (e-mail: katia@cs.cmu.edu).","Automation; Computational modeling; Human&#x2013;robot interaction; human&#x2013;swarm interaction; multirobot systems; Predictive models; Robot kinematics; Robot sensing systems; swarm robotics; Task analysis; trust","Automation; Computation theory; Control theory; Inverse problems; Job analysis; Learning algorithms; Markov processes; Reinforcement learning; Swarm intelligence; Computational model; Multi-robot systems; Predictive models; Robot interactions; Robot kinematics; Robot sensing system; Swarm robotics; Task analysis; trust; Human robot interaction",Article in Press,"Article in Press",,Scopus,2-s2.0-85062147993
[No author name available],[No author id available],"36th International Conference on Machine Learning, ICML 2019",2019,"36th International Conference on Machine Learning, ICML 2019","2019-June",,,"","",13315,,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078071359&partnerID=40&md5=b4290c1ef49e778ae9d1ff80c42220ea",,"",,,Conference Review,"Final",,Scopus,2-s2.0-85078071359
[No author name available],[No author id available],"36th International Conference on Machine Learning, ICML 2019",2019,"36th International Conference on Machine Learning, ICML 2019","2019-June",,,"","",13315,,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078054621&partnerID=40&md5=acf4de121bc5ec781aa8a403eda34cba",,"",,,Conference Review,"Final",,Scopus,2-s2.0-85078054621
[No author name available],[No author id available],"36th International Conference on Machine Learning, ICML 2019",2019,"36th International Conference on Machine Learning, ICML 2019","2019-June",,,"","",13315,,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078038307&partnerID=40&md5=d655127b793dae15e9fe3add868749ec",,"",,,Conference Review,"Final",,Scopus,2-s2.0-85078038307
[No author name available],[No author id available],"36th International Conference on Machine Learning, ICML 2019",2019,"36th International Conference on Machine Learning, ICML 2019","2019-June",,,"","",13315,,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078013581&partnerID=40&md5=0a17bac0701f12daa57bf54410bf499e",,"",,,Conference Review,"Final",,Scopus,2-s2.0-85078013581
[No author name available],[No author id available],"36th International Conference on Machine Learning, ICML 2019",2019,"36th International Conference on Machine Learning, ICML 2019","2019-June",,,"","",13315,,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078009151&partnerID=40&md5=e34a59a852f421b7c48e5bd9677f4405",,"",,,Conference Review,"Final",,Scopus,2-s2.0-85078009151
"Brown D.S., Goo W., Nagarajan P., Niekum S.","57213913792;57210801544;57213686992;15080949400;","Extrapolating beyond suboptimal demonstrations via inverse reinforcement learning from observations",2019,"36th International Conference on Machine Learning, ICML 2019","2019-June",,,"1232","1241",,,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077968698&partnerID=40&md5=67d115e4871807c260e8e13a84a9380b","Department of Computer Science, University of Texas, Austin, United States; Preferred Networks, Japan","Brown, D.S., Department of Computer Science, University of Texas, Austin, United States; Goo, W., Department of Computer Science, University of Texas, Austin, United States; Nagarajan, P., Preferred Networks, Japan; Niekum, S., Department of Computer Science, University of Texas, Austin, United States",,"Benchmarking; Clouds; Deep learning; Demonstrations; Extrapolation; Inverse problems; Machine learning; Critical flaw; High quality; Imitation learning; Inverse reinforcement learning; Near-optimal; Reward function; State of the art; Reinforcement learning",Conference Paper,"Final",,Scopus,2-s2.0-85077968698
[No author name available],[No author id available],"International Workshop on Massively Multi-agent Systems, MMAS 2018",2019,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","11422 LNAI",,,"","",162,,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85066147022&partnerID=40&md5=3fd5278b947bbabcbbf4c287c7221976",,"",,,Conference Review,"Final",,Scopus,2-s2.0-85066147022
"Petousis P., Han S.X., Hsu W., Bui A.A.T.","57190854336;56612535000;55841504400;7005377593;","Generating reward functions using IRL towards individualized cancer screening",2019,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","11326 LNAI",,,"213","227",,,"10.1007/978-3-030-12738-1_16","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062514201&doi=10.1007%2f978-3-030-12738-1_16&partnerID=40&md5=3f62e4fcff9c89bb827c74b1c5f0ea53","UCLA Bioengineering Department, Los Angeles, CA  90095, United States; UCLA Department of Radiological Sciences, Los Angeles, CA  90095, United States","Petousis, P., UCLA Bioengineering Department, Los Angeles, CA  90095, United States; Han, S.X., UCLA Bioengineering Department, Los Angeles, CA  90095, United States; Hsu, W., UCLA Bioengineering Department, Los Angeles, CA  90095, United States, UCLA Department of Radiological Sciences, Los Angeles, CA  90095, United States; Bui, A.A.T., UCLA Bioengineering Department, Los Angeles, CA  90095, United States, UCLA Department of Radiological Sciences, Los Angeles, CA  90095, United States","Cancer screening; Maximum entropy inverse reinforcement learning; Partially-observable Markov decision processes","Behavioral research; Biological organs; Decision making; Machine learning; Markov processes; Reinforcement learning; Adaptive step size; Breast cancer screening; Cancer screening; Decision making tool; Inverse reinforcement learning; Multiplicative model; Partially observable Markov decision process; Retrospective screenings; Diseases",Conference Paper,"Final",,Scopus,2-s2.0-85062514201
"Kostrikov I., Agrawal K.K., Dwibedi D., Levine S., Tompson J.","56422342900;57210638157;56613465600;35731728100;56374097400;","Discriminator-actor-critic: Addressing sample inefficiency and reward bias in adversarial imitation learning",2019,"7th International Conference on Learning Representations, ICLR 2019",,,,"","",,3,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071165223&partnerID=40&md5=6b58929085a085231d621b6ea865a6c1","Courant Institute of Mathematical Sciences, New York University, New York, NY, United States; Google Brain, Mountain View, CA, United States; Google AI Residency Program","Kostrikov, I., Courant Institute of Mathematical Sciences, New York University, New York, NY, United States, Google Brain, Mountain View, CA, United States; Agrawal, K.K., Google Brain, Mountain View, CA, United States, Google AI Residency Program; Dwibedi, D., Google Brain, Mountain View, CA, United States, Google AI Residency Program; Levine, S., Google Brain, Mountain View, CA, United States; Tompson, J., Google Brain, Mountain View, CA, United States",,"Discriminators; Efficiency; Machine learning; Reinforcement learning; Absorbing state; Actor critic; Imitation learning; Inherent instability; Inverse reinforcement learning; Real-world; Reward function; Learning algorithms",Conference Paper,"Final",,Scopus,2-s2.0-85071165223
"Peng X.B., Kanazawa A., Toyer S., Abbeel P., Levine S.","57208216498;55430977000;57202418191;8269962600;35731728100;","Variational discriminator bottleneck: Improving imitation learning, inverse RL, and GANs by constraining information flow",2019,"7th International Conference on Learning Representations, ICLR 2019",,,,"","",,1,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071147079&partnerID=40&md5=7d44958bdd1bbbb5bb13f432252c9d3a","University of California, Berkeley, United States","Peng, X.B., University of California, Berkeley, United States; Kanazawa, A., University of California, Berkeley, United States; Toyer, S., University of California, Berkeley, United States; Abbeel, P., University of California, Berkeley, United States; Levine, S., University of California, Berkeley, United States",,"Discriminators; Image enhancement; Inverse problems; Learning algorithms; Adversarial learning; Imitation learning; Information bottleneck; Internal representation; Inverse reinforcement learning; Mutual informations; Stabilization methods; Video demonstration; Reinforcement learning",Conference Paper,"Final",,Scopus,2-s2.0-85071147079
"Lage I., Doshi-Velez F., Lifschitz D., Amir O.","57208445031;34874672900;57211744093;55023457200;","Toward robust policy summarization",2019,"Proceedings of the International Joint Conference on Autonomous Agents and Multiagent Systems, AAMAS","4",,,"2081","2083",,1,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069868681&partnerID=40&md5=aa27f20c7d8b7522f484beff1ce8af29","Harvard University, United States; Technion - Israel Institute of Technology, Israel","Lage, I., Harvard University, United States; Doshi-Velez, F., Harvard University, United States; Lifschitz, D., Technion - Israel Institute of Technology, Israel; Amir, O., Technion - Israel Institute of Technology, Israel","Explainable AI; Policy summarization","Behavioral research; Decision making; Extraction; Inverse problems; Multi agent systems; Reinforcement learning; Repair; Computational model; Decision making process; High quality reconstruction; Human-in-the-loop; Imitation learning; Inverse reinforcement learning; Reconstruction quality; Teaching problems; Autonomous agents",Conference Paper,"Final",,Scopus,2-s2.0-85069868681
"Massimo D., Ricci F.","56433358100;35270071500;","Users’ evaluation of next-POI recommendations",2019,"CEUR Workshop Proceedings","2435",,,"1","8",,,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072657853&partnerID=40&md5=20f33cb85c149369a9996faab90cd62d","Free University of Bolzano, Italy","Massimo, D., Free University of Bolzano, Italy; Ricci, F., Free University of Bolzano, Italy","Clustering; Inverse reinforcement learning; Recommender systems; User study","Recommender systems; Reinforcement learning; Behaviour models; Behavioural model; Clustering; Inverse reinforcement learning; Performance measure; System accuracy; Traditional approaches; User study; Behavioral research",Conference Paper,"Final",,Scopus,2-s2.0-85072657853
"Cushman F.","8286891500;","Rationalization is rational",2019,"Behavioral and Brain Sciences",,,,"","",,,"10.1017/S0140525X19001730","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85066405208&doi=10.1017%2fS0140525X19001730&partnerID=40&md5=952f9b92ec970813b43e6c0adbe18045","Harvard University, United States; Fiery Cushman, 1480 William James Hall, 33 Kirkland St., Cambridge, MA  02138, United States","Cushman, F., Harvard University, United States, Fiery Cushman, 1480 William James Hall, 33 Kirkland St., Cambridge, MA  02138, United States","Cognitive dissonance; Habitization; Inverse reinforcement learning; Rationalization; Reflective equilibrium; Representational exchange; Self-perception; Social learning; Theory of mind; Useful fiction","adult; article; decision making; defense mechanism; female; human; human experiment; male; perception; reinforcement; social learning; theory of mind",Article,"Article in Press",Open Access,Scopus,2-s2.0-85066405208
"Schwarting W., Pierson A., Alonso-Mora J., Karaman S., Rus D.","56724078600;56742977900;37057150200;24923242500;7004511052;","Social behavior for autonomous vehicles",2019,"Proceedings of the National Academy of Sciences of the United States of America","116","50",,"2492","24978",,1,"10.1073/pnas.1820676116","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076241999&doi=10.1073%2fpnas.1820676116&partnerID=40&md5=646282cf372473a85ad7a77987af02c0","Computer Science and Artificial Intelligence Laboratory, Massachusetts Institute of Technology, Cambridge, MA  02139, United States; Cognitive Robotics, Delft University of Technology, Delft, 2628 CD, Netherlands; Laboratory of Information and Decision Systems, Massachusetts Institute of Technology, Cambridge, MA  02139, United States","Schwarting, W., Computer Science and Artificial Intelligence Laboratory, Massachusetts Institute of Technology, Cambridge, MA  02139, United States; Pierson, A., Computer Science and Artificial Intelligence Laboratory, Massachusetts Institute of Technology, Cambridge, MA  02139, United States; Alonso-Mora, J., Cognitive Robotics, Delft University of Technology, Delft, 2628 CD, Netherlands; Karaman, S., Laboratory of Information and Decision Systems, Massachusetts Institute of Technology, Cambridge, MA  02139, United States; Rus, D., Computer Science and Artificial Intelligence Laboratory, Massachusetts Institute of Technology, Cambridge, MA  02139, United States","Autonomous driving; Game theory; Inverse reinforcement learning; Social compliance; Social Value Orientation","algorithm; altruism; article; decision making; human; human experiment; Nash equilibrium; prediction; reinforcement; simulation",Article,"Final",Open Access,Scopus,2-s2.0-85076241999
"Lee D., Srinivasan S., Doshi-Velez F.","57211757866;57213354376;34874672900;","Truly batch apprenticeship learning with deep successor features",2019,"IJCAI International Joint Conference on Artificial Intelligence","2019-August",,,"5909","5915",,,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074956612&partnerID=40&md5=5e62df2f6a96744818a60e9af042199b","SEAS, Harvard University, United States","Lee, D., SEAS, Harvard University, United States; Srinivasan, S., SEAS, Harvard University, United States; Doshi-Velez, F., SEAS, Harvard University, United States",,,Conference Paper,"Final",,Scopus,2-s2.0-85074956612
"Noothigattu R., Bouneffouf D., Mattei N., Chandra R., Madan P., Varshney K.R., Campbell M., Singh M., Rossi F.","57191410942;55210872800;35234406800;57211758567;57211752662;14069407800;7403371273;55443750200;56066648900;","Teaching AI agents ethical values using reinforcement learning and policy orchestration (extended abstract)",2019,"IJCAI International Joint Conference on Artificial Intelligence","2019-August",,,"6377","6381",,2,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074909469&partnerID=40&md5=36ed448567d247212ba774da12950e6f","IBM Research, Yorktown Heights, NY, United States; IBM Research, Cambridge, MA, United States; Carnegie Mellon University, Pittsburgh, PA, United States; Tulane University, New Orleans, LA, United States","Noothigattu, R., Carnegie Mellon University, Pittsburgh, PA, United States; Bouneffouf, D., IBM Research, Yorktown Heights, NY, United States; Mattei, N., Tulane University, New Orleans, LA, United States; Chandra, R., IBM Research, Cambridge, MA, United States; Madan, P., IBM Research, Cambridge, MA, United States; Varshney, K.R., IBM Research, Yorktown Heights, NY, United States; Campbell, M., IBM Research, Yorktown Heights, NY, United States; Singh, M., IBM Research, Yorktown Heights, NY, United States; Rossi, F., IBM Research, Yorktown Heights, NY, United States",,,Conference Paper,"Final",,Scopus,2-s2.0-85074909469
"Massimo D., Ricci F.","56433358100;35270071500;","Tangible decision-making in sensors augmented spaces",2019,"CEUR Workshop Proceedings","2495",,,"53","62",,,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075109550&partnerID=40&md5=ef43617360ed8dbcac2ec734551c6175","Free University of Bolzano, Italy","Massimo, D., Free University of Bolzano, Italy; Ricci, F., Free University of Bolzano, Italy","Clustering; Inverse reinforcement learning; Recommender systems; User Study","Artificial intelligence; Decision making; Online systems; Recommender systems; Reinforcement learning; Augmented environments; Clustering; Inverse reinforcement learning; Nearest neighbour; Off-line analysis; On-line decision makings; Points of interest; User study; Behavioral research",Conference Paper,"Final",,Scopus,2-s2.0-85075109550
"Yang S.Y., Yu Y., Almahdi S.","24802841600;57203202581;57194597306;","An investor sentiment reward-based trading system using Gaussian inverse reinforcement learning algorithm",2018,"Expert Systems with Applications","114",,,"388","401",,1,"10.1016/j.eswa.2018.07.056","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050940929&doi=10.1016%2fj.eswa.2018.07.056&partnerID=40&md5=7e4ed9ae031970e123d054c91c1b3d11","Financial Engineering Program, School of Business, Stevens Institute of Technology, 1 Castle Point on Hudson, Hoboken, NJ  07030, United States","Yang, S.Y., Financial Engineering Program, School of Business, Stevens Institute of Technology, 1 Castle Point on Hudson, Hoboken, NJ  07030, United States; Yu, Y., Financial Engineering Program, School of Business, Stevens Institute of Technology, 1 Castle Point on Hudson, Hoboken, NJ  07030, United States; Almahdi, S., Financial Engineering Program, School of Business, Stevens Institute of Technology, 1 Castle Point on Hudson, Hoboken, NJ  07030, United States","Inverse reinforcement learning; Investor sentiment; Sentiment reward; Support vector machine learning","Commerce; Costs; Inverse problems; Learning algorithms; Reinforcement learning; Signal processing; Support vector machines; Extraction mechanisms; Inverse reinforcement learning; Investor sentiments; Investor's sentiments; Market volatility; Sentiment reward; Transaction cost; Volatile markets; Investments",Article,"Final",,Scopus,2-s2.0-85050940929
"Kasenberg D., Arnold T., Scheutz M.","57201556232;56895720600;6603548841;","Norms, Rewards, and the Intentional Stance: Comparing Machine Learning Approaches to Ethical Training",2018,"AIES 2018 - Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society",,,,"184","190",,3,"10.1145/3278721.3278774","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045238240&doi=10.1145%2f3278721.3278774&partnerID=40&md5=03f064b4f58eab282156155a9d5dbca0","Tufts University, Medford, MA, United States","Kasenberg, D., Tufts University, Medford, MA, United States; Arnold, T., Tufts University, Medford, MA, United States; Scheutz, M., Tufts University, Medford, MA, United States","intentional stance; norm inference; value alignment","Behavioral research; Machine learning; Reinforcement learning; AI systems; Ethical training; intentional stance; Inverse reinforcement learning; Machine learning approaches; norm inference; Reward function; Philosophical aspects",Conference Paper,"Final",,Scopus,2-s2.0-85045238240
"Shen M., Habibi G., How J.P.","57187580800;24605330500;7006512768;","Transferable Pedestrian Motion Prediction Models at Intersections",2018,"IEEE International Conference on Intelligent Robots and Systems",,, 8593783,"4547","4553",,,"10.1109/IROS.2018.8593783","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062986089&doi=10.1109%2fIROS.2018.8593783&partnerID=40&md5=481b99898cdc8d569df95c60b7a23212","Department of Mechanical Engineering Massachusetts Institute of Technology (MIT), Aerospace Controls Laboratory (ACL), 77 Massachusetts Ave., Cambridge, MA, United States; Department of Aeronautics and Astronautics, ACL, MIT, United States","Shen, M., Department of Mechanical Engineering Massachusetts Institute of Technology (MIT), Aerospace Controls Laboratory (ACL), 77 Massachusetts Ave., Cambridge, MA, United States; Habibi, G., Department of Aeronautics and Astronautics, ACL, MIT, United States; How, J.P., Department of Aeronautics and Astronautics, ACL, MIT, United States",,"Forecasting; Intelligent robots; Learning algorithms; Reinforcement learning; Trajectories; Accurate prediction; Baseline accuracy; Inverse reinforcement learning; Non-negative sparse coding; Pedestrian motion; Pedestrian trajectories; Trajectory Planning; Motion estimation",Conference Paper,"Final",,Scopus,2-s2.0-85062986089
"Carey R.","57205675159;","Incorrigibility in the CIRL Framework",2018,"AIES 2018 - Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society",,,,"30","35",,,"10.1145/3278721.3278750","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061065414&doi=10.1145%2f3278721.3278750&partnerID=40&md5=e0611033fcf8297642824df480d04a87","Future of Humanity Institute, Oxford University, United Kingdom","Carey, R., Future of Humanity Institute, Oxford University, United Kingdom","ai safety; cirl; cooperative inverse reinforcement learning; corrigibility","Philosophical aspects; Reinforcement learning; cirl; corrigibility; Inverse reinforcement learning; Learning frameworks; Parameterized; Reward function; Technical sense; Probability distributions",Conference Paper,"Final",,Scopus,2-s2.0-85061065414
"Fahad M., Chen Z., Guo Y.","57210222946;57203908927;35205046400;","Learning How Pedestrians Navigate: A Deep Inverse Reinforcement Learning Approach",2018,"IEEE International Conference on Intelligent Robots and Systems",,, 8593438,"819","826",,1,"10.1109/IROS.2018.8593438","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062994046&doi=10.1109%2fIROS.2018.8593438&partnerID=40&md5=5debcc1b683e0be8357adcc47df132d7","Department of Electrical Computer Engineering, Stevens Institute of Technology, Hoboken, NJ  07030, United States","Fahad, M., Department of Electrical Computer Engineering, Stevens Institute of Technology, Hoboken, NJ  07030, United States; Chen, Z., Department of Electrical Computer Engineering, Stevens Institute of Technology, Hoboken, NJ  07030, United States; Guo, Y., Department of Electrical Computer Engineering, Stevens Institute of Technology, Hoboken, NJ  07030, United States",,"Air navigation; Behavioral research; Deep neural networks; Intelligent robots; Inverse problems; Large dataset; Machine learning; Maximum entropy methods; Navigation; Reinforcement learning; Trajectories; Evaluation results; Human navigation; Human robot Interaction (HRI); Inverse reinforcement learning; Pedestrian trajectories; Prediction accuracy; Social navigation; State-of-the-art methods; Human robot interaction",Conference Paper,"Final",,Scopus,2-s2.0-85062994046
"Wu G., Li Y., Bao J., Zheng Y., Ye J., Luo J.","57194610715;56193387600;36197972400;56382645500;57194611807;56463258200;","Human-Centric Urban Transit Evaluation and Planning",2018,"Proceedings - IEEE International Conference on Data Mining, ICDM","2018-November",, 8594879,"547","556",,1,"10.1109/ICDM.2018.00070","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061374967&doi=10.1109%2fICDM.2018.00070&partnerID=40&md5=b78c82340b6cb65795fc035e3b56d381","Worcester Polytechnic Institute, United States; JD Urban Computing Business Unit, United States; JD Intelligent City Research, United States; DiDi AI Labs, United States; Machine Intelligence Center, Lenovo Group Limited, United States","Wu, G., Worcester Polytechnic Institute, United States; Li, Y., Worcester Polytechnic Institute, United States; Bao, J., JD Urban Computing Business Unit, United States, JD Intelligent City Research, United States; Zheng, Y., JD Urban Computing Business Unit, United States, JD Intelligent City Research, United States; Ye, J., DiDi AI Labs, United States; Luo, J., Machine Intelligence Center, Lenovo Group Limited, United States","Human-centric transit plan evaluation; Inverse reinforcement learning; Urban computing","Behavioral research; Learning algorithms; Markov processes; Motor transportation; Railroads; Rapid transit; Reinforcement learning; Traffic congestion; Urban transportation; Evaluation results; Inverse reinforcement learning; Markov Decision Processes; Plan evaluation; Preference learning; Road network traffic; Urban computing; Urban traffic congestion; Data mining",Conference Paper,"Final",,Scopus,2-s2.0-85061374967
"Hirakawa T., Yamashita T., Yoda K., Tamaki T., Fujiyoshi H.","55903063300;18042839400;7101801572;16204110300;6602601896;","Travel time-dependent maximum entropy inverse reinforcement learning for seabird trajectory prediction",2018,"Proceedings - 4th Asian Conference on Pattern Recognition, ACPR 2017",,, 8575862,"436","441",,,"10.1109/ACPR.2017.20","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060540464&doi=10.1109%2fACPR.2017.20&partnerID=40&md5=9b4df47ded97494301a45397999c21c0","Chubu University, Japan; Nagoya University, Japan; Hiroshima University, Japan","Hirakawa, T., Chubu University, Japan; Yamashita, T., Chubu University, Japan; Yoda, K., Nagoya University, Japan; Tamaki, T., Hiroshima University, Japan; Fujiyoshi, H., Chubu University, Japan","Animal Behavior Analysis; Markov Decision Process; Maximum Entropy Inverse Reinforcement Learning; Trajectory Prediction","Computer vision; Forecasting; Inverse problems; Learning algorithms; Machine learning; Markov processes; Reinforcement learning; Trajectories; Travel time; Animal behavior; Avoiding obstacle; Goal directed; Inverse reinforcement learning; Markov Decision Processes; Number of methods; Trajectory prediction; Maximum entropy methods",Conference Paper,"Final",,Scopus,2-s2.0-85060540464
"Sun L., Zhan W., Tomizuka M.","56007948700;57193013312;35567325500;","Probabilistic Prediction of Interactive Driving Behavior via Hierarchical Inverse Reinforcement Learning",2018,"IEEE Conference on Intelligent Transportation Systems, Proceedings, ITSC","2018-November",, 8569453,"2111","2117",,8,"10.1109/ITSC.2018.8569453","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060465934&doi=10.1109%2fITSC.2018.8569453&partnerID=40&md5=5b13b0da39d2cb5dd1373402becbc5a4","Department of Mechanical Engineering, University of California, Berkeley, CA  94720, United States","Sun, L., Department of Mechanical Engineering, University of California, Berkeley, CA  94720, United States; Zhan, W., Department of Mechanical Engineering, University of California, Berkeley, CA  94720, United States; Tomizuka, M., Department of Mechanical Engineering, University of California, Berkeley, CA  94720, United States",,"Behavioral research; Forecasting; Highway traffic control; Intelligent systems; Intelligent vehicle highway systems; Roads and streets; Trajectories; Vehicles; Autonomous Vehicles; Hierarchical trajectory; Historical information; Human demonstrations; Inverse reinforcement learning; Mixture of distributions; Probabilistic prediction; Quantitative result; Reinforcement learning",Conference Paper,"Final",,Scopus,2-s2.0-85060465934
"Morales L.Y., Naoki A., Yoshihara Y., Murase H.","57205545886;57205546702;57007898500;7101900108;","Towards Predictive Driving through Blind Intersections",2018,"IEEE Conference on Intelligent Transportation Systems, Proceedings, ITSC","2018-November",, 8569931,"716","722",,3,"10.1109/ITSC.2018.8569931","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060457479&doi=10.1109%2fITSC.2018.8569931&partnerID=40&md5=97fdbb1221961744765b46794d3f8011","Institute of Innovation for Future Society, Nagoya University, Nagoya, 464-8603, Japan; Graduate School of Information Science, Nagoya University, Nagoya, 464-8603, Japan","Morales, L.Y., Institute of Innovation for Future Society, Nagoya University, Nagoya, 464-8603, Japan; Naoki, A., Institute of Innovation for Future Society, Nagoya University, Nagoya, 464-8603, Japan; Yoshihara, Y., Institute of Innovation for Future Society, Nagoya University, Nagoya, 464-8603, Japan; Murase, H., Graduate School of Information Science, Nagoya University, Nagoya, 464-8603, Japan",,"Cost functions; Facings; Intelligent systems; Intelligent vehicle highway systems; Reinforcement learning; Trajectories; Blind intersection; Dangerous situations; Driving features; Feature weight; Inverse reinforcement learning; Modified Hausdorff Distance; Predictive drivings; State of the art; Cost benefit analysis",Conference Paper,"Final",,Scopus,2-s2.0-85060457479
"Zhan W., Sun L., Hu Y., Li J., Tomizuka M.","57193013312;56007948700;57201981248;57201996480;35567325500;","Towards a Fatality-Aware Benchmark of Probabilistic Reaction Prediction in Highly Interactive Driving Scenarios",2018,"IEEE Conference on Intelligent Transportation Systems, Proceedings, ITSC","2018-November",, 8569785,"3274","3280",,7,"10.1109/ITSC.2018.8569785","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060498838&doi=10.1109%2fITSC.2018.8569785&partnerID=40&md5=2e122f2bc8445c448bed53dca4903820","Department of Mechanical Engineering, University of California, Berkeley, CA  94720, United States","Zhan, W., Department of Mechanical Engineering, University of California, Berkeley, CA  94720, United States; Sun, L., Department of Mechanical Engineering, University of California, Berkeley, CA  94720, United States; Hu, Y., Department of Mechanical Engineering, University of California, Berkeley, CA  94720, United States; Li, J., Department of Mechanical Engineering, University of California, Berkeley, CA  94720, United States; Tomizuka, M., Department of Mechanical Engineering, University of California, Berkeley, CA  94720, United States",,"Decision making; Forecasting; Intelligent systems; Intelligent vehicle highway systems; Motion planning; Problem solving; Reinforcement learning; Trajectories; Autonomous Vehicles; Interacting entities; Inverse reinforcement learning; Neural network (nn); Probabilistic graphical models (PGM); Probabilistic prediction; Probabilistic reaction; Situation prediction; Inverse problems",Conference Paper,"Final",,Scopus,2-s2.0-85060498838
"Singh S., Lacotte J., Majumdar A., Pavone M.","56042918600;57202207899;55325126200;12240587200;","Risk-sensitive inverse reinforcement learning via semi- and non-parametric methods",2018,"International Journal of Robotics Research","37","13-14",,"1713","1740",,1,"10.1177/0278364918772017","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047389334&doi=10.1177%2f0278364918772017&partnerID=40&md5=49e47af409b3c83611f581feeeed702b","Department of Aeronautics and Astronautics, Stanford University, United States; Department of Electrical Engineering, Stanford University, United States; Department of Mechanical and Aerospace Engineering, Princeton University, United States","Singh, S., Department of Aeronautics and Astronautics, Stanford University, United States; Lacotte, J., Department of Electrical Engineering, Stanford University, United States; Majumdar, A., Department of Mechanical and Aerospace Engineering, Princeton University, United States; Pavone, M., Department of Aeronautics and Astronautics, Stanford University, United States","coherent risk measures; non-parametric method; risk-sensitive inverse reinforcement learning; semi-parametric method","Behavioral research; Cost functions; Health hazards; Inverse problems; Linear programming; Maximum likelihood; Reinforcement learning; Risk perception; Coherent risk measures; Dynamic decision making; Inverse reinforcement learning; Non-parametric algorithm; Nonparametric methods; Risk sensitivity; Semiparametric; Simulated driving; Risk assessment",Article,"Final",Open Access,Scopus,2-s2.0-85047389334
"Ezzeddine A., Mourad N., Araabi B.N., Ahmadabadi M.N.","57202777893;57202773807;35560470000;57189586135;","Combination of learning from non-optimal demonstrations and feedbacks using inverse reinforcement learning and Bayesian policy improvement",2018,"Expert Systems with Applications","112",,,"331","341",,,"10.1016/j.eswa.2018.06.035","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049349995&doi=10.1016%2fj.eswa.2018.06.035&partnerID=40&md5=a900d2e0d857d00c65794189adfcec98","Machine Learning and Computational Modeling Laboratory, School of ECE, College of Engineering, University of TehranTehran, Iran; Cognitive Systems Laboratory, School of ECE, College of Engineering, University of TehranTehran, Iran","Ezzeddine, A., Machine Learning and Computational Modeling Laboratory, School of ECE, College of Engineering, University of TehranTehran, Iran; Mourad, N., Cognitive Systems Laboratory, School of ECE, College of Engineering, University of TehranTehran, Iran; Araabi, B.N., Machine Learning and Computational Modeling Laboratory, School of ECE, College of Engineering, University of TehranTehran, Iran; Ahmadabadi, M.N., Cognitive Systems Laboratory, School of ECE, College of Engineering, University of TehranTehran, Iran","Human evaluative feedbacks; Interactive learning; Inverse reinforcement learning; Teaching by demonstrations","Air navigation; Demonstrations; Educational technology; Inverse problems; Iterative methods; Robots; Interactive learning; Inverse reinforcement learning; Learner Agent; Navigation tasks; Probabilistic modeling; Reward function; Teaching by demonstration; Transition model; Reinforcement learning",Article,"Final",,Scopus,2-s2.0-85049349995
"Li G., He B., Gomez R., Nakamura K.","56132381600;8355392200;12806763700;55489762000;","Interactive Reinforcement Learning from Demonstration and Human Evaluative Feedback",2018,"RO-MAN 2018 - 27th IEEE International Symposium on Robot and Human Interactive Communication",,, 8525837,"1156","1162",,1,"10.1109/ROMAN.2018.8525837","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058071354&doi=10.1109%2fROMAN.2018.8525837&partnerID=40&md5=aaaa1b008207c9a7ce324816b16d4ed1","College of Information Science and Engineering, Ocean University of China, Songling Road 238, Qingdao, 266100, China; Honda Research Institute Japan Ltd. Co., Wako, Japan","Li, G., College of Information Science and Engineering, Ocean University of China, Songling Road 238, Qingdao, 266100, China; He, B., College of Information Science and Engineering, Ocean University of China, Songling Road 238, Qingdao, 266100, China; Gomez, R., Honda Research Institute Japan Ltd. Co., Wako, Japan; Nakamura, K., Honda Research Institute Japan Ltd. Co., Wako, Japan",,"Demonstrations; Inverse problems; Robots; Discount factors; Incorrect action; Interactive Reinforcement Learning; Inverse reinforcement learning; Learning from demonstration; Model-based method; Natural interactions; Optimal policies; Reinforcement learning",Conference Paper,"Final",,Scopus,2-s2.0-85058071354
"Gao H., Shi G., Xie G., Cheng B.","57195674051;57205095621;57195683595;36611317200;","Car-following method based on inverse reinforcement learning for autonomous vehicle decision-making",2018,"International Journal of Advanced Robotic Systems","15","6",,"","",,3,"10.1177/1729881418817162","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058520448&doi=10.1177%2f1729881418817162&partnerID=40&md5=1fef28768842be0feec0041c987b8f09","State Key Laboratory of Automotive Safety and Energy, Tsinghua University, Beijing, China; Center for Intelligent Connected Vehicles and Transportation, Tsinghua University, Beijing, China; Electrical Engineering Department, California Institute of Technology, Pasadena, CA, United States; Department of Automotive Engineering, Hunan University, Changsha, Hunan, China","Gao, H., State Key Laboratory of Automotive Safety and Energy, Tsinghua University, Beijing, China, Center for Intelligent Connected Vehicles and Transportation, Tsinghua University, Beijing, China; Shi, G., Electrical Engineering Department, California Institute of Technology, Pasadena, CA, United States; Xie, G., Department of Automotive Engineering, Hunan University, Changsha, Hunan, China; Cheng, B., State Key Laboratory of Automotive Safety and Energy, Tsinghua University, Beijing, China","automatic driving; autonomous vehicle; Car-following; decision-making; inverse reinforcement learning (IRL)","Automobile drivers; Behavioral research; Data visualization; Decision making; Inverse problems; Learning algorithms; Automatic driving; Autonomous Vehicles; Car following; Decision-making systems; Driving characteristics; Inverse reinforcement learning; Optimization problems; Sequential decisions; Reinforcement learning",Article,"Final",Open Access,Scopus,2-s2.0-85058520448
"Sheffield E.C., Shah M.D.","57205078055;57190221969;","Dungeon digger: Apprenticeship learning for procedural dungeon building agents",2018,"CHI PLAY 2018 - Proceedings of the 2018 Annual Symposium on Computer-Human Interaction in Play Companion Extended Abstracts",,,,"603","610",,1,"10.1145/3270316.3271539","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058437749&doi=10.1145%2f3270316.3271539&partnerID=40&md5=d52053ed1174c8a49a6ac018a4129706","College of Computer and Information Science, Northeastern University, Boston, MA  02115, United States","Sheffield, E.C., College of Computer and Information Science, Northeastern University, Boston, MA  02115, United States; Shah, M.D., College of Computer and Information Science, Northeastern University, Boston, MA  02115, United States","Apprenticeship Learning; Inverse Reinforcement Learning; Machine Learning; Procedural Content Generation","Apprentices; Artificial intelligence; Interactive computer systems; Learning systems; Reinforcement learning; Static analysis; Apprenticeship learning; Digital games; Game Levels; Inverse reinforcement learning; Level design; Procedural content generations; Usability studies; Video game levels; Human computer interaction",Conference Paper,"Final",,Scopus,2-s2.0-85058437749
"Zou Q., Li H., Zhang R.","54406213100;57201675742;7404861962;","Inverse Reinforcement Learning via Neural Network in Driver Behavior Modeling",2018,"IEEE Intelligent Vehicles Symposium, Proceedings","2018-June",, 8500666,"1245","1250",,1,"10.1109/IVS.2018.8500666","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056785788&doi=10.1109%2fIVS.2018.8500666&partnerID=40&md5=2956ed31f8466b3b023f7e03611d0a13","BeiDou High Precision Positioning Service Technology Engineering Laboratory, Information Engineering College, Dalian University, DaLian, China; Mechanical and Electrical Engineering College, Dalian Minzu University, DaLian, China","Zou, Q., BeiDou High Precision Positioning Service Technology Engineering Laboratory, Information Engineering College, Dalian University, DaLian, China; Li, H., BeiDou High Precision Positioning Service Technology Engineering Laboratory, Information Engineering College, Dalian University, DaLian, China; Zhang, R., Mechanical and Electrical Engineering College, Dalian Minzu University, DaLian, China","Advanced Driver Assistance Systems; Automated Vehicles; Human Factors and Human Machine Interaction","Automobile drivers; Behavioral research; Decision making; Human computer interaction; Inverse problems; Markov processes; Neural networks; Reinforcement learning; Vehicles; Accuracy of decision makings; Automated vehicles; Convolutional neural network; Driver behavior modeling; Human machine interaction; Inverse reinforcement learning; Linear combinations; Markov Decision Processes; Advanced driver assistance systems",Conference Paper,"Final",,Scopus,2-s2.0-85056785788
"Hirakawa T., Yamashita T., Tamaki T., Fujiyoshi H., Umezu Y., Takeuchi I., Matsumoto S., Yoda K.","55903063300;18042839400;16204110300;6602601896;57200918462;35312449700;57045182000;7101801572;","Can AI predict animal movements? Filling gaps in animal trajectories using inverse reinforcement learning",2018,"Ecosphere","9","10", e02447,"","",,4,"10.1002/ecs2.2447","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055748412&doi=10.1002%2fecs2.2447&partnerID=40&md5=257baf0562befecafec54fdf9f9d4e6a","Department of Computer Science, Chubu University, 1200 Matsumoto, Kasugai, Aichi  487-0027, Japan; Graduate School of Engineering, Hiroshima University, 1-4-1 Kagamiyama, Higashi Hiroshima, Hiroshima  739-8527, Japan; Department of Computer Science, Nagoya Institute of Technology, Gokiso-cho, Showa-ku, Nagoya, 466-8555, Japan; RIKEN Center for Advanced Intelligence Project, 1-4-1 Nihonbashi, Chuo-ku, Tokyo, 103-0027, Japan; Center for Materials Research by Information Integration, National Institute for Materials Science, 1-2-1 Sengen, Tsukuba, 305-0047, Japan; Graduate School of Environmental Studies, Nagoya University, Furo, Chikusa, Nagoya, 464-8601, Japan","Hirakawa, T., Department of Computer Science, Chubu University, 1200 Matsumoto, Kasugai, Aichi  487-0027, Japan; Yamashita, T., Department of Computer Science, Chubu University, 1200 Matsumoto, Kasugai, Aichi  487-0027, Japan; Tamaki, T., Graduate School of Engineering, Hiroshima University, 1-4-1 Kagamiyama, Higashi Hiroshima, Hiroshima  739-8527, Japan; Fujiyoshi, H., Department of Computer Science, Chubu University, 1200 Matsumoto, Kasugai, Aichi  487-0027, Japan; Umezu, Y., Department of Computer Science, Nagoya Institute of Technology, Gokiso-cho, Showa-ku, Nagoya, 466-8555, Japan; Takeuchi, I., Department of Computer Science, Nagoya Institute of Technology, Gokiso-cho, Showa-ku, Nagoya, 466-8555, Japan, RIKEN Center for Advanced Intelligence Project, 1-4-1 Nihonbashi, Chuo-ku, Tokyo, 103-0027, Japan, Center for Materials Research by Information Integration, National Institute for Materials Science, 1-2-1 Sengen, Tsukuba, 305-0047, Japan; Matsumoto, S., Graduate School of Environmental Studies, Nagoya University, Furo, Chikusa, Nagoya, 464-8601, Japan; Yoda, K., Graduate School of Environmental Studies, Nagoya University, Furo, Chikusa, Nagoya, 464-8601, Japan","animal movement; behavioral monitoring; bio-logging; biotelemetry; Calonectris leucomelas; habitat selection; interpolation; inverse reinforcement learning; machine learning; reward map; tracking data",,Article,"Final",Open Access,Scopus,2-s2.0-85055748412
"Šošić A., Zoubir A.M., Rueckert E., Peters J., Koeppl H.","55416190200;35584414100;56743244800;35248912800;6603491586;","Inverse reinforcement learning via nonparametric spatio-temporal subgoal modeling",2018,"Journal of Machine Learning Research","19",,,"","",,,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057871867&partnerID=40&md5=9b87203aea94efef65c619b619ac54b7","Signal Processing Group, Technische Universität Darmstadt, Darmstadt, 64283, Germany; Institute for Robotics and Cognitive Systems, University of Lübeck, Lübeck, 23538, Germany; Autonomous Systems Labs, Technische Universität Darmstadt, Darmstadt, 64289, Germany; Bioinspired Communication Systems, Technische Universität Darmstadt, Darmstadt, 64283, Germany","Šošić, A., Signal Processing Group, Technische Universität Darmstadt, Darmstadt, 64283, Germany; Zoubir, A.M., Signal Processing Group, Technische Universität Darmstadt, Darmstadt, 64283, Germany; Rueckert, E., Institute for Robotics and Cognitive Systems, University of Lübeck, Lübeck, 23538, Germany; Peters, J., Autonomous Systems Labs, Technische Universität Darmstadt, Darmstadt, 64289, Germany; Koeppl, H., Bioinspired Communication Systems, Technische Universität Darmstadt, Darmstadt, 64283, Germany","Bayesian Nonparametric Modeling; Gibbs Sampling; Graphical Models; Inverse Reinforcement Learning; Learning from Demonstration; Subgoal Inference","Demonstrations; Inverse problems; Software agents; Bayesian nonparametric modeling; Gibbs sampling; GraphicaL model; Inverse reinforcement learning; Learning from demonstration; Subgoals; Reinforcement learning",Article,"Final",,Scopus,2-s2.0-85057871867
"Bogert K., Doshi P.","36095803300;23008336000;","Multi-robot inverse reinforcement learning under occlusion with estimation of state transitions",2018,"Artificial Intelligence","263",,,"46","73",,,"10.1016/j.artint.2018.07.002","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050363079&doi=10.1016%2fj.artint.2018.07.002&partnerID=40&md5=434c2a884a42d57e2e7d8b3a676c010a","Department of Computer Science, University of North Carolina Asheville, Asheville, NC  28804, United States; THINC Lab, Department of Computer Science, University of Georgia, Athens, GA  30602, United States","Bogert, K., Department of Computer Science, University of North Carolina Asheville, Asheville, NC  28804, United States; Doshi, P., THINC Lab, Department of Computer Science, University of Georgia, Athens, GA  30602, United States","Inverse reinforcement learning; Machine learning; Maximum entropy; Robotics","Learning systems; Maximum entropy methods; Reinforcement learning; Robotics; Robots; Application scenario; Future position; Inverse reinforcement learning; Learning agents; Learning from demonstration; Reward function; Robotic applications; State transitions; Inverse problems",Article,"Final",,Scopus,2-s2.0-85050363079
"Zhang R., Zhang S., Tong M.H., Cui Y., Rothkopf C.A., Ballard D.H., Hayhoe M.M.","57184546900;57195962668;16426328500;57204585882;15078701200;35809594800;7004513666;","Modeling sensory-motor decisions in natural behavior",2018,"PLoS Computational Biology","14","10", e1006518,"","",,,"10.1371/journal.pcbi.1006518","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056274489&doi=10.1371%2fjournal.pcbi.1006518&partnerID=40&md5=89ff249dd77c10675e3fedb38376bbf1","Department of Computer Science, The University of Texas at Austin, Austin, TX, United States; Computer Science and Engineering, University of Michigan, Ann Arbor, MI, United States; Center for Perceptual Systems, The University of Texas at Austin, Austin, TX, United States; Cognitive Science Center and Institute of Psychology, Technical University Darmstadt, Darmstadt, Germany","Zhang, R., Department of Computer Science, The University of Texas at Austin, Austin, TX, United States; Zhang, S., Computer Science and Engineering, University of Michigan, Ann Arbor, MI, United States; Tong, M.H., Center for Perceptual Systems, The University of Texas at Austin, Austin, TX, United States; Cui, Y., Department of Computer Science, The University of Texas at Austin, Austin, TX, United States; Rothkopf, C.A., Cognitive Science Center and Institute of Psychology, Technical University Darmstadt, Darmstadt, Germany; Ballard, D.H., Department of Computer Science, The University of Texas at Austin, Austin, TX, United States; Hayhoe, M.M., Center for Perceptual Systems, The University of Texas at Austin, Austin, TX, United States",,"adult; article; female; human; human experiment; learning algorithm; male; prediction; reinforcement; reward; virtual reality; algorithm; biological model; biology; decision making; physiology; psychomotor performance; reinforcement; Algorithms; Computational Biology; Decision Making; Humans; Models, Biological; Psychomotor Performance; Reinforcement (Psychology); Reward",Article,"Final",Open Access,Scopus,2-s2.0-85056274489
"Massimo D., Ricci F.","56433358100;35270071500;","Harnessing a generalised user behaviour model for next-POI recommendation",2018,"RecSys 2018 - 12th ACM Conference on Recommender Systems",,,,"402","406",,5,"10.1145/3240323.3240392","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056800692&doi=10.1145%2f3240323.3240392&partnerID=40&md5=1ec935bc808b253e9dbc2cb395d6c23b","Free University of Bolzano-Bozen Bolzano, Italy","Massimo, D., Free University of Bolzano-Bozen Bolzano, Italy; Ricci, F., Free University of Bolzano-Bozen Bolzano, Italy","Inverse Reinforcement Learning; Recommender Systems; Tourism; User Modelling","Decision making; Recommender systems; Reinforcement learning; Action prediction; Experimental analysis; Human decision making; Inverse reinforcement learning; Points of interest; Recommendation strategies; Tourism; User Modelling; Behavioral research",Conference Paper,"Final",,Scopus,2-s2.0-85056800692
"Cui Y., Niekum S.","57204585882;15080949400;","Active Reward Learning from Critiques",2018,"Proceedings - IEEE International Conference on Robotics and Automation",,, 8460854,"6907","6914",,2,"10.1109/ICRA.2018.8460854","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063160591&doi=10.1109%2fICRA.2018.8460854&partnerID=40&md5=f38892345ad6d7646d538d58fc96c627","Department of Computer Science, University of Texas at Austin, Austin, TX  78712, United States","Cui, Y., Department of Computer Science, University of Texas at Austin, Austin, TX  78712, United States; Niekum, S., Department of Computer Science, University of Texas at Austin, Austin, TX  78712, United States",,"Demonstrations; Inverse problems; Machine learning; Reinforcement learning; Robot programming; Robotics; Trajectories; Active learning methods; Automatically generated; Expected informations; Inverse reinforcement learning; Learning from demonstration; Programming robots; Simulated domains; Trajectory segmentation; Learning algorithms",Conference Paper,"Final",,Scopus,2-s2.0-85063160591
"Li K., Rath M., Burdick J.W.","57207262277;55893097600;7103361644;","Inverse Reinforcement Learning via Function Approximation for Clinical Motion Analysis",2018,"Proceedings - IEEE International Conference on Robotics and Automation",,, 8460563,"610","617",,,"10.1109/ICRA.2018.8460563","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063164463&doi=10.1109%2fICRA.2018.8460563&partnerID=40&md5=ca197bf81bf63364b8ea0f1afd1b11bc","California Institute of Technology, Department of Mechanical and Civil Engineering, Pasadena, CA  91125, United States; University of California Los Angeles, Los Angeles, CA  90095, United States","Li, K., California Institute of Technology, Department of Mechanical and Civil Engineering, Pasadena, CA  91125, United States; Rath, M., University of California Los Angeles, Los Angeles, CA  90095, United States; Burdick, J.W., California Institute of Technology, Department of Mechanical and Civil Engineering, Pasadena, CA  91125, United States",,"Inverse problems; Machine learning; Motion analysis; Patient rehabilitation; Robotics; Function approximation; High-dimensional; Human movements; Inverse reinforcement learning; Linearly proportional; Optimality equation; Simulated environment; Spinal cord injuries (SCI); Reinforcement learning",Conference Paper,"Final",,Scopus,2-s2.0-85063164463
"Rehder E., Wirth F., Lauer M., Stiller C.","56724034800;57208073979;56207151400;7102694878;","Pedestrian Prediction by Planning Using Deep Neural Networks",2018,"Proceedings - IEEE International Conference on Robotics and Automation",,, 8460203,"5903","5908",,12,"10.1109/ICRA.2018.8460203","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061302737&doi=10.1109%2fICRA.2018.8460203&partnerID=40&md5=7947d3bbf3174e02d503ee26c325ecdb","Daimler RandD, Environment Perception, Sindelfingen, Germany; Institute of Measurement and Control Systems, Karlsruhe Institute of Technology (KIT), Karlsruhe, 76131, Germany","Rehder, E., Daimler RandD, Environment Perception, Sindelfingen, Germany; Wirth, F., Institute of Measurement and Control Systems, Karlsruhe Institute of Technology (KIT), Karlsruhe, 76131, Germany; Lauer, M., Institute of Measurement and Control Systems, Karlsruhe Institute of Technology (KIT), Karlsruhe, 76131, Germany; Stiller, C., Institute of Measurement and Control Systems, Karlsruhe Institute of Technology (KIT), Karlsruhe, 76131, Germany",,"Autonomous vehicles; Forecasting; Motion estimation; Reinforcement learning; Robotics; Behavior patterns; Convolutional networks; Entire system; Experimental validations; Inverse reinforcement learning; Mixture density function; Motion prediction; Planning stages; Deep neural networks",Conference Paper,"Final",,Scopus,2-s2.0-85061302737
"Shkurti F., Kakodkar N., Dudek G.","47762168300;57193132765;7006413576;","Model-Based Probabilistic Pursuit via Inverse Reinforcement Learning",2018,"Proceedings - IEEE International Conference on Robotics and Automation",,, 8463196,"7804","7811",,1,"10.1109/ICRA.2018.8463196","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063153304&doi=10.1109%2fICRA.2018.8463196&partnerID=40&md5=f932861979d5226fd748eff6de9bae2d","Center for Intelligent Machines (CIM), School of Computer Science, McGill University in Montreal, Mobile Robotics Lab, Canada","Shkurti, F., Center for Intelligent Machines (CIM), School of Computer Science, McGill University in Montreal, Mobile Robotics Lab, Canada; Kakodkar, N., Center for Intelligent Machines (CIM), School of Computer Science, McGill University in Montreal, Mobile Robotics Lab, Canada; Dudek, G., Center for Intelligent Machines (CIM), School of Computer Science, McGill University in Montreal, Mobile Robotics Lab, Canada",,"Inverse problems; Machine learning; Robot programming; Robotics; Robots; Semantics; Visual servoing; Combinatorial search; Domain-specific knowledge; Graph representation; Integrated prediction; Inverse reinforcement learning; Multiple satellites; Pursuit algorithms; Short-term behavior; Reinforcement learning",Conference Paper,"Final",,Scopus,2-s2.0-85063153304
"Guo M., Andersson S., Dimarogonas D.V.","55921188600;57196085202;6506281602;","Human-in-the-Loop Mixed-Initiative Control under Temporal Tasks",2018,"Proceedings - IEEE International Conference on Robotics and Automation",,, 8460793,"6395","6400",,1,"10.1109/ICRA.2018.8460793","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063131458&doi=10.1109%2fICRA.2018.8460793&partnerID=40&md5=836fbadebfa913c634c76c2fa906ee9f","School of Electrical Engineering, KTH Royal Institute of Technology, ACCESS Linnaeus CenterSE-100 44, Sweden","Guo, M., School of Electrical Engineering, KTH Royal Institute of Technology, ACCESS Linnaeus CenterSE-100 44, Sweden; Andersson, S., School of Electrical Engineering, KTH Royal Institute of Technology, ACCESS Linnaeus CenterSE-100 44, Sweden; Dimarogonas, D.V., School of Electrical Engineering, KTH Royal Institute of Technology, ACCESS Linnaeus CenterSE-100 44, Sweden",,"Controllers; Coordination reactions; Iterative methods; Reinforcement learning; Robotics; Robots; Temporal logic; Continuous controller; Hard and soft constraints; Human-in-the-loop; Human-in-the-loop simulations; Inverse reinforcement learning; Linear temporal logic; Mixed initiative; Online coordination; Robot programming",Conference Paper,"Final",,Scopus,2-s2.0-85063131458
"Perez-Higueras N., Caballero F., Merino L.","56414879200;8956620100;7003946345;","Learning Human-Aware Path Planning with Fully Convolutional Networks",2018,"Proceedings - IEEE International Conference on Robotics and Automation",,, 8460851,"5897","5902",,2,"10.1109/ICRA.2018.8460851","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063126724&doi=10.1109%2fICRA.2018.8460851&partnerID=40&md5=6f25a11a5ccf947faa39b0cc6901c7cc","Universidad Pablo de Olavide, School of Engineering, Crta. Utrera km 1, Seville, Spain","Perez-Higueras, N., Universidad Pablo de Olavide, School of Engineering, Crta. Utrera km 1, Seville, Spain; Caballero, F., Universidad Pablo de Olavide, School of Engineering, Crta. Utrera km 1, Seville, Spain; Merino, L., Universidad Pablo de Olavide, School of Engineering, Crta. Utrera km 1, Seville, Spain",,"Air navigation; Convolution; Learning algorithms; Neural networks; Random errors; Reinforcement learning; Robot programming; Robotics; Robots; Configuration space; Convolutional networks; Convolutional neural network; Inverse reinforcement learning; Path planning for robots; Rapidly-exploring random trees; Real trajectories; Social navigation; Motion planning",Conference Paper,"Final",,Scopus,2-s2.0-85063126724
"Gonzalez D.S., Erkent O., Romero-Cano V., Dibangoye J., Laugier C.","56382000700;16549364800;55920292800;25640845700;7007031683;","Modeling Driver Behavior from Demonstrations in Dynamic Environments Using Spatiotemporal Lattices",2018,"Proceedings - IEEE International Conference on Robotics and Automation",,, 8460208,"3384","3390",,1,"10.1109/ICRA.2018.8460208","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063162728&doi=10.1109%2fICRA.2018.8460208&partnerID=40&md5=8ba2c7dc745aa5dfea2919ced1ae3097","Inria Rhone-Alpes, Grenoble, France; Universidad Autónoma de Occidente, Cali, Colombia","Gonzalez, D.S., Inria Rhone-Alpes, Grenoble, France; Erkent, O., Inria Rhone-Alpes, Grenoble, France; Romero-Cano, V., Universidad Autónoma de Occidente, Cali, Colombia; Dibangoye, J., Inria Rhone-Alpes, Grenoble, France; Laugier, C., Inria Rhone-Alpes, Grenoble, France",,"Collision avoidance; Cost functions; Motion planning; Reinforcement learning; Robotics; Trajectories; Vehicles; Computational power; Dynamic environments; Dynamic obstacles; Instrumented vehicle; Inverse reinforcement learning; Model assessment; Model parameters; Path planning techniques; Inverse problems",Conference Paper,"Final",,Scopus,2-s2.0-85063162728
"Zhou Z., Bloem M., Bambos N.","55971412900;23570762400;7004840524;","Infinite Time Horizon Maximum Causal Entropy Inverse Reinforcement Learning",2018,"IEEE Transactions on Automatic Control","63","9", 8115277,"2787","2802",,3,"10.1109/TAC.2017.2775960","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85035769716&doi=10.1109%2fTAC.2017.2775960&partnerID=40&md5=9ff127d1b5cd0f94f452dd030dbc991c","Department of Electrical Engineering, Department of Management Science and Engineering, Stanford University, Stanford, CA  94305, United States; Advanced Analytics, Steelcase, Inc., Grand Rapids, MI  49508, United States","Zhou, Z., Department of Electrical Engineering, Department of Management Science and Engineering, Stanford University, Stanford, CA  94305, United States; Bloem, M., Advanced Analytics, Steelcase, Inc., Grand Rapids, MI  49508, United States; Bambos, N., Department of Electrical Engineering, Department of Management Science and Engineering, Stanford University, Stanford, CA  94305, United States","Decision making; learning (artificial intelligence); Markov processes; optimization","Behavioral research; Convex optimization; Decision making; Entropy; Inverse problems; Markov processes; Analytical properties; Convex optimization problems; Efficient computation; Gradient based algorithm; Infinite time horizon; Inverse reinforcement learning; Markov Decision Processes; Optimization programs; Reinforcement learning",Article,"Final",,Scopus,2-s2.0-85035769716
"Kangasrääsiö A., Kaski S.","56188897500;35302360900;","Inverse reinforcement learning from summary data",2018,"Machine Learning","107","8-10",,"1517","1535",,2,"10.1007/s10994-018-5730-4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049083108&doi=10.1007%2fs10994-018-5730-4&partnerID=40&md5=b72fde9cd37c2f3ced115271761e3e38","Department of Computer Science, Aalto University, Espoo, Finland","Kangasrääsiö, A., Department of Computer Science, Aalto University, Espoo, Finland; Kaski, S., Department of Computer Science, Aalto University, Espoo, Finland","Approximate Bayesian computation; Bayesian inference; Inverse reinforcement learning; Monte-Carlo estimation","Bayesian networks; Inference engines; Inverse problems; Uncertainty analysis; Approximate Bayesian; Bayesian inference; Exact and approximate inferences; Inverse reinforcement learning; Monte-Carlo estimation; Parameter uncertainty; Reinforcement learning models; Task completion time; Reinforcement learning",Article,"Final",Open Access,Scopus,2-s2.0-85049083108
"Bezzo N.","36547454800;","Predicting Malicious Intention in CPS under Cyber-Attack",2018,"Proceedings - 9th ACM/IEEE International Conference on Cyber-Physical Systems, ICCPS 2018",,, 8443756,"351","352",,,"10.1109/ICCPS.2018.00049","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053515139&doi=10.1109%2fICCPS.2018.00049&partnerID=40&md5=85840601a39fd91aeb4fa4d1818fac2d","Department of Systems and Information Engineering, Department of Electrical and Computer Engineering, University of Virginia, United States","Bezzo, N., Department of Systems and Information Engineering, Department of Electrical and Computer Engineering, University of Virginia, United States","autonomous vehicles; CPS; cyber security; machine learning; malicious intention; reachability analysis; resileincy; robotics; sensor spoofing","Crime; Cyber Physical System; Embedded systems; Forecasting; Inverse problems; Learning systems; Network security; Reinforcement learning; Risk assessment; Robotics; Autonomous Vehicles; Cyber security; malicious intention; Reachability analysis; resileincy; Computer crime",Conference Paper,"Final",,Scopus,2-s2.0-85053515139
"Kamalapurkar R.","55210138200;","Linear inverse reinforcement learning in continuous time and space",2018,"Proceedings of the American Control Conference","2018-June",, 8431430,"1683","1688",,1,"10.23919/ACC.2018.8431430","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052564010&doi=10.23919%2fACC.2018.8431430&partnerID=40&md5=a7f69319e3a63eee4b97b01d974621db","Oklahoma State University, School of Mechanical and Aerospace Engineering, United States","Kamalapurkar, R., Oklahoma State University, School of Mechanical and Aerospace Engineering, United States",,,Conference Paper,"Final",,Scopus,2-s2.0-85052564010
"Imani M., Braga-Neto U.","57189031055;6603390822;","Optimal Control of Gene Regulatory Networks with Unknown Cost Function",2018,"Proceedings of the American Control Conference","2018-June",, 8431514,"3939","3944",,2,"10.23919/ACC.2018.8431514","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052573917&doi=10.23919%2fACC.2018.8431514&partnerID=40&md5=1c71a2419ebfe49214826375d313bd5e","Texas AM University, Department of Electrical and Computer Engineering, College Station, TX, United States","Imani, M., Texas AM University, Department of Electrical and Computer Engineering, College Station, TX, United States; Braga-Neto, U., Texas AM University, Department of Electrical and Computer Engineering, College Station, TX, United States",,,Conference Paper,"Final",,Scopus,2-s2.0-85052573917
"Ouattara A., Aswani A.","57203659630;23391951400;","Duality Approach to Bilevel Programs with a Convex Lower Level",2018,"Proceedings of the American Control Conference","2018-June",, 8431802,"1388","1395",,,"10.23919/ACC.2018.8431802","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052592802&doi=10.23919%2fACC.2018.8431802&partnerID=40&md5=b4cc2b7588691958e06e4a96788aa645","University of California, Department of Industrial Engineering and Operations Research, Berkeley, CA  94720, United States","Ouattara, A., University of California, Department of Industrial Engineering and Operations Research, Berkeley, CA  94720, United States; Aswani, A., University of California, Department of Industrial Engineering and Operations Research, Berkeley, CA  94720, United States",,,Conference Paper,"Final",,Scopus,2-s2.0-85052592802
"Elnaggar M., Bezzo N.","57196424673;36547454800;","An IRL Approach for Cyber-Physical Attack Intention Prediction and Recovery",2018,"Proceedings of the American Control Conference","2018-June",, 8430922,"222","227",,2,"10.23919/ACC.2018.8430922","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052605411&doi=10.23919%2fACC.2018.8430922&partnerID=40&md5=be92afe98b5c45424b356be77129ff34","Department of Electrical Computer Engineering, University of Virginia, Department of Systems Information Engineering, United States","Elnaggar, M., Department of Electrical Computer Engineering, University of Virginia, Department of Systems Information Engineering, United States; Bezzo, N., Department of Electrical Computer Engineering, University of Virginia, Department of Systems Information Engineering, United States",,,Conference Paper,"Final",,Scopus,2-s2.0-85052605411
"Mao J.-Y., Wu H., Sun W.-W.","57190986835;57118394600;55726565900;","Vehicle Trajectory Anomaly Detection in Road Network via Markov Decision Process [路网空间下基于马尔可夫决策过程的异常车辆轨迹检测算法]",2018,"Jisuanji Xuebao/Chinese Journal of Computers","41","8",,"1928","1942",,,"10.11897/SP.J.1016.2018.01928","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055440413&doi=10.11897%2fSP.J.1016.2018.01928&partnerID=40&md5=a4c000922f6f906a9150e5586503ad1c","School of Computer Science, Fudan University, Shanghai, 201203, China; Shanghai Key Laboratory of Data Science, Fudan University, Shanghai, 201203, China","Mao, J.-Y., School of Computer Science, Fudan University, Shanghai, 201203, China, Shanghai Key Laboratory of Data Science, Fudan University, Shanghai, 201203, China; Wu, H., School of Computer Science, Fudan University, Shanghai, 201203, China, Shanghai Key Laboratory of Data Science, Fudan University, Shanghai, 201203, China; Sun, W.-W., School of Computer Science, Fudan University, Shanghai, 201203, China, Shanghai Key Laboratory of Data Science, Fudan University, Shanghai, 201203, China","Anomaly detection; Location-based services; Markov decision process; Reinforcement learning; Trajectory computing","Behavioral research; Drops; Hidden Markov models; Inverse problems; Learning algorithms; Reinforcement learning; Roads and streets; Supervised learning; Telecommunication services; Telecommunication traffic; Trajectories; Vehicles; Anomaly detection; Inverse reinforcement learning; Markov chain Monte Carlo; Markov Decision Processes; Online anomaly detection; Origin and destinations; Positioning technologies; Semi- supervised learning; Location based services",Article,"Final",,Scopus,2-s2.0-85055440413
"Lelerre M., Mouaddib A.-I., Jeanpierre L.","57192427231;6701745605;24490808200;","Robust inverse planning approaches for policy estimation of semi-autonomous agents",2018,"Proceedings - International Conference on Tools with Artificial Intelligence, ICTAI","2017-November",,,"951","958",,,"10.1109/ICTAI.2017.00146","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048476528&doi=10.1109%2fICTAI.2017.00146&partnerID=40&md5=c8fd3f12b78525c91f9514f951adfb18","Universite de Caen Normandie, Caen, France","Lelerre, M., Universite de Caen Normandie, Caen, France; Mouaddib, A.-I., Universite de Caen Normandie, Caen, France; Jeanpierre, L., Universite de Caen Normandie, Caen, France","Activity and Plan Recognition; Inverse reinforcement learning; MDP","Artificial intelligence; Efficiency; Inverse problems; Learning algorithms; Reinforcement learning; Coordination technique; Inverse planning; Inverse reinforcement learning; Optimal policies; Plan recognition; Prediction methods; Semi-autonomous agents; Autonomous agents",Conference Paper,"Final",,Scopus,2-s2.0-85048476528
"Yamaguchi S., Naoki H., Ikeda M., Tsukada Y., Nakano S., Mori I., Ishii S.","57202389056;36608736300;57202389441;42862484600;7401622196;7201752391;7403110142;","Identification of animal behavioral strategies by inverse reinforcement learning",2018,"PLoS Computational Biology","14","5", e1006122,"","",,2,"10.1371/journal.pcbi.1006122","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048196142&doi=10.1371%2fjournal.pcbi.1006122&partnerID=40&md5=e7e3c630193f9442757a83682cb43263","Integrated Systems Biology Laboratory, Graduate School of Informatics, Kyoto University, Sakyo, Kyoto, Japan; Laboratory of Theoretical Biology, Graduate School of Biostudies, Kyoto University, Yoshidakonoecho, Sakyo, Kyoto, Japan; Data-driven Modeling Team, Research Center for Dynamic Living Systems, Graduate School of Biostudies, Kyoto University, Yoshidakonoecho, Sakyo, Kyoto, Japan; Group of Molecular Neurobiology, Graduate School of Science, Nagoya University, Furoucho, Chikusa, Nagoya, Aichi, Japan","Yamaguchi, S., Integrated Systems Biology Laboratory, Graduate School of Informatics, Kyoto University, Sakyo, Kyoto, Japan; Naoki, H., Laboratory of Theoretical Biology, Graduate School of Biostudies, Kyoto University, Yoshidakonoecho, Sakyo, Kyoto, Japan, Data-driven Modeling Team, Research Center for Dynamic Living Systems, Graduate School of Biostudies, Kyoto University, Yoshidakonoecho, Sakyo, Kyoto, Japan; Ikeda, M., Group of Molecular Neurobiology, Graduate School of Science, Nagoya University, Furoucho, Chikusa, Nagoya, Aichi, Japan; Tsukada, Y., Group of Molecular Neurobiology, Graduate School of Science, Nagoya University, Furoucho, Chikusa, Nagoya, Aichi, Japan; Nakano, S., Group of Molecular Neurobiology, Graduate School of Science, Nagoya University, Furoucho, Chikusa, Nagoya, Aichi, Japan; Mori, I., Group of Molecular Neurobiology, Graduate School of Science, Nagoya University, Furoucho, Chikusa, Nagoya, Aichi, Japan; Ishii, S., Integrated Systems Biology Laboratory, Graduate School of Informatics, Kyoto University, Sakyo, Kyoto, Japan",,"animal experiment; article; Caenorhabditis elegans; decision making; nerve cell; nonhuman; reinforcement; time series analysis; animal; animal behavior; biology; learning; physiology; taxis response; Animals; Behavior, Animal; Caenorhabditis elegans; Computational Biology; Decision Making; Learning; Reinforcement (Psychology); Taxis Response",Article,"Final",Open Access,Scopus,2-s2.0-85048196142
"Pérez-Higueras N., Caballero F., Merino L.","56414879200;8956620100;7003946345;","Teaching Robot Navigation Behaviors to Optimal RRT Planners",2018,"International Journal of Social Robotics","10","2",,"235","249",,6,"10.1007/s12369-017-0448-1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044769865&doi=10.1007%2fs12369-017-0448-1&partnerID=40&md5=3b7eaad23bf1d0bab4882fd7f02609a5","School of Engineering, Universidad Pablo de Olavide, Crta. Utrera km 1, Seville, Spain; Department of System Engineering and Automation, University of Seville, Camino de los Descubrimientos, s/n, Seville, Spain","Pérez-Higueras, N., School of Engineering, Universidad Pablo de Olavide, Crta. Utrera km 1, Seville, Spain; Caballero, F., Department of System Engineering and Automation, University of Seville, Camino de los Descubrimientos, s/n, Seville, Spain; Merino, L., School of Engineering, Universidad Pablo de Olavide, Crta. Utrera km 1, Seville, Spain","Learning from demonstration; Path planning; Social robots","Air navigation; Cost functions; Demonstrations; Intelligent robots; Inverse problems; Learning algorithms; Motion planning; Navigation; Reinforcement learning; Inverse reinforcement learning; Learning from demonstration; Navigation behavior; Rapidly-exploring random trees; Social navigation; Social robots; State-of-the-art algorithms; Teaching robots; Robot programming",Article,"Final",,Scopus,2-s2.0-85044769865
"González D.S., Romero-Cano V., DIbangoye J.S., Laugier C.","56382000700;55920292800;25640845700;7007031683;","Interaction-aware driver maneuver inference in highways using realistic driver models",2018,"IEEE Conference on Intelligent Transportation Systems, Proceedings, ITSC","2018-March",,,"1","8",,4,"10.1109/ITSC.2017.8317709","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046258026&doi=10.1109%2fITSC.2017.8317709&partnerID=40&md5=c7f71e3ed36f67f2ac86eefd0559ad8f","Inria Grenoble Rhône-Alpes, Montbonnot-Saint-Martin, 38330, France; Universidad Autónoma de Occidente, Cali, Colombia","González, D.S., Inria Grenoble Rhône-Alpes, Montbonnot-Saint-Martin, 38330, France; Romero-Cano, V., Universidad Autónoma de Occidente, Cali, Colombia; DIbangoye, J.S., Inria Grenoble Rhône-Alpes, Montbonnot-Saint-Martin, 38330, France; Laugier, C., Inria Grenoble Rhône-Alpes, Montbonnot-Saint-Martin, 38330, France",,"Intelligent systems; Intelligent vehicle highway systems; Reinforcement learning; State space methods; Behavior reasoning; False positive rates; Instrumented vehicle; Interacting multiple model filters; Inverse reinforcement learning; Lane change maneuvers; Scene understanding; State - space models; Forecasting",Conference Paper,"Final",,Scopus,2-s2.0-85046258026
"Massimo D.","56433358100;","User preference modeling and exploitation in IoT scenarios",2018,"International Conference on Intelligent User Interfaces, Proceedings IUI",,,,"675","676",,2,"10.1145/3172944.3173151","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045153051&doi=10.1145%2f3172944.3173151&partnerID=40&md5=a140c55ea7ee7fc4dbbc1a1b96503b75","Free University of Bozen-Bolzano, Bolzano, Italy","Massimo, D., Free University of Bozen-Bolzano, Bolzano, Italy","Behaviour learning; Internet of Things; Inverse reinforcement learning; Recommender System","Decision making; Internet of things; Recommender systems; Reinforcement learning; User interfaces; Behaviour learning; Collective dynamics; Human behaviours; Human decision making; Inverse reinforcement learning; Learning approach; Points of interest; User preference modeling; Behavioral research",Conference Paper,"Final",,Scopus,2-s2.0-85045153051
"Tan C., Li Y., Cheng Y.","57201779251;55719284400;9733966500;","An inverse reinforcement learning algorithm for semi-Markov decision processes",2018,"2017 IEEE Symposium Series on Computational Intelligence, SSCI 2017 - Proceedings","2018-January",,,"1","6",,1,"10.1109/SSCI.2017.8280816","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046081256&doi=10.1109%2fSSCI.2017.8280816&partnerID=40&md5=5b926860778e8d141fcb45f6da89c1e7","Harbin Institute of Technology Shenzhen Graduate School, Shenzhen, Guangdong Province, China; China University of Mining and Technology, Xuzhou, Jiangsu Province, China","Tan, C., Harbin Institute of Technology Shenzhen Graduate School, Shenzhen, Guangdong Province, China; Li, Y., Harbin Institute of Technology Shenzhen Graduate School, Shenzhen, Guangdong Province, China; Cheng, Y., China University of Mining and Technology, Xuzhou, Jiangsu Province, China","Inverse Reinforcement Learning; Optimization; Performance Sensitivity; Reward Function; SMDP","Artificial intelligence; Convex optimization; Inverse problems; Markov processes; Optimization; Reinforcement learning; Sensitivity analysis; Average reward; Convex optimization problems; Inverse reinforcement learning; Performance sensitivity; Reward function; Semi-Markov decision process; SMDP; Structural form; Learning algorithms",Conference Paper,"Final",,Scopus,2-s2.0-85046081256
"Tateo D., Pirotta M., Restelli M., Bonarini A.","57201780401;49362257000;6603404086;7003349352;","Gradient-based minimization for multi-expert Inverse Reinforcement Learning",2018,"2017 IEEE Symposium Series on Computational Intelligence, SSCI 2017 - Proceedings","2018-January",,,"1","8",,1,"10.1109/SSCI.2017.8280919","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046080056&doi=10.1109%2fSSCI.2017.8280919&partnerID=40&md5=bd97e3e2f0e1cd234cfb3f63e807ac51","Department of Electronics, Information and Bioengineering, Politecnico Di Milano, Milano, Italy","Tateo, D., Department of Electronics, Information and Bioengineering, Politecnico Di Milano, Milano, Italy; Pirotta, M., Department of Electronics, Information and Bioengineering, Politecnico Di Milano, Milano, Italy; Restelli, M., Department of Electronics, Information and Bioengineering, Politecnico Di Milano, Milano, Italy; Bonarini, A., Department of Electronics, Information and Bioengineering, Politecnico Di Milano, Milano, Italy",,"Iterative methods; Problem solving; Reinforcement learning; Direct problems; Gradient based; Human expert; Inverse reinforcement learning; Model-free method; Multi-expert; Reward function; Inverse problems",Conference Paper,"Final",,Scopus,2-s2.0-85046080056
"Bhattacharyya R., Hazarika S.M.","57203215262;24343762400;","Object Affordance Driven Inverse Reinforcement Learning Through Conceptual Abstraction and Advice",2018,"Paladyn","9","1",,"277","294",,,"10.1515/pjbr-2018-0021","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054413782&doi=10.1515%2fpjbr-2018-0021&partnerID=40&md5=9d854aa8b94ced757826204d35626860","Department of CSE, Indian Institute of Information Technology Bhagalpur, Bihar, India; Biomimetic and Cognitive Robotics Lab, Department of Mechanical Engineering, Indian Institute of Technology Guwahati, Assam, India","Bhattacharyya, R., Department of CSE, Indian Institute of Information Technology Bhagalpur, Bihar, India; Hazarika, S.M., Biomimetic and Cognitive Robotics Lab, Department of Mechanical Engineering, Indian Institute of Technology Guwahati, Assam, India","human intent recognition; inverse reinforcement learning; MDP; object affordance",,Article,"Final",Open Access,Scopus,2-s2.0-85054413782
"Xie N., Yang Y., Shen H.T., Zhao T.T.","57203385327;57208586877;7404523209;53664758000;","Stroke-based stylization by learning sequential drawing examples",2018,"Journal of Visual Communication and Image Representation","51",,,"29","39",,,"10.1016/j.jvcir.2017.12.012","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040464828&doi=10.1016%2fj.jvcir.2017.12.012&partnerID=40&md5=8f331ef3ebd1c327e184f6c3b7fd1b08","Center for Future Media, School of Computer Science and Engineering, University of Electronic Science and Technology of China, Chengdu, 611731, China; School of Computer Science and Information Engineering, Tianjin University of Science and Technology, Tianjin, 300457, China","Xie, N., Center for Future Media, School of Computer Science and Engineering, University of Electronic Science and Technology of China, Chengdu, 611731, China; Yang, Y., Center for Future Media, School of Computer Science and Engineering, University of Electronic Science and Technology of China, Chengdu, 611731, China; Shen, H.T., Center for Future Media, School of Computer Science and Engineering, University of Electronic Science and Technology of China, Chengdu, 611731, China; Zhao, T.T., School of Computer Science and Information Engineering, Tianjin University of Science and Technology, Tianjin, 300457, China","Inverse reinforcement learning; Reinforcement learning; Stroke-based stylization","Computer graphics; Gradient methods; Rendering (computer graphics); Brush stroke; Drawing styles; Inverse reinforcement learning; Non-Photorealistic Rendering; PhotoShop; Policy gradient methods; Stroke-based stylization; Reinforcement learning",Article,"Final",,Scopus,2-s2.0-85040464828
"Mazumdar E., Ratliff L.J., Fiez T., Shankar Sastry S.","57193098494;16424663700;57201799663;56964435900;","Gradient-based inverse risk-sensitive reinforcement learning",2018,"2017 IEEE 56th Annual Conference on Decision and Control, CDC 2017","2018-January",,,"5796","5801",,,"10.1109/CDC.2017.8264535","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046115944&doi=10.1109%2fCDC.2017.8264535&partnerID=40&md5=11f615a326d9b1b8780708560705989b","Electrical Engineering and Computer Sciences Department at the University of California, Berkeley, United States; Electrical Engineering Department at the University of Washington, Seattle, United States","Mazumdar, E., Electrical Engineering and Computer Sciences Department at the University of California, Berkeley, United States; Ratliff, L.J., Electrical Engineering and Computer Sciences Department at the University of California, Berkeley, United States; Fiez, T., Electrical Engineering and Computer Sciences Department at the University of California, Berkeley, United States; Shankar Sastry, S., Electrical Engineering Department at the University of Washington, Seattle, United States",,"Behavioral research; Decision making; Economics; Inverse problems; Learning algorithms; Markov processes; Travel time; Behavioral psychologies; Gradient based; Human decision making; Inverse reinforcement learning; Loss functions; Markov Decision Processes; Transition probabilities; Travel time data; Reinforcement learning",Conference Paper,"Final",,Scopus,2-s2.0-85046115944
"Wang X., Klabjan D.","57204816856;6603598029;","Competitive multi-agent inverse reinforcement learning with sub-optimal demonstrations",2018,"35th International Conference on Machine Learning, ICML 2018","11",,,"8148","8175",,1,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057312102&partnerID=40&md5=720543ce79910a2c818996d6e417fc2b","Department of Industrial Engineering and Management Sciences, Northwestern University, Evanston, IL, United States","Wang, X., Department of Industrial Engineering and Management Sciences, Northwestern University, Evanston, IL, United States; Klabjan, D., Department of Industrial Engineering and Management Sciences, Northwestern University, Evanston, IL, United States",,"Approximation algorithms; Computation theory; Deep neural networks; Demonstrations; Game theory; Inverse problems; Multi agent systems; Reinforcement learning; Stochastic systems; Inverse reinforcement learning; Model approximations; Nash equilibrium policy; Numerical experiments; Objective functions; Reward function; Training algorithms; Zero-sum stochastic games; Learning algorithms",Conference Paper,"Final",,Scopus,2-s2.0-85057312102
"Rhinehart N., Kitani K.","56743358400;15835267300;","First-Person Activity Forecasting from Video with Online Inverse Reinforcement Learning",2018,"IEEE Transactions on Pattern Analysis and Machine Intelligence",,,,"","",,1,"10.1109/TPAMI.2018.2873794","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054551080&doi=10.1109%2fTPAMI.2018.2873794&partnerID=40&md5=c449aee89097d1389c27cca215171710","Robotics Institute, Carnegie Mellon University, 6612 Pittsburgh, Pennsylvania United States 15213-3815 (e-mail: nrhineha@cs.cmu.edu); Robotics Institute, Carnegie Mellon University, Pittsburgh, Pennsylvania United States (e-mail: kkitani@cs.cmu.edu)","Rhinehart, N., Robotics Institute, Carnegie Mellon University, 6612 Pittsburgh, Pennsylvania United States 15213-3815 (e-mail: nrhineha@cs.cmu.edu); Kitani, K., Robotics Institute, Carnegie Mellon University, Pittsburgh, Pennsylvania United States (e-mail: kkitani@cs.cmu.edu)","Activity Forecasting; Cameras; First-Person Vision; Forecasting; Inverse Reinforcement Learning; Learning (artificial intelligence); Online Learning; Predictive models; Task analysis; Trajectory; Visualization","E-learning; Forecasting; Inverse problems; Semantics; Activity forecasting; Daily behaviors; First-person visions; Inverse reinforcement learning; Long-term goals; Modeling and forecasting; Online learning; Visual observations; Reinforcement learning; adult; article; forecasting; human; reinforcement; reward; videorecording",Article in Press,"Article in Press",,Scopus,2-s2.0-85054551080
"The Nguyen H., Bui L.T., Garratt M., Abbass H.","57204162280;8961832500;14015693000;6701824380;","Apprenticeship bootstrapping: Inverse Reinforcement learning in a multi-skill UAV-UGV coordination task",2018,"Proceedings of the International Joint Conference on Autonomous Agents and Multiagent Systems, AAMAS","3",,,"2204","2206",,1,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054712049&partnerID=40&md5=6e740b0ce76602b698947f6764f26649","School of Engineering and IT, UNSW-Canberra, Canberra, Australia; Department of Information Technology, Le Quy Don Technical University, Hanoi, Viet Nam","The Nguyen, H., School of Engineering and IT, UNSW-Canberra, Canberra, Australia; Bui, L.T., Department of Information Technology, Le Quy Don Technical University, Hanoi, Viet Nam; Garratt, M., School of Engineering and IT, UNSW-Canberra, Canberra, Australia; Abbass, H., School of Engineering and IT, UNSW-Canberra, Canberra, Australia","Apprenticeship learning; Deep Q-learning; Ground-air interaction; Inverse reinforcement Learning; Uavs; UGVs","Antennas; Apprentices; Autonomous agents; Deep learning; Demonstrations; Ground vehicles; Intelligent vehicle highway systems; Learning algorithms; Multi agent systems; Unmanned aerial vehicles (UAV); Apprenticeship learning; Ground-air interaction; Inverse reinforcement learning; Q-learning; UGVs; Reinforcement learning",Conference Paper,"Final",,Scopus,2-s2.0-85054712049
"Song J., Ren H., Sadigh D., Ermon S.","57202059115;57207374365;55052871500;35791579200;","Multi-agent generative adversarial imitation learning",2018,"6th International Conference on Learning Representations, ICLR 2018 - Workshop Track Proceedings",,,,"","",,3,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062993622&partnerID=40&md5=aeba4a4b8f53d03cfa71b79c9de4e5ec","Computer Science Department, Stanford University, United States","Song, J., Computer Science Department, Stanford University, United States; Ren, H., Computer Science Department, Stanford University, United States; Sadigh, D., Computer Science Department, Stanford University, United States; Ermon, S., Computer Science Department, Stanford University, United States",,"Inverse problems; Reinforcement learning; Actor-critic algorithm; Competitive agents; Empirical performance; High dimensional environment; Imitation learning; Inverse reinforcement learning; Markov games; Multi agent; Multi agent systems",Conference Paper,"Final",,Scopus,2-s2.0-85062993622
"Fu J., Luo K., Levine S.","57192429455;57210643709;35731728100;","Learning robust rewards with adversarial inverse reinforcement learning",2018,"6th International Conference on Learning Representations, ICLR 2018 - Conference Track Proceedings",,,,"","",,12,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056134319&partnerID=40&md5=48adeaf58164f352650de69ed07213a0","Department of Electrical Engineering and Computer Science, University of California, Berkeley, Berkeley, CA  94720, United States","Fu, J., Department of Electrical Engineering and Computer Science, University of California, Berkeley, Berkeley, CA  94720, United States; Luo, K., Department of Electrical Engineering and Computer Science, University of California, Berkeley, Berkeley, CA  94720, United States; Levine, S., Department of Electrical Engineering and Computer Science, University of California, Berkeley, Berkeley, CA  94720, United States",,"Decision making; Deep learning; Inverse problems; Learning algorithms; Machine learning; High-dimensional problems; Inverse reinforcement learning; ITS applications; Learning formulation; Reinforcement learning method; Reward function; Reinforcement learning",Conference Paper,"Final",,Scopus,2-s2.0-85056134319
"Song J., Ren H., Ermon S., Sadigh D.","57202059115;57207374365;35791579200;55052871500;","Multi-agent generative adversarial imitation learning",2018,"Advances in Neural Information Processing Systems","2018-December",,,"7461","7472",,5,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064825526&partnerID=40&md5=bab83320612366687c47fe8c01edbaaf","Stanford University, United States","Song, J., Stanford University, United States; Ren, H., Stanford University, United States; Ermon, S., Stanford University, United States; Sadigh, D., Stanford University, United States",,"Inverse problems; Learning algorithms; Reinforcement learning; Actor-critic algorithm; Empirical performance; High dimensional environment; Imitation learning; Inverse reinforcement learning; Markov games; Multi-agent setting; Non-stationary environment; Multi agent systems",Conference Paper,"Final",,Scopus,2-s2.0-85064825526
"Li S., Xiao S., Zhu S., Du N., Xie Y., Song L.","57189095555;57192388438;57213978075;55697607100;35729495800;55587150100;","Learning temporal point processes via reinforcement learning",2018,"Advances in Neural Information Processing Systems","2018-December",,,"10781","10791",,1,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064806693&partnerID=40&md5=deb62993a6674599215aafe528b1a833","Georgia Institute of Technology, United States; Ant Financial, China; Google Brain, United States","Li, S., Georgia Institute of Technology, United States; Xiao, S., Ant Financial, China; Zhu, S., Georgia Institute of Technology, United States; Du, N., Google Brain, United States; Xie, Y., Georgia Institute of Technology, United States; Song, L., Georgia Institute of Technology, United States, Ant Financial, China",,"Continuous time systems; Information services; Learning algorithms; Machine learning; Maximum likelihood estimation; Recurrent neural networks; Stochastic systems; Generative process; Information networks; Intensity functions; Inverse reinforcement learning; Learning paradigms; Model misspecification; Stochastic policy; Synthetic and real data; Reinforcement learning",Conference Paper,"Final",,Scopus,2-s2.0-85064806693
"Linares R., Furfaro R.","57202005934;12645288600;","Maneuvering detection and prediction using inverse reinforcement learning for space situational awareness",2018,"Advances in the Astronautical Sciences","162",,,"527","536",,,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049380771&partnerID=40&md5=bc05db72e7b6ece1533e15b199d75559","Department of Aerospace Engineering and Mechanics, University of Minnesota, Minneapolis, MN  55455, United States; Department of Systems and Industrial Engineering, University of Arizona, Tucson, AZ  85721, United States","Linares, R., Department of Aerospace Engineering and Mechanics, University of Minnesota, Minneapolis, MN  55455, United States; Furfaro, R., Department of Systems and Industrial Engineering, University of Arizona, Tucson, AZ  85721, United States",,"Astrophysics; Reinforcement learning; Systems engineering; Feature matching; Inverse reinforcement learning; Observational data; Orbital element; Proof of concept; Reward function; Simulation example; Space situational awareness; Inverse problems",Conference Paper,"Final",,Scopus,2-s2.0-85049380771
"Kitazato Y., Arai S.","57201988776;14057611500;","Estimation of reward function maximizing learning efficiency in inverse reinforcement learning",2018,"ICAART 2018 - Proceedings of the 10th International Conference on Agents and Artificial Intelligence","2",,,"276","283",,,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046640591&partnerID=40&md5=d2f2497071020caa214b67ce92ba3b06","Graduate School and Faculty of Engineering, Chiba University, 1-33 Yayoi-cho, Inage-ku, Chiba, 263-8522, Japan","Kitazato, Y., Graduate School and Faculty of Engineering, Chiba University, 1-33 Yayoi-cho, Inage-ku, Chiba, 263-8522, Japan; Arai, S., Graduate School and Faculty of Engineering, Chiba University, 1-33 Yayoi-cho, Inage-ku, Chiba, 263-8522, Japan","Genetic Algorithm; Inverse Reinforcement Learning","Artificial intelligence; Efficiency; Genetic algorithms; Inverse problems; Learning algorithms; Genetic algorithm approach; Ill posed; Inverse reinforcement learning; Learning efficiency; Reward function; Reinforcement learning",Conference Paper,"Final",,Scopus,2-s2.0-85046640591
"Xie N., Zhao T.-T., Yang Y., Wei Q., Heng T.S.","57203385327;53664758000;57206629968;57202837194;57202829500;","Creative Sequential Data Learning Method for Artistic Stylisation and Rendering System",2018,"Ruan Jian Xue Bao/Journal of Software","29","4",,"1071","1084",,,"10.13328/j.cnki.jos.005414","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049536351&doi=10.13328%2fj.cnki.jos.005414&partnerID=40&md5=3469f5065f4b59520a3ace753ae3b64c","Center for Future Media, University of Electronic Science and Technology of China, Chengdu, 611731, China; School of Computer Science and Engineering, University of Electronic Science and Technology of China, Chengdu, 611731, China; School of Computer Science and Information Engineering, Tianjin University of Science and Technology, Tianjin, 300457, China; Guizhou Provincial Key Laboratory of Public Big Data, Guizhou University, Guiyang, 550025, China","Xie, N., Center for Future Media, University of Electronic Science and Technology of China, Chengdu, 611731, China, School of Computer Science and Engineering, University of Electronic Science and Technology of China, Chengdu, 611731, China; Zhao, T.-T., School of Computer Science and Information Engineering, Tianjin University of Science and Technology, Tianjin, 300457, China; Yang, Y., Center for Future Media, University of Electronic Science and Technology of China, Chengdu, 611731, China, School of Computer Science and Engineering, University of Electronic Science and Technology of China, Chengdu, 611731, China; Wei, Q., Guizhou Provincial Key Laboratory of Public Big Data, Guizhou University, Guiyang, 550025, China; Heng, T.S., Center for Future Media, University of Electronic Science and Technology of China, Chengdu, 611731, China, School of Computer Science and Engineering, University of Electronic Science and Technology of China, Chengdu, 611731, China","Image artistic stylization; IRL (inverse reinforcement learning); Multimedia information processing; Policy search; Sequential data analysis; Stroke-based rendering","Gradient methods; Reinforcement learning; Image artistic stylization; Inverse reinforcement learning; Multimedia information processing; Policy search; Sequential data analysis; Stroke based rendering; Rendering (computer graphics)",Article,"Final",,Scopus,2-s2.0-85049536351
"Bazenkov N., Goubko M.","6506058697;55826165700;","Advanced planning of home appliances with consumer’s preference learning",2018,"Communications in Computer and Information Science","934",,,"249","259",,1,"10.1007/978-3-030-00617-4_23","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061644446&doi=10.1007%2f978-3-030-00617-4_23&partnerID=40&md5=cbbd3b8eea1da85e139de0d431cf54f9","Trapeznikov Institute of Control Sciences, 65 Profsoyuznaya st., Moscow, 117997, Russian Federation","Bazenkov, N., Trapeznikov Institute of Control Sciences, 65 Profsoyuznaya st., Moscow, 117997, Russian Federation; Goubko, M., Trapeznikov Institute of Control Sciences, 65 Profsoyuznaya st., Moscow, 117997, Russian Federation","Bayesian learning; Consumer simulation; Inverse reinforcement learning; Preference learning; Smart grid","Automation; Bayesian networks; Costs; Decision trees; Domestic appliances; Energy efficiency; Energy utilization; Inverse problems; Machine learning; Parameter estimation; Reinforcement learning; Sales; Bayesian learning; Consumer simulation; Inverse reinforcement learning; Preference learning; Smart grid; Learning algorithms",Conference Paper,"Final",,Scopus,2-s2.0-85061644446
"Kitazato Y., Arai S.","57201988776;14057611500;","Estimation of reward function maximizing learning efficiency in inverse reinforcement learning",2018,"IEEJ Transactions on Electronics, Information and Systems","138","6",,"720","727",,,"10.1541/ieejeiss.138.720","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048060379&doi=10.1541%2fieejeiss.138.720&partnerID=40&md5=0e15fb9824cb09a3e2ace50a4ac84561","Graduate School, Faculty of Engineering, Chiba University, 1-33, Yayoi-cho, Inage-ku, Chiba, 263-8522, Japan","Kitazato, Y., Graduate School, Faculty of Engineering, Chiba University, 1-33, Yayoi-cho, Inage-ku, Chiba, 263-8522, Japan; Arai, S., Graduate School, Faculty of Engineering, Chiba University, 1-33, Yayoi-cho, Inage-ku, Chiba, 263-8522, Japan","Inverse reinforcement learning; Learning efficiency","Efficiency; Genetic algorithms; Inverse problems; Evaluation measures; Faster convergence; Ill posed; Inverse reinforcement learning; Learning efficiency; Objective functions; Optimal policies; Reward function; Reinforcement learning",Article,"Final",,Scopus,2-s2.0-85048060379
"Henderson P., Chang W.-D., Bacon P.-L., Meger D., Pineau J., Precup D.","57202852272;57205539126;56742851900;23009425800;13404973100;6603288659;","Optiongan: Learning joint reward-policy options using generative adversarial inverse reinforcement learning",2018,"32nd AAAI Conference on Artificial Intelligence, AAAI 2018",,,,"3199","3206",,4,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060430951&partnerID=40&md5=543b0636d2b448bbde3bee62e5463107","School of Computer Science, McGill University, Montreal, Canada; Department of Electrical, Computer, and Software Engineering, McGill University, Montreal, Canada","Henderson, P., School of Computer Science, McGill University, Montreal, Canada; Chang, W.-D., Department of Electrical, Computer, and Software Engineering, McGill University, Montreal, Canada; Bacon, P.-L., School of Computer Science, McGill University, Montreal, Canada; Meger, D., School of Computer Science, McGill University, Montreal, Canada; Pineau, J., School of Computer Science, McGill University, Montreal, Canada; Precup, D., School of Computer Science, McGill University, Montreal, Canada",,"Artificial intelligence; Inverse problems; Complex problems; Continuous control; Inverse reinforcement learning; Joint rewards; Learning policy; Policy options; Reward function; Transfer learning; Reinforcement learning",Conference Paper,"Final",,Scopus,2-s2.0-85060430951
"Fu J., Singh A., Ghosh D., Yang L., Levine S.","57192429455;57212845940;57208438508;57200615246;35731728100;","Variational inverse control with events: A general framework for data-driven reward definition",2018,"Advances in Neural Information Processing Systems","2018-December",,,"8538","8547",,2,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064846573&partnerID=40&md5=355461d82ca6b877144a2c9ffde7f3a6","University of California, Berkeley, United States","Fu, J., University of California, Berkeley, United States; Singh, A., University of California, Berkeley, United States; Ghosh, D., University of California, Berkeley, United States; Yang, L., University of California, Berkeley, United States; Levine, S., University of California, Berkeley, United States",,"Inverse problems; Machine learning; Continuous control; Data driven; High-dimensional; Inverse control; Inverse reinforcement learning; Real-world; Reward function; Reinforcement learning",Conference Paper,"Final",,Scopus,2-s2.0-85064846573
"Pan X., Shen Y.","57196036026;35180368000;","Human-interactive subgoal supervision for efficient inverse reinforcement learning",2018,"Proceedings of the International Joint Conference on Autonomous Agents and Multiagent Systems, AAMAS","2",,,"1380","1387",,2,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054640918&partnerID=40&md5=fe0f214fe890bc1243ff94b17aa148cf","University of California, Berkeley, CA, United States; Samsung Research America, Mountain View, CA, United States","Pan, X., University of California, Berkeley, CA, United States; Shen, Y., Samsung Research America, Mountain View, CA, United States","Human-in-the-loop; Inverse reinforcement learning; Sub-goals","Autonomous agents; Demonstrations; Human robot interaction; Motion planning; Multi agent systems; Efficient learning; Human-in-the-loop; Inverse reinforcement learning; Learning agents; Learning process; Reward function; Sequential task; Subgoals; Reinforcement learning",Conference Paper,"Final",,Scopus,2-s2.0-85054640918
"Mindermann S., Armstrong S.","57208439611;18133396700;","Occam's razor is insufficient to infer the preferences of irrational agents",2018,"Advances in Neural Information Processing Systems","2018-December",,,"5598","5609",,,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064837879&partnerID=40&md5=2d1b5bf3eadea63ef527743599c3e024","Vector Institute, University of Toronto, Canada; Future of Humanity Institute, University of Oxford, United Kingdom; Machine Intelligence Research Institute, Berkeley, United States","Mindermann, S., Vector Institute, University of Toronto, Canada; Armstrong, S., Future of Humanity Institute, University of Oxford, United Kingdom, Machine Intelligence Research Institute, Berkeley, United States",,"Inverse problems; Reinforcement learning; Inverse reinforcement learning; No free lunch; Occam's razor; Planning algorithms; Reward function; Behavioral research",Conference Paper,"Final",,Scopus,2-s2.0-85064837879
"Jiang Y., Deng W., Wang J., Zhu B.","57190565558;35229007600;57202342421;51666195400;","Studies on Drivers' Driving Styles Based on Inverse Reinforcement Learning",2018,"SAE Technical Papers","2018-April",,,"","",,,"10.4271/2018-01-0612","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045526693&doi=10.4271%2f2018-01-0612&partnerID=40&md5=b739715ea81417e25a5fbccb4160734d","Jilin University, China; General Motors LLC, China","Jiang, Y., Jilin University, China; Deng, W., Jilin University, China; Wang, J., General Motors LLC, China; Zhu, B., Jilin University, China",,"Accident prevention; Automobile drivers; Automotive industry; Behavioral research; Boltzmann equation; Highway accidents; Human computer interaction; Inverse problems; Maximum likelihood; Reinforcement learning; Boltzmann distribution; Driving characteristics; Inverse reinforcement learning; Longitudinal acceleration; Longitudinal driving; Physical approaches; Reinforcement Learning theories; Surrounding environment; Advanced driver assistance systems",Conference Paper,"Final",,Scopus,2-s2.0-85045526693
"Yang J., Ye X., Trivedi R., Xu H., Zha H.","57202454087;57205479712;36976457200;55430229800;57201737688;","Learning deep mean field games for modeling large population behavior",2018,"6th International Conference on Learning Representations, ICLR 2018 - Conference Track Proceedings",,,,"","",,1,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071178232&partnerID=40&md5=2abd6f849f34b2037d638fe5714c4315","Georgia Institute of Technology, United States; Georgia State University, United States","Yang, J., Georgia Institute of Technology, United States; Ye, X., Georgia State University, United States; Trivedi, R., Georgia Institute of Technology, United States; Xu, H., Georgia Institute of Technology, United States; Zha, H., Georgia Institute of Technology, United States",,"Game theory; Inverse problems; Markov processes; Population distribution; Population statistics; Reinforcement learning; Collective behavior; Discrete state space; Inverse reinforcement learning; Large population; Markov Decision Processes; Mean field games; Real-world system; Temporal evolution; Deep learning",Conference Paper,"Final",,Scopus,2-s2.0-85071178232
"Malik D., Palaniappan M., Fisac J.F., Hadfield-Menell D., Russell S., Dragan A.D.","57204799631;57204809339;56692857500;55921863400;7401538237;55193779100;","An efficient, generalized bellman update for cooperative inverse reinforcement learning",2018,"35th International Conference on Machine Learning, ICML 2018","8",,,"5435","5443",,1,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057282681&partnerID=40&md5=81944ea55dc9e370b62bf02d9e041d66","Department of Electrical Engineering and Computer Sciences, University of California, Berkeley, United States","Malik, D., Department of Electrical Engineering and Computer Sciences, University of California, Berkeley, United States; Palaniappan, M., Department of Electrical Engineering and Computer Sciences, University of California, Berkeley, United States; Fisac, J.F., Department of Electrical Engineering and Computer Sciences, University of California, Berkeley, United States; Hadfield-Menell, D., Department of Electrical Engineering and Computer Sciences, University of California, Berkeley, United States; Russell, S., Department of Electrical Engineering and Computer Sciences, University of California, Berkeley, United States; Dragan, A.D., Department of Electrical Engineering and Computer Sciences, University of California, Berkeley, United States",,"Artificial intelligence; Game theory; Human robot interaction; Inverse problems; Reinforcement learning; Alignment Problems; Exponential factors; Full informations; Inverse reinforcement learning; Non trivial problems; Parameter spaces; Specific properties; Two-player games; Behavioral research",Conference Paper,"Final",,Scopus,2-s2.0-85057282681
"Haug L., Tschiatschek S., Singla A.","56462485700;54917447400;24766779100;","Teaching inverse reinforcement learners via features and demonstrations",2018,"Advances in Neural Information Processing Systems","2018-December",,,"8464","8473",,3,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064831957&partnerID=40&md5=90044c43b1aa2fbdd1dec2dc76405c5e","Department of Computer Science, ETH Zurich, Switzerland; Microsoft Research, Cambridge, United Kingdom; Max Planck Institute for Software Systems, Saarbrücken, Germany","Haug, L., Department of Computer Science, ETH Zurich, Switzerland; Tschiatschek, S., Microsoft Research, Cambridge, United Kingdom; Singla, A., Max Planck Institute for Software Systems, Saarbrücken, Germany",,"Inverse problems; Reinforcement learning; Risk assessment; Inverse reinforcement learning; Learning from demonstration; Near-optimal; Near-optimal policies; Reward function; Standard algorithms; Suboptimality; Demonstrations",Conference Paper,"Final",,Scopus,2-s2.0-85064831957
"Petousis P., Han S.X., Hsu W., Bui A.A.T.","57190854336;56612535000;55841504400;7005377593;","Generating Reward Functions using IRL Towards Individualized Cancer Screening",2018,"CEUR Workshop Proceedings","2142",,,"109","120",,,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050910657&partnerID=40&md5=d9de0fc1ce69803507ac1e1f9016ec18","UCLA Bioengineering Department, Los Angeles, CA  90095, United States; UCLA Department of Radiological Sciences, Los Angeles, CA  90095, United States","Petousis, P., UCLA Bioengineering Department, Los Angeles, CA  90095, United States; Han, S.X., UCLA Bioengineering Department, Los Angeles, CA  90095, United States; Hsu, W., UCLA Bioengineering Department, Los Angeles, CA  90095, United States, UCLA Department of Radiological Sciences, Los Angeles, CA  90095, United States; Bui, A.A.T., UCLA Bioengineering Department, Los Angeles, CA  90095, United States, UCLA Department of Radiological Sciences, Los Angeles, CA  90095, United States","Cancer screening; Maximum entropy inverse reinforcement learning; Partially-observable Markov decision processes","Behavioral research; Biological organs; Decision making; Intrusion detection; Markov processes; Reinforcement learning; Adaptive step size; Breast cancer screening; Cancer screening; False positive rates; Inverse reinforcement learning; Multiplicative model; Partially observable Markov decision process; Unintended consequences; Diseases",Conference Paper,"Final",,Scopus,2-s2.0-85050910657
"Lin H.-I., Nguyen X.-A., Chen W.-K.","24780828300;57192820181;56027664700;","Active intention inference for robot-human collaboration",2018,"International Journal of Computational Methods and Experimental Measurements","6","4",,"772","784",,,"10.2495/CMEM-V6-N4-772-784","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064214841&doi=10.2495%2fCMEM-V6-N4-772-784&partnerID=40&md5=b4a049894f1ec7805e4e4a5df4e4d8f8","Graduate Institute of Automation Technology, National Taipei University of Technology, Taipei, Taiwan","Lin, H.-I., Graduate Institute of Automation Technology, National Taipei University of Technology, Taipei, Taiwan; Nguyen, X.-A., Graduate Institute of Automation Technology, National Taipei University of Technology, Taipei, Taiwan; Chen, W.-K., Graduate Institute of Automation Technology, National Taipei University of Technology, Taipei, Taiwan","Human gesture recognition; Human-robot collaboration; Markov decision process",,Article,"Final",Open Access,Scopus,2-s2.0-85064214841
"Rhinehart N., Kitani K.M.","56743358400;15835267300;","First-Person Activity Forecasting with Online Inverse Reinforcement Learning",2017,"Proceedings of the IEEE International Conference on Computer Vision","2017-October",, 8237661,"3716","3725",,20,"10.1109/ICCV.2017.399","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041899383&doi=10.1109%2fICCV.2017.399&partnerID=40&md5=3bae9da81be00c91f5f10c896e53d69a","Robotics Institute, Carnegie Mellon University, Pittsburgh, PA  15213, United States","Rhinehart, N., Robotics Institute, Carnegie Mellon University, Pittsburgh, PA  15213, United States; Kitani, K.M., Robotics Institute, Carnegie Mellon University, Pittsburgh, PA  15213, United States",,"Behavioral research; Computer vision; Forecasting; Inverse problems; Reinforcement learning; Semantics; Activity forecasting; Daily behaviors; Inverse reinforcement learning; Long-term goals; Modeling and forecasting; Space and time; Streaming data; Visual observations; E-learning",Conference Paper,"Final",,Scopus,2-s2.0-85041899383
"Zeng K.-H., Shen W.B., Huang D.-A., Sun M., Niebles J.C.","57191475620;57208007751;57213498113;8321522800;22234659400;","Visual Forecasting by Imitating Dynamics in Natural Sequences",2017,"Proceedings of the IEEE International Conference on Computer Vision","2017-October",, 8237588,"3018","3027",,7,"10.1109/ICCV.2017.326","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041931020&doi=10.1109%2fICCV.2017.326&partnerID=40&md5=08135a8842b8991962da0998c726faab","Stanford University, United States; National Tsing Hua University, China","Zeng, K.-H., Stanford University, United States, National Tsing Hua University, China; Shen, W.B., Stanford University, United States; Huang, D.-A., Stanford University, United States; Sun, M., National Tsing Hua University, China; Niebles, J.C., Stanford University, United States",,"Computer vision; Dynamic programming; Dynamics; Forecasting; Inverse problems; Reinforcement learning; Semantics; Action anticipations; Computational bottlenecks; Continuous state-action spaces; Dual formulations; Feature representation; Gradient computation; Inverse reinforcement learning; Level of abstraction; Pixels",Conference Paper,"Final",,Scopus,2-s2.0-85041931020
"Nam C., Walker P., Lewis M., Sycara K.","35590319700;54785528900;57200329926;7006431929;","Predicting trust in human control of swarms via inverse reinforcement learning",2017,"RO-MAN 2017 - 26th IEEE International Symposium on Robot and Human Interactive Communication","2017-January",,,"528","533",,2,"10.1109/ROMAN.2017.8172353","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045762040&doi=10.1109%2fROMAN.2017.8172353&partnerID=40&md5=b14fde749d9be2526be024136c488c8d","Robotics Institute, Carnegie Mellon University, Pittsburgh, PA, United States; School of Information Science, University of PittsburghPA, United States","Nam, C., Robotics Institute, Carnegie Mellon University, Pittsburgh, PA, United States; Walker, P., School of Information Science, University of PittsburghPA, United States; Lewis, M., School of Information Science, University of PittsburghPA, United States; Sycara, K., Robotics Institute, Carnegie Mellon University, Pittsburgh, PA, United States",,"Learning algorithms; Markov processes; Robots; Human-in-the-loop; Inverse reinforcement learning; Markov Decision Processes; Operator control; Physical characteristics; Physical parameters; Search missions; Task performance; Reinforcement learning",Conference Paper,"Final",,Scopus,2-s2.0-85045762040
"Inga J., Köpf F., Flad M., Hohmann S.","57188985251;57201292669;6603246805;56027574000;","Individual human behavior identification using an inverse reinforcement learning method",2017,"2017 IEEE International Conference on Systems, Man, and Cybernetics, SMC 2017","2017-January",,,"99","104",,3,"10.1109/SMC.2017.8122585","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044202103&doi=10.1109%2fSMC.2017.8122585&partnerID=40&md5=a08f6e202a8945f74f285a104624c29a","Institute of Control Systems, Karlsruhe Institute of Technology, Karlsruhe, Germany","Inga, J., Institute of Control Systems, Karlsruhe Institute of Technology, Karlsruhe, Germany; Köpf, F., Institute of Control Systems, Karlsruhe Institute of Technology, Karlsruhe, Germany; Flad, M., Institute of Control Systems, Karlsruhe Institute of Technology, Karlsruhe, Germany; Hohmann, S., Institute of Control Systems, Karlsruhe Institute of Technology, Karlsruhe, Germany","Human behavior identification; Inverse optimal control; Inverse reinforcement learning; Shared control","Cost functions; Costs; Cybernetics; Inverse problems; Navigation; Reinforcement learning; Human behaviors; Human machine interaction; Inverse reinforcement learning; Inverse-optimal control; Movement trajectories; Optimal control theory; Reference trajectories; Shared control; Behavioral research",Conference Paper,"Final",,Scopus,2-s2.0-85044202103
"Pang Y., Yabe T., Tsubouchi K., Sekimoto Y.","57196486490;57189643794;24315190800;53364561900;","Modeling and reproducing human daily travel behavior from GPS data: A markov decision process approach",2017,"Proceedings of the 1st ACM SIGSPATIAL Workshop on Prediction of Human Mobility, PredictGIS 2017","2017-January",,,"","",,,"10.1145/3152341.3152347","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050971626&doi=10.1145%2f3152341.3152347&partnerID=40&md5=ed62d514ac52e551346fc444dbde1d21","Department of Civil Engineering, University of Tokyo, 4-6-1, Komaba, Meguro, Tokyo, 153-8505, Japan; Yahoo Japan Corporation, Chiyoda, Tokyo, Japan; Institute of Industrial Science, University of Tokyo, 4-6-1, Komaba, Meguro, Tokyo, 153-8505, Japan","Pang, Y., Department of Civil Engineering, University of Tokyo, 4-6-1, Komaba, Meguro, Tokyo, 153-8505, Japan; Yabe, T., Department of Civil Engineering, University of Tokyo, 4-6-1, Komaba, Meguro, Tokyo, 153-8505, Japan; Tsubouchi, K., Yahoo Japan Corporation, Chiyoda, Tokyo, Japan; Sekimoto, Y., Institute of Industrial Science, University of Tokyo, 4-6-1, Komaba, Meguro, Tokyo, 153-8505, Japan","Daily travel behavior; Human mobility; Urban dynamic","Decision making; Global positioning system; Inverse problems; Markov processes; mHealth; Population statistics; Reinforcement learning; Smartphones; Urban planning; Urban transportation; Different granularities; Human mobility; Inverse reinforcement learning; Markov Decision Processes; Smart-phone applications; Tokyo metropolitan areas; Travel behaviors; Urban dynamics; Behavioral research",Conference Paper,"Final",,Scopus,2-s2.0-85050971626
"Wu H., Sun W., Zheng B.","57118394600;55726565900;7201781454;","A fast trajectory outlier detection approach via driving behavior modeling",2017,"International Conference on Information and Knowledge Management, Proceedings","Part F131841",,,"837","846",,4,"10.1145/3132847.3132933","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85037359974&doi=10.1145%2f3132847.3132933&partnerID=40&md5=1cc3bcc4b12b91ae826204a1300f9f11","Fudan University, Shanghai Key Laboratory of Data Science, Shanghai, China; Singapore Management University, Singapore, Singapore","Wu, H., Fudan University, Shanghai Key Laboratory of Data Science, Shanghai, China; Sun, W., Fudan University, Shanghai Key Laboratory of Data Science, Shanghai, China; Zheng, B., Singapore Management University, Singapore, Singapore","Driving behavior; Inverse reinforcement learning; Outlier detection; Trajectory data processing","Data handling; Knowledge management; Location based services; Reinforcement learning; Statistics; Telecommunication services; Vehicles; Driving behavior; Effectiveness and efficiencies; Fundamental building blocks; Inverse reinforcement learning; Outlier Detection; Outlier detection algorithm; Probabilistic modeling; Trajectory data; Trajectories",Conference Paper,"Final",,Scopus,2-s2.0-85037359974
"Li C., Cao L., Zhang Y., Chen X., Zhou Y., Duan L.","56103739800;57198528477;56653349600;57192470874;35760002400;57200961456;","Knowledge-based deep reinforcement learning: a review",2017,"Xi Tong Gong Cheng Yu Dian Zi Ji Shu/Systems Engineering and Electronics","39","11",,"2603","2613",,4,"10.3969/j.issn.1001-506X.2017.11.30","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042690702&doi=10.3969%2fj.issn.1001-506X.2017.11.30&partnerID=40&md5=323703ebfb8ffdca926e6267bbe78122","Institute of Command Information System, PLA University of Science and Technology, Nanjing, 210007, China; College of Mechanical Engineering, Zhejiang University, Hangzhou, 310027, China","Li, C., Institute of Command Information System, PLA University of Science and Technology, Nanjing, 210007, China; Cao, L., Institute of Command Information System, PLA University of Science and Technology, Nanjing, 210007, China; Zhang, Y., Institute of Command Information System, PLA University of Science and Technology, Nanjing, 210007, China; Chen, X., Institute of Command Information System, PLA University of Science and Technology, Nanjing, 210007, China; Zhou, Y., Institute of Command Information System, PLA University of Science and Technology, Nanjing, 210007, China; Duan, L., College of Mechanical Engineering, Zhejiang University, Hangzhou, 310027, China","Deep reinforcement learning; Exploration strategy; Inverse reinforcement learning; Knowledge","Deep learning; Knowledge based systems; Exploration strategies; Inverse reinforcement learning; Knowledge; Knowledge based; Learning efficiency; Sequential decisions; Structured information; Trial and error; Reinforcement learning",Review,"Final",,Scopus,2-s2.0-85042690702
"Li Z., Kiseleva J., De Rijke M., Grotov A.","57195630442;36170720700;54790525600;56568207600;","Towards learning reward functions from user interactions",2017,"ICTIR 2017 - Proceedings of the 2017 ACM SIGIR International Conference on the Theory of Information Retrieval",,,,"289","292",,1,"10.1145/3121050.3121098","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85033215376&doi=10.1145%2f3121050.3121098&partnerID=40&md5=ddbe414389c116035f0fe3ceee854a01","University of Amsterdam, Amsterdam, Netherlands; UserSat.com, University of Amsterdam, Amsterdam, Netherlands","Li, Z., University of Amsterdam, Amsterdam, Netherlands; Kiseleva, J., UserSat.com, University of Amsterdam, Amsterdam, Netherlands; De Rijke, M., University of Amsterdam, Amsterdam, Netherlands; Grotov, A., University of Amsterdam, Amsterdam, Netherlands","Interactive systems; Inverse reinforcement learning; Online evaluation","Behavioral research; Function evaluation; Information retrieval; Inverse problems; Learning systems; Online systems; Reinforcement learning; Search engines; Analytic approach; Cultural heritages; Current situation; Dynamic approaches; Interactive system; Inverse reinforcement learning; On-line evaluation; Sequence of actions; Recommender systems",Conference Paper,"Final",,Scopus,2-s2.0-85033215376
"Truong X.-T., Ngo T.D.","56581881400;15755660100;","Toward Socially Aware Robot Navigation in Dynamic and Crowded Environments: A Proactive Social Motion Model",2017,"IEEE Transactions on Automation Science and Engineering","14","4", 8011466,"1743","1760",,15,"10.1109/TASE.2017.2731371","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028458074&doi=10.1109%2fTASE.2017.2731371&partnerID=40&md5=e86e54f4fefe9c9188bd30c966d79287","Faculty of Science, University of Brunei Darussalam, Bandar Seri Begawan, BE1410, Brunei Darussalam; Faculty of Control Engineering, Le Quy Don Technical University, Hanoi, 10000, Viet Nam; More-Than-One Robotics Laboratory, School of Sustainable Design Engineering, University of Prince Edward Island, Charlottetown, PE  C1A 4P3, Canada","Truong, X.-T., Faculty of Science, University of Brunei Darussalam, Bandar Seri Begawan, BE1410, Brunei Darussalam, Faculty of Control Engineering, Le Quy Don Technical University, Hanoi, 10000, Viet Nam; Ngo, T.D., Faculty of Science, University of Brunei Darussalam, Bandar Seri Begawan, BE1410, Brunei Darussalam, More-Than-One Robotics Laboratory, School of Sustainable Design Engineering, University of Prince Edward Island, Charlottetown, PE  C1A 4P3, Canada","Human comfortable safety; mobile service robots; proactive social motion model (PSMM); social robots; socially aware robot navigation","Behavioral research; Human robot interaction; Intelligent robots; Mobile robots; Mobile telecommunication systems; Motion planning; Navigation; Robot programming; Robots; Dynamic environments; Interactive informations; Mobile service robots; Motion modeling; Path planning techniques; Reciprocal velocity obstacles; Robot navigation; Social robots; Economic and social effects",Article,"Final",,Scopus,2-s2.0-85028458074
"Wang X., Zhang W., Chen J.","57197749223;35812744600;46661788200;","Comparison and implementation of high cited inverse reinforcement learning algorithms in object world",2017,"Proceedings - 9th International Conference on Intelligent Human-Machine Systems and Cybernetics, IHMSC 2017","2",, 8048179,"374","377",,,"10.1109/IHMSC.2017.195","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85034427860&doi=10.1109%2fIHMSC.2017.195&partnerID=40&md5=778cb9e35f26b700d29dc0db0e824af0","College of Mechatronic Engineering and Automation, National University of Defense Technology, Changsha, China","Wang, X., College of Mechatronic Engineering and Automation, National University of Defense Technology, Changsha, China; Zhang, W., College of Mechatronic Engineering and Automation, National University of Defense Technology, Changsha, China; Chen, J., College of Mechatronic Engineering and Automation, National University of Defense Technology, Changsha, China","Inverse reinforcement learning; MAXENT; MMP; MMPBOOST","Cybernetics; Intelligent agents; Inverse problems; Learning algorithms; Man machine systems; Energy-based methods; Environment change; Inverse reinforcement learning; MAXENT; Maximum margin planning; MMPBOOST; Reinforcement learning agent; Reward function; Reinforcement learning",Conference Paper,"Final",,Scopus,2-s2.0-85034427860
"Saha O., Dasgupta P.","57188823078;7201405936;","Improved reward estimation for efficient robot navigation using inverse reinforcement learning",2017,"2017 NASA/ESA Conference on Adaptive Hardware and Systems, AHS 2017",,, 8046385,"245","252",,2,"10.1109/AHS.2017.8046385","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032889169&doi=10.1109%2fAHS.2017.8046385&partnerID=40&md5=04f8bc8c076b6efc6467926d58f5769b","Computer Science Department, University of Nebraska, Omaha, United States","Saha, O., Computer Science Department, University of Nebraska, Omaha, United States; Dasgupta, P., Computer Science Department, University of Nebraska, Omaha, United States",,"Hardware; Inverse problems; Learning algorithms; Learning systems; Markov processes; NASA; Navigation; Random errors; Reinforcement learning; Central problems; Distance minimizations; Extraterrestrial environments; Inverse reinforcement learning; Navigation algorithms; Reinforcement learning techniques; Robot navigation; Semi-Markov decision process; Robots",Conference Paper,"Final",,Scopus,2-s2.0-85032889169
"Piot B., Geist M., Pietquin O.","55697434100;25929145100;16040586900;","Bridging the gap between imitation learning and inverse reinforcement learning",2017,"IEEE Transactions on Neural Networks and Learning Systems","28","8", 7464854,"1814","1826",,10,"10.1109/TNNLS.2016.2543000","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84966372828&doi=10.1109%2fTNNLS.2016.2543000&partnerID=40&md5=7f1b1a0062b3c2a8ae88733fc40554ac","Université Lille 1, Centrale Lille, INRIA, CNRS, Lille, 59000, France; UMI 2958, Georgia Tech-CNRS, CentraleSupélec, Université Paris-Saclay, Metz, 57070, France; IUF, Université Lille 1, CNRS, Lille, 59000, France","Piot, B., Université Lille 1, Centrale Lille, INRIA, CNRS, Lille, 59000, France; Geist, M., UMI 2958, Georgia Tech-CNRS, CentraleSupélec, Université Paris-Saclay, Metz, 57070, France; Pietquin, O., IUF, Université Lille 1, CNRS, Lille, 59000, France","Imitation learning (IL); inverse reinforcement learning (IRL); learning from demonstrations (LfD)","Algorithms; Apprentices; Demonstrations; Inverse problems; Markov processes; Dynamic environments; Imitation learning; Inverse reinforcement learning; Learning from demonstration; Markov Decision Processes; Policy framework; Robust solutions; Trajectory matching; Reinforcement learning",Article,"Final",,Scopus,2-s2.0-84966372828
"Choi S., Kim S., Jin Kim H.","56031605700;55268251400;6506659733;","Inverse reinforcement learning control for trajectory tracking of a multirotor UAV",2017,"International Journal of Control, Automation and Systems","15","4",,"1826","1834",,9,"10.1007/s12555-015-0483-3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85022187278&doi=10.1007%2fs12555-015-0483-3&partnerID=40&md5=fb628e8e312b5b7d40ae309e470410dd","Department of Mechanical and Aerospace engineering, Seoul National University, Seoul, South Korea","Choi, S., Department of Mechanical and Aerospace engineering, Seoul National University, Seoul, South Korea; Kim, S., Department of Mechanical and Aerospace engineering, Seoul National University, Seoul, South Korea; Jin Kim, H., Department of Mechanical and Aerospace engineering, Seoul National University, Seoul, South Korea","Inverse reinforcement learning; learning from demonstration; multirotor control; particle swarm optimization","Aircraft control; Controllers; Demonstrations; Education; Hidden Markov models; Markov processes; Number theory; Particle swarm optimization (PSO); Trajectories; Unmanned aerial vehicles (UAV); Control performance; Dynamic time warping; Flight maneuvers; Inverse reinforcement learning; Learning from demonstration; Trajectory tracking; Trajectory tracking errors; UAV (unmanned aerial vehicle); Reinforcement learning",Article,"Final",,Scopus,2-s2.0-85022187278
"Shiarlis K., Messias J., Whiteson S.","57193494529;55208968600;10240257400;","Rapidly exploring learning trees",2017,"Proceedings - IEEE International Conference on Robotics and Automation",,, 7989184,"1541","1548",,5,"10.1109/ICRA.2017.7989184","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028006248&doi=10.1109%2fICRA.2017.7989184&partnerID=40&md5=7ba03142234d21e98caffa7f2ed02da4","Informatics Institute, University of Amsterdam, Netherlands; Department of Computer Science, University of Oxford, United Kingdom","Shiarlis, K., Informatics Institute, University of Amsterdam, Netherlands; Messias, J., Informatics Institute, University of Amsterdam, Netherlands; Whiteson, S., Department of Computer Science, University of Oxford, United Kingdom",,"Cost functions; Costs; Inverse problems; Motion planning; Reinforcement learning; Robot programming; Robotics; Visual communication; Computational costs; Inverse learning; Inverse reinforcement learning; Maximum margin planning; Planning procedure; Rapidly-exploring random trees; Social navigation; Telepresence robots; Robots",Conference Paper,"Final",,Scopus,2-s2.0-85028006248
"Massimo D., Elahi M., Ricci F.","56433358100;34879649100;35270071500;","Learning user preferences by observing user-items interactions in an IoT augmented space",2017,"UMAP 2017 - Adjunct Publication of the 25th Conference on User Modeling, Adaptation and Personalization",,,,"35","40",,5,"10.1145/3099023.3099070","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026852471&doi=10.1145%2f3099023.3099070&partnerID=40&md5=472589b29ddc14b835e24dc467efea30","Free University of Bolzano-Bozen Piazza Domenicani, 3 Bolzano, Italy","Massimo, D., Free University of Bolzano-Bozen Piazza Domenicani, 3 Bolzano, Italy; Elahi, M., Free University of Bolzano-Bozen Piazza Domenicani, 3 Bolzano, Italy; Ricci, F., Free University of Bolzano-Bozen Piazza Domenicani, 3 Bolzano, Italy",,"Decision making; Internet of things; Inverse problems; Reinforcement learning; Decision making process; Inverse reinforcement learning; Making decision; Preference learning; Proof of concept; Sequential manners; User behaviour; Behavioral research",Conference Paper,"Final",,Scopus,2-s2.0-85026852471
"Budhraja K.K., Oates T.","55605655400;7005499321;","Neuroevolution-based Inverse Reinforcement Learning",2017,"2017 IEEE Congress on Evolutionary Computation, CEC 2017 - Proceedings",,, 7969297,"67","76",,,"10.1109/CEC.2017.7969297","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027867586&doi=10.1109%2fCEC.2017.7969297&partnerID=40&md5=1841976a29abf97075f0b5db4073fbd3","Computer Science and Electrical Engineering, University of Maryland, Baltimore County, MD  21250, United States","Budhraja, K.K., Computer Science and Electrical Engineering, University of Maryland, Baltimore County, MD  21250, United States; Oates, T., Computer Science and Electrical Engineering, University of Maryland, Baltimore County, MD  21250, United States",,,Conference Paper,"Final",,Scopus,2-s2.0-85027867586
"Köpf F., Inga J., Rothfuß S., Flad M., Hohmann S.","57201292669;57188985251;56236786400;6603246805;56027574000;","Inverse Reinforcement Learning for Identification in Linear-Quadratic Dynamic Games",2017,"IFAC-PapersOnLine","50","1",,"14902","14908",,4,"10.1016/j.ifacol.2017.08.2537","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044866145&doi=10.1016%2fj.ifacol.2017.08.2537&partnerID=40&md5=8f45822eae64fc413bee409ffabc4b7a","Institute of Control Systems, Karlsruhe Institute of Technology, Karlsruhe, 76131, Germany","Köpf, F., Institute of Control Systems, Karlsruhe Institute of Technology, Karlsruhe, 76131, Germany; Inga, J., Institute of Control Systems, Karlsruhe Institute of Technology, Karlsruhe, 76131, Germany; Rothfuß, S., Institute of Control Systems, Karlsruhe Institute of Technology, Karlsruhe, 76131, Germany; Flad, M., Institute of Control Systems, Karlsruhe Institute of Technology, Karlsruhe, 76131, Germany; Hohmann, S., Institute of Control Systems, Karlsruhe Institute of Technology, Karlsruhe, 76131, Germany","Game Theory; Identification; Inverse Optimal Control; Inverse Reinforcement Learning; Maximum Entropy; Shared Control","Behavioral research; Cost functions; Costs; Game theory; Identification (control systems); Inverse problems; Maximum entropy methods; Automatic controllers; Identification algorithms; Inverse reinforcement learning; Inverse-optimal control; Real-time application; Shared control; Simulation example; Theory of dynamics; Reinforcement learning",Article,"Final",Open Access,Scopus,2-s2.0-85044866145
"Banovic N., Wang A., Jin Y., Chang C., Ramos J., Dey A.K., Mankoff J.","54794750000;57201449827;57194274303;57201451844;24587910500;7101701731;57203175552;","Leveraging human routine models to detect and generate human behaviors",2017,"Conference on Human Factors in Computing Systems - Proceedings","2017-May",,,"6683","6694",,5,"10.1145/3025453.3025571","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019650891&doi=10.1145%2f3025453.3025571&partnerID=40&md5=3f0a221d2e13a150ca49cd103e362bac","Human-Computer Interaction Institute, CMU, Pittsburgh, PA  15213, United States; Computer Science, Middlebury College, Middlebury, VA  05753, United States","Banovic, N., Human-Computer Interaction Institute, CMU, Pittsburgh, PA  15213, United States; Wang, A., Human-Computer Interaction Institute, CMU, Pittsburgh, PA  15213, United States; Jin, Y., Computer Science, Middlebury College, Middlebury, VA  05753, United States; Chang, C., Human-Computer Interaction Institute, CMU, Pittsburgh, PA  15213, United States; Ramos, J., Human-Computer Interaction Institute, CMU, Pittsburgh, PA  15213, United States; Dey, A.K., Human-Computer Interaction Institute, CMU, Pittsburgh, PA  15213, United States; Mankoff, J., Human-Computer Interaction Institute, CMU, Pittsburgh, PA  15213, United States","Inverse reinforcement learning; Maximum entropy","Human engineering; Maximum entropy methods; Reinforcement learning; Supervised learning; Aggressive driving behaviors; Data labeling; Domain experts; Human behaviors; Inverse reinforcement learning; Supervised machine learning; Wellbeing; Behavioral research",Conference Paper,"Final",,Scopus,2-s2.0-85019650891
"Daniele A.F., Bansal M., Walter M.R.","57194715661;16466939600;15063686700;","Navigational Instruction Generation as Inverse Reinforcement Learning with Neural Machine Translation",2017,"ACM/IEEE International Conference on Human-Robot Interaction","Part F127194",,,"109","118",,6,"10.1145/2909824.3020241","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021795561&doi=10.1145%2f2909824.3020241&partnerID=40&md5=3e5ff2ad7553420b7728ab511e9e4f2f","TTI-Chicago, United States; UNC Chapel Hill, United States","Daniele, A.F., TTI-Chicago, United States; Bansal, M., UNC Chapel Hill, United States; Walter, M.R., TTI-Chicago, United States","human-robot interaction; natural language generation; selective generation","Education; Information dissemination; Inverse problems; Man machine systems; Natural language processing systems; Reinforcement learning; Robots; Translation (languages); Human demonstrations; Instruction generations; Inverse reinforcement learning; Machine translations; Natural language generation; Natural languages; Robotics applications; Selective generation; Human robot interaction",Conference Paper,"Final",,Scopus,2-s2.0-85021795561
"Nouri E., Georgila K., Traum D.","54938669800;14017952500;6603622906;","Culture-specific models of negotiation for virtual characters: multi-attribute decision-making based on culture-specific values",2017,"AI and Society","32","1",,"51","63",,3,"10.1007/s00146-014-0570-7","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85011661373&doi=10.1007%2fs00146-014-0570-7&partnerID=40&md5=f509e501fa5bb2d27e24cd20a62ee4a3","Institute for Creative Technologies, University of Southern California, 12015 Waterfront Drive, Playa Vista, CA  90094, United States","Nouri, E., Institute for Creative Technologies, University of Southern California, 12015 Waterfront Drive, Playa Vista, CA  90094, United States; Georgila, K., Institute for Creative Technologies, University of Southern California, 12015 Waterfront Drive, Playa Vista, CA  90094, United States; Traum, D., Institute for Creative Technologies, University of Southern California, 12015 Waterfront Drive, Playa Vista, CA  90094, United States","Cultural decision-making; Inverse reinforcement learning; Negotiation; Ultimatum Game; Virtual agents","Behavioral research; Reinforcement learning; Virtual reality; Inverse reinforcement learning; Multi attribute decision making; Negotiation; Specific values; Ultimatum game; Virtual agent; Virtual character; Virtual humans; Decision making",Article,"Final",,Scopus,2-s2.0-85011661373
"Previtali F., Bordallo A., Iocchi L., Ramamoorthy S.","56210921700;57118029200;6602922345;15042865300;","Predicting future agent motions for dynamic environments",2017,"Proceedings - 2016 15th IEEE International Conference on Machine Learning and Applications, ICMLA 2016",,, 7838128,"94","99",,,"10.1109/ICMLA.2016.137","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85015386109&doi=10.1109%2fICMLA.2016.137&partnerID=40&md5=f326d38c25b2ecfc55e5653a135b63da","Sapienza University of Rome, Italy; University of Edinburgh, United Kingdom","Previtali, F., Sapienza University of Rome, Italy; Bordallo, A., University of Edinburgh, United Kingdom; Iocchi, L., Sapienza University of Rome, Italy; Ramamoorthy, S., University of Edinburgh, United Kingdom",,"Artificial intelligence; Behavioral research; Forecasting; Learning algorithms; Learning systems; Markov processes; Reinforcement learning; Semantics; Computational ability; Emerging applications; Empirical experiments; Inverse reinforcement learning; Markov Decision Processes; Multi resolution representation; Multi-camera tracking; State-of-the-art methods; Inverse problems",Conference Paper,"Final",,Scopus,2-s2.0-85015386109
"Saitake R., Arai S.","57193403941;14057611500;","Parameter estimation of multi-objeetive reinforeement learning to reaeh arbitrary pareto solution",2017,"Proceedings - 2016 International Conference on Agents, ICA 2016",,, 7812982,"110","111",,1,"10.1109/ICA.2016.17","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85013679345&doi=10.1109%2fICA.2016.17&partnerID=40&md5=ea654622395bc27b538594f14000dd3e","Graduate School of Engineering, Chiba University, Chiba, Japan","Saitake, R., Graduate School of Engineering, Chiba University, Chiba, Japan; Arai, S., Graduate School of Engineering, Chiba University, Chiba, Japan","Inverse problem; Multi-Objective Reinforcement Learning; Parameter estimation","Inverse problems; Optimal systems; Pareto principle; Reinforcement learning; Apprenticeship learning; Bench-mark problems; Computational costs; Inverse reinforcement learning; Multi objective; Optimal sequence; Pareto optimal solutions; Pareto solution; Parameter estimation",Conference Paper,"Final",,Scopus,2-s2.0-85013679345
"Zouzou A., Bouhoute A., Boubouh K., El Kamili M., Berrada I.","57200511399;56321351000;57200511274;14041412100;8963275500;","Predicting lane change maneuvers using Inverse Reinforcement Learning",2017,"Proceedings - 2017 International Conference on Wireless Networks and Mobile Communications, WINCOM 2017",,, 8238204,"","",,,"10.1109/WINCOM.2017.8238204","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041448992&doi=10.1109%2fWINCOM.2017.8238204&partnerID=40&md5=61f20a10697b55a2872eb32b214bddd7","LIMS Laboratory, Faculty of Sciences Dhar Mahraz, Université Sidi Mohammed Ben Abdellah - Fez, Morocco","Zouzou, A., LIMS Laboratory, Faculty of Sciences Dhar Mahraz, Université Sidi Mohammed Ben Abdellah - Fez, Morocco; Bouhoute, A., LIMS Laboratory, Faculty of Sciences Dhar Mahraz, Université Sidi Mohammed Ben Abdellah - Fez, Morocco; Boubouh, K., LIMS Laboratory, Faculty of Sciences Dhar Mahraz, Université Sidi Mohammed Ben Abdellah - Fez, Morocco; El Kamili, M., LIMS Laboratory, Faculty of Sciences Dhar Mahraz, Université Sidi Mohammed Ben Abdellah - Fez, Morocco; Berrada, I., LIMS Laboratory, Faculty of Sciences Dhar Mahraz, Université Sidi Mohammed Ben Abdellah - Fez, Morocco","Inverse reinforcement learning; Machine learning; Markov decision process; Maximum causal entropy","Behavioral research; Data visualization; Entropy; Inverse problems; Learning algorithms; Learning systems; Markov processes; Mobile telecommunication systems; Wireless networks; Human behaviors; Inverse reinforcement learning; Lane change; Lane change maneuvers; Markov Decision Processes; Open-source solutions; Plug-ins; Reinforcement learning",Conference Paper,"Final",,Scopus,2-s2.0-85041448992
"Shahryari S., Doshi P.","57189354278;23008336000;","Inverse reinforcement learning under noisy observations",2017,"Proceedings of the International Joint Conference on Autonomous Agents and Multiagent Systems, AAMAS","3",,,"1733","1735",,2,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046423089&partnerID=40&md5=ffa79201abab0b43941dd7e29469ccc8","Institute for Artificial Intelligence, University of Georgia, Athens, GA  30602, United States; THINC Lab, Dept. Of Computer Science, University of Georgia, Athens, GA  30602, United States","Shahryari, S., Institute for Artificial Intelligence, University of Georgia, Athens, GA  30602, United States; Doshi, P., THINC Lab, Dept. Of Computer Science, University of Georgia, Athens, GA  30602, United States",,"Autonomous agents; Inverse problems; Maximum principle; Multi agent systems; Trajectories; Expectation - maximizations; Inverse reinforcement learning; Noisy observations; Observation model; Reinforcement learning",Conference Paper,"Final",,Scopus,2-s2.0-85046423089
"Šošic A., KhudaBukhsh W.R., Zoubir A.M., Koeppl H.","55416190200;57190680542;35584414100;6603491586;","Inverse reinforcement learning in swarm systems",2017,"Proceedings of the International Joint Conference on Autonomous Agents and Multiagent Systems, AAMAS","3",,,"1413","1420",,7,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040758666&partnerID=40&md5=5c6d8eb1951628959f8c41370a525801","Department of Electrical Engineering and Information Technology, Technische Universität, Darmstadt, Germany","Šošic, A., Department of Electrical Engineering and Information Technology, Technische Universität, Darmstadt, Germany; KhudaBukhsh, W.R., Department of Electrical Engineering and Information Technology, Technische Universität, Darmstadt, Germany; Zoubir, A.M., Department of Electrical Engineering and Information Technology, Technische Universität, Darmstadt, Germany; Koeppl, H., Department of Electrical Engineering and Information Technology, Technische Universität, Darmstadt, Germany","Inverse reinforcement learning; Multi-agent systems; Swarms","Autonomous agents; Inverse problems; Markov processes; Reinforcement learning; Swarm intelligence; Behavioral model; Control problems; Inverse reinforcement learning; Large-scale problem; Learning schemes; Partially observable Markov decision process; Swarming behavior; Swarms; Multi agent systems",Conference Paper,"Final",,Scopus,2-s2.0-85040758666
"Amin K., Jiang N., Singh S.","51461010600;56421285700;55548164600;","Repeated inverse reinforcement learning",2017,"Advances in Neural Information Processing Systems","2017-December",,,"1816","1825",,10,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046996869&partnerID=40&md5=7b1b60c8ae6a41e7d0d84be20f126e6f","Google Research, New York, NY  10011, United States; Computer Science and Engineering, University of Michigan, Ann Arbor, MI  48104, United States","Amin, K., Google Research, New York, NY  10011, United States; Jiang, N., Computer Science and Engineering, University of Michigan, Ann Arbor, MI  48104, United States; Singh, S., Computer Science and Engineering, University of Michigan, Ann Arbor, MI  48104, United States",,"Behavioral research; Reinforcement learning; Inverse reinforcement learning; Inverse problems",Conference Paper,"Final",,Scopus,2-s2.0-85046996869
"Prasad V., Jangir R., Balaraman R., Krishna K.M.","57201912364;57201903033;57201895562;7102445320;","Data driven strategies for active monocular SLAM using inverse reinforcement learning",2017,"Proceedings of the International Joint Conference on Autonomous Agents and Multiagent Systems, AAMAS","3",,,"1697","1699",,,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046472099&partnerID=40&md5=f5a535ebd0a8f43ffb1c0b7f05fd4a44","Robotics Research Centre, International Institute of Information Technology, Hyderabad, India; Department of Physics, Indian Institute of Technology Guwahati, Guwahati, India; Department of Computer Science, Indian Institute of Technology Madras, Madras, India","Prasad, V., Robotics Research Centre, International Institute of Information Technology, Hyderabad, India; Jangir, R., Department of Physics, Indian Institute of Technology Guwahati, Guwahati, India; Balaraman, R., Department of Computer Science, Indian Institute of Technology Madras, Madras, India; Krishna, K.M., Robotics Research Centre, International Institute of Information Technology, Hyderabad, India","Active monocular SLAM; Inverse reinforcement learning","Autonomous agents; Inverse problems; Markov processes; Multi agent systems; Vision; Complex task; Computational model; Data driven; Inverse reinforcement learning; Markov Decision Processes; Monocular SLAM; Reward function; State-of-the-art methods; Reinforcement learning",Conference Paper,"Final",,Scopus,2-s2.0-85046472099
"Metelli A.M., Pirotta M., Restelli M.","57195947711;49362257000;6603404086;","Compatible reward inverse reinforcement learning",2017,"Advances in Neural Information Processing Systems","2017-December",,,"2051","2060",,1,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047016144&partnerID=40&md5=8ab01ef18feb66d08f8c8a4f1491129a","DEIB, Politecnico di Milano, Italy; SequeL Team, Inria, Lille, France","Metelli, A.M., DEIB, Politecnico di Milano, Italy; Pirotta, M., SequeL Team, Inria, Lille, France; Restelli, M., DEIB, Politecnico di Milano, Italy",,"Inverse problems; Taxicabs; Basis functions; Effective approaches; Function spaces; Inverse reinforcement learning; Linear quadratic Gaussian; Optimal policies; Policy gradient; Reward function; Reinforcement learning",Conference Paper,"Final",,Scopus,2-s2.0-85047016144
"Mobley D.","57196122373;","Multi-agent systems of inverse reinforcement learners in complex games",2017,"IJCAI International Joint Conference on Artificial Intelligence",,,,"5191","5192",,,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031923562&partnerID=40&md5=e5a55f7f4d68ef69e6aa9c66310ba5f4","University of Kentucky, United States","Mobley, D., University of Kentucky, United States",,"Artificial intelligence; Intelligent agents; Inverse problems; Reinforcement learning; Core problems; Inverse reinforcement learning; Task modeling; Task-based; Multi agent systems",Conference Paper,"Final",,Scopus,2-s2.0-85031923562
"Pulido J.V., Fields M., Barnes L.","57202675123;22957625600;7103077338;","On-the-fly learning and monitoring of partially observed navigation plan",2017,"Proceedings of the International Joint Conference on Autonomous Agents and Multiagent Systems, AAMAS","3",,,"1700","1702",,1,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046480828&partnerID=40&md5=d4ac25ddd1220265f0f10d02757c5d38","University of Virginia, 151 Engineer's Way, Charlottesville, VA, United States; U.S. Army Research Laboratory, 4727 Deer Creek Loop, Aberdeen Proving Grounds, MD, United States","Pulido, J.V., University of Virginia, 151 Engineer's Way, Charlottesville, VA, United States; Fields, M., U.S. Army Research Laboratory, 4727 Deer Creek Loop, Aberdeen Proving Grounds, MD, United States; Barnes, L., University of Virginia, 151 Engineer's Way, Charlottesville, VA, United States","Intent recognition; Inverse reinforcement learning","Autonomous agents; Inverse problems; Multi agent systems; Navigation; Robots; Intent recognition; Inverse reinforcement learning; Maximum margin; Navigation model; On the flies; Predictive abilities; Real time; Reinforcement learning",Conference Paper,"Final",,Scopus,2-s2.0-85046480828
"Santos E., Jr., Nguyen H., Russell J., Kim K., Veenhuis L., Boparai R., Stautland T.K.","23980706000;55667736200;7404208804;14619319600;57195074097;57195071094;57195071293;","Capturing a Commander's decision making style",2017,"Proceedings of SPIE - The International Society for Optical Engineering","10184",, 1018412,"","",,1,"10.1117/12.2268452","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85025141788&doi=10.1117%2f12.2268452&partnerID=40&md5=9dfa7b28398ca54dc72712b49e15f7ba","Thayer Engineering School, Dartmouth College, 8000 Cummings Hall, Hanover, NH  03755, United States; Dept. of Computer Science, UW-Whitewater, 800 W. Main Street, Whitewater, WI  53190, United States","Santos, E., Jr., Thayer Engineering School, Dartmouth College, 8000 Cummings Hall, Hanover, NH  03755, United States; Nguyen, H., Dept. of Computer Science, UW-Whitewater, 800 W. Main Street, Whitewater, WI  53190, United States; Russell, J., Thayer Engineering School, Dartmouth College, 8000 Cummings Hall, Hanover, NH  03755, United States; Kim, K., Thayer Engineering School, Dartmouth College, 8000 Cummings Hall, Hanover, NH  03755, United States; Veenhuis, L., Dept. of Computer Science, UW-Whitewater, 800 W. Main Street, Whitewater, WI  53190, United States; Boparai, R., Dept. of Computer Science, UW-Whitewater, 800 W. Main Street, Whitewater, WI  53190, United States; Stautland, T.K., Dept. of Computer Science, UW-Whitewater, 800 W. Main Street, Whitewater, WI  53190, United States","cognitive states; Decision making process; Double Transition Model; inverse optimal control; inverse reinforcement learning","Education; Law enforcement; National security; Naval warfare; Network security; Reinforcement learning; Security systems; Cognitive state; Decision making process; Inverse reinforcement learning; Inverse-optimal control; Transition model; Decision making",Conference Paper,"Final",,Scopus,2-s2.0-85025141788
"Kohjima M., Matsubayashi T., Sawada H.","56925050300;55111598300;7401969520;","Generalized Inverse Reinforcement Learning with Linearly Solvable MDP",2017,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","10535 LNAI",,,"373","388",,,"10.1007/978-3-319-71246-8_23","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040255772&doi=10.1007%2f978-3-319-71246-8_23&partnerID=40&md5=7dfcbb708253715da7764a6deb6081ff","NTT Service Evolution Laboratories, NTT Corporation, Kanagawa, Japan","Kohjima, M., NTT Service Evolution Laboratories, NTT Corporation, Kanagawa, Japan; Matsubayashi, T., NTT Service Evolution Laboratories, NTT Corporation, Kanagawa, Japan; Sawada, H., NTT Service Evolution Laboratories, NTT Corporation, Kanagawa, Japan","Bayesian method; Inverse reinforcement learning; Linearly solvable MDP","Artificial intelligence; Bayesian networks; Cost benefit analysis; Cost estimating; Cost functions; Costs; Inverse problems; Learning algorithms; Learning systems; Markov processes; Bayesian methods; Generalized inverse; Inverse reinforcement learning; Linearly solvable MDP; Markov Decision Processes; Synthetic data; Theoretical study; Transition probabilities; Reinforcement learning",Conference Paper,"Final",,Scopus,2-s2.0-85040255772
"Bogert K., Doshi P.","36095803300;23008336000;","Scaling expectation-maximization for inverse reinforcement learning to multiple robots under occlusion",2017,"Proceedings of the International Joint Conference on Autonomous Agents and Multiagent Systems, AAMAS","1",,,"522","529",,1,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046430383&partnerID=40&md5=36cf821fe8e0fa8c124e848462cf729e","Department of Computer Science, University of North Carolina, Asheville, NC  28804, United States; Dept. of Computer Science, THINC Lab, University of Georgia, Athens, GA  30602, United States","Bogert, K., Department of Computer Science, University of North Carolina, Asheville, NC  28804, United States; Doshi, P., Dept. of Computer Science, THINC Lab, University of Georgia, Athens, GA  30602, United States",,"Industrial robots; Inverse problems; Maximum principle; Multi agent systems; Reinforcement learning; Trajectories; Close proximity; Conditional expectation; Expectation - maximizations; Gibbs sampling; Inverse reinforcement learning; Multi-robot domains; Multiple agents; Multiple robot; Autonomous agents",Conference Paper,"Final",,Scopus,2-s2.0-85046430383
"Arnold T., Kasenberg D., Scheutz M.","56895720600;57201556232;6603548841;","Value alignment or misalignment - What will keep systems accountable?",2017,"AAAI Workshop - Technical Report","WS-17-01 - WS-17-15",,,"81","88",,17,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045276127&partnerID=40&md5=fa6c749c157a02113fb2891967a225da","Department of Computer Science, Tufts University, Medford, MA  02155, United States","Arnold, T., Department of Computer Science, Tufts University, Medford, MA  02155, United States; Kasenberg, D., Department of Computer Science, Tufts University, Medford, MA  02155, United States; Scheutz, M., Department of Computer Science, Tufts University, Medford, MA  02155, United States",,"Alignment; Computer games; Deep learning; Inverse problems; Knowledge based systems; Operations research; Philosophical aspects; Reinforcement learning; Computational architecture; Counterfactuals; Hybrid approach; Inverse reinforcement learning; Temporal dimensions; Problem solving",Conference Paper,"Final",,Scopus,2-s2.0-85045276127
"Shimosaka M., Sato J., Takenaka K., Hitomi K.","7003853628;57212913027;55533275600;36787198700;","Fast inverse reinforcement learning with interval consistent graph for driving behavior prediction",2017,"31st AAAI Conference on Artificial Intelligence, AAAI 2017",,,,"1532","1538",,3,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030462694&partnerID=40&md5=7bf9704f3e3183ddf7a9eb70f7b216f1","Tokyo Institute of Technology, Japan; University of Tokyo, Japan; DENSO CORPORATION, Japan","Shimosaka, M., Tokyo Institute of Technology, Japan; Sato, J., University of Tokyo, Japan; Takenaka, K., DENSO CORPORATION, Japan; Hitomi, K., DENSO CORPORATION, Japan",,"Artificial intelligence; Behavioral research; Forecasting; Inverse problems; Markov processes; Maximum entropy methods; State space methods; Computational costs; Effective approaches; Exponential growth; High-dimensional; Inverse reinforcement learning; Markov Decision Processes; Prediction performance; State - space models; Reinforcement learning",Conference Paper,"Final",,Scopus,2-s2.0-85030462694
"Majumdar A., Singh S., Mandlekar A., Pavone M.","55325126200;56042918600;55884466700;12240587200;","Risk-sensitive inverse reinforcement learning via coherent risk models",2017,"Robotics: Science and Systems","13",,,"","",,4,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048804859&partnerID=40&md5=98a8c4db495ffc059899fb830739f0b8","Department of Aeronautics and Astronautics, Stanford University, Stanford, CA  94305, United States; Electrical Engineering, Stanford University, Stanford, CA  94305, United States","Majumdar, A., Department of Aeronautics and Astronautics, Stanford University, Stanford, CA  94305, United States; Singh, S., Department of Aeronautics and Astronautics, Stanford University, Stanford, CA  94305, United States; Mandlekar, A., Electrical Engineering, Stanford University, Stanford, CA  94305, United States; Pavone, M., Department of Aeronautics and Astronautics, Stanford University, Stanford, CA  94305, United States",,"Behavioral research; Cost functions; Decision making; Inverse problems; Linear programming; Risk assessment; Robotics; Driving styles; Dynamic decision making; Expected values; Inverse reinforcement learning; Risk neutrals; Risk preference; Risk sensitivity; Simulated driving; Reinforcement learning",Conference Paper,"Final",,Scopus,2-s2.0-85048804859
"Lin Z., Gehring J., Khalidov V., Synnaeve G.","57208446046;57210079450;25927242400;36987111200;","STARDATA: A starcraft AI research dataset",2017,"Proceedings of the 13th AAAI Conference on Artificial Intelligence and Interactive Digital Entertainment, AIIDE 2017",,,,"50","56",,6,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056824016&partnerID=40&md5=45f6eb4d7b474016a53c3a5579d93e18","Facebook, 770 Broadway, New York, NY  10003, United States; Facebook, 6, rue Ménars, Paris, 75002, France","Lin, Z., Facebook, 770 Broadway, New York, NY  10003, United States; Gehring, J., Facebook, 6, rue Ménars, Paris, 75002, France; Khalidov, V., Facebook, 6, rue Ménars, Paris, 75002, France; Synnaeve, G., Facebook, 770 Broadway, New York, NY  10003, United States",,"Data mining; Human computer interaction; Inverse problems; Machine learning; Reinforcement learning; Forward modeling; Imitation learning; Inverse reinforcement learning; Player action; Classification (of information)",Conference Paper,"Final",,Scopus,2-s2.0-85056824016
"Kohjima M., Matsubayashi T., Sawada H.","56925050300;55111598300;7401969520;","What-if prediction via inverse reinforcement learning",2017,"FLAIRS 2017 - Proceedings of the 30th International Florida Artificial Intelligence Research Society Conference",,,,"74","79",,2,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029514976&partnerID=40&md5=c4aefb22c05d4f90833a2f27dbedd0ae","NTT Service Evolution Laboratories, NTT Corporation 1-1 Hikari-no-oka, Yokosuka-shi, Kanagawa, 239-0847, Japan","Kohjima, M., NTT Service Evolution Laboratories, NTT Corporation 1-1 Hikari-no-oka, Yokosuka-shi, Kanagawa, 239-0847, Japan; Matsubayashi, T., NTT Service Evolution Laboratories, NTT Corporation 1-1 Hikari-no-oka, Yokosuka-shi, Kanagawa, 239-0847, Japan; Sawada, H., NTT Service Evolution Laboratories, NTT Corporation 1-1 Hikari-no-oka, Yokosuka-shi, Kanagawa, 239-0847, Japan",,,Conference Paper,"Final",,Scopus,2-s2.0-85029514976
"Uchida S., Oba S., Ishii S.","57197729458;7007003360;7403110142;","Estimation of the change of agents behavior strategy using state-action history",2017,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","10614 LNCS",,,"100","107",,,"10.1007/978-3-319-68612-7_12","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85034220134&doi=10.1007%2f978-3-319-68612-7_12&partnerID=40&md5=ae13d12fb643e3b22489e93958a5b34b","Kyoto University, Yoshidahonmachi 36-1, Sakyo-ku, Kyoto-city, Japan; ATR Cognitive Mechanism Laboratories, Kyoto, Japan","Uchida, S., Kyoto University, Yoshidahonmachi 36-1, Sakyo-ku, Kyoto-city, Japan; Oba, S., Kyoto University, Yoshidahonmachi 36-1, Sakyo-ku, Kyoto-city, Japan; Ishii, S., Kyoto University, Yoshidahonmachi 36-1, Sakyo-ku, Kyoto-city, Japan, ATR Cognitive Mechanism Laboratories, Kyoto, Japan","Behavior strategy; Change point; Inverse reinforcement learning; Maximum likelihood estimation; Reinforcement learning","Intelligent agents; Inverse problems; Learning algorithms; Learning systems; Maximum likelihood; Maximum likelihood estimation; Neural networks; Behavior strategy; Change-points; Computational model; Current situation; Environmental uncertainty; Inverse reinforcement learning; Stationary policy; Uncertain environments; Reinforcement learning",Conference Paper,"Final",,Scopus,2-s2.0-85034220134
"Rzepka R., Araki K.","6603550196;7402551308;","What people say? Web-based casuistry for artificial morality experiments",2017,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","10414 LNAI",,,"178","187",,,"10.1007/978-3-319-63703-7_17","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028467326&doi=10.1007%2f978-3-319-63703-7_17&partnerID=40&md5=29ea934e93eb208ddcac3d5b0d9e8e85","Graduate School of Information Science and Technology, Hokkaido University, Kita 14 Nishi 9, Kita-ku, Sapporo, 060-0814, Japan","Rzepka, R., Graduate School of Information Science and Technology, Hokkaido University, Kita 14 Nishi 9, Kita-ku, Sapporo, 060-0814, Japan; Araki, K., Graduate School of Information Science and Technology, Hokkaido University, Kita 14 Nishi 9, Kita-ku, Sapporo, 060-0814, Japan",,"Inverse problems; Philosophical aspects; Reinforcement learning; Affect recognition; Cognitive bias; Decision-based; Human subjects; Human values; Inverse reinforcement learning; Real world situations; Web mining systems; Autonomous agents",Conference Paper,"Final",,Scopus,2-s2.0-85028467326
"Huang S.H., Held D., Abbeel P., Dragan A.D.","56422734600;35955893400;8269962600;55193779100;","Enabling robots to communicate their objectives",2017,"Robotics: Science and Systems","13",,,"","",,2,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048814981&partnerID=40&md5=0dd2774a9519120c230b491ea66ecd37","University of California, EECS, Berkeley, CA, United States; OpenAI, United States","Huang, S.H., University of California, EECS, Berkeley, CA, United States; Held, D., University of California, EECS, Berkeley, CA, United States; Abbeel, P., University of California, EECS, Berkeley, CA, United States, OpenAI, United States; Dragan, A.D., University of California, EECS, Berkeley, CA, United States",,"Cognitive systems; Inverse problems; Reinforcement learning; Robotics; Approximate inference; Autonomous driving; Human learning; Inverse reinforcement learning; Mental model; Objective functions; Robot behavior; Robot model; Robots",Conference Paper,"Final",,Scopus,2-s2.0-85048814981
[No author name available],[No author id available],"Uncertainty in Artificial Intelligence - Proceedings of the 33rd Conference, UAI 2017",2017,"Uncertainty in Artificial Intelligence - Proceedings of the 33rd Conference, UAI 2017",,,,"","",1203,,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031106481&partnerID=40&md5=057f494148c6ed759a4911fd8054c479",,"",,,Conference Review,"Final",,Scopus,2-s2.0-85031106481
"González D.S., Dibangoye J.S., Laugier C.","56382000700;25640845700;7007031683;","High-speed highway scene prediction based on driver models learned from demonstrations",2016,"IEEE Conference on Intelligent Transportation Systems, Proceedings, ITSC",,, 7795546,"149","155",,10,"10.1109/ITSC.2016.7795546","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85010042392&doi=10.1109%2fITSC.2016.7795546&partnerID=40&md5=da85968a3efa84f73c5d5bcefc155c8c","INRIA Grenoble Rhone-Alpes, Montbonnot- Saint-Martin, 38330, France","González, D.S., INRIA Grenoble Rhone-Alpes, Montbonnot- Saint-Martin, 38330, France; Dibangoye, J.S., INRIA Grenoble Rhone-Alpes, Montbonnot- Saint-Martin, 38330, France; Laugier, C., INRIA Grenoble Rhone-Alpes, Montbonnot- Saint-Martin, 38330, France",,"Collision avoidance; Cost functions; Highway traffic control; Intelligent systems; Intelligent vehicle highway systems; Long Term Evolution (LTE); Markov processes; Reinforcement learning; Remotely operated vehicles; Transportation; Wireless telecommunication systems; Dynamic environments; Dynamic obstacles; Inverse reinforcement learning; Linear combinations; Markov Decision Processes; Number of vehicles; Prediction-based; Semi-autonomous vehicles; Forecasting",Conference Paper,"Final",,Scopus,2-s2.0-85010042392
"Manzoor M., Kunwar F.","57193136656;55983640600;","Learn pedestrian navigation from expert",2016,"2016 2nd International Conference on Robotics and Artificial Intelligence, ICRAI 2016",,, 7791244,"147","151",,,"10.1109/ICRAI.2016.7791244","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85011017824&doi=10.1109%2fICRAI.2016.7791244&partnerID=40&md5=4a49da8be69eeceee03eaa72a8d1d4e6","Department of Mechatronics Engineering (CEME), National University of Science and Technology (NUST), Islamabad, Pakistan","Manzoor, M., Department of Mechatronics Engineering (CEME), National University of Science and Technology (NUST), Islamabad, Pakistan; Kunwar, F., Department of Mechatronics Engineering (CEME), National University of Science and Technology (NUST), Islamabad, Pakistan","Apprenticeship; IRL; SFM","Artificial intelligence; Human computer interaction; Motion estimation; Reinforcement learning; Robotics; Robots; Apprenticeship; Human robot Interaction (HRI); Human-human interactions; Intention predictions; Inverse reinforcement learning; Pedestrian behavior; Pedestrian navigation; Simulation based approaches; Human robot interaction",Conference Paper,"Final",,Scopus,2-s2.0-85011017824
"Lee K., Choi S., Oh S.","56742456900;55954115000;8703374300;","Inverse reinforcement learning with leveraged Gaussian processes",2016,"IEEE International Conference on Intelligent Robots and Systems","2016-November",, 7759575,"3907","3912",,2,"10.1109/IROS.2016.7759575","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85006379795&doi=10.1109%2fIROS.2016.7759575&partnerID=40&md5=b18c28d292aae0c076a0de015a7b009b","Department of Electrical and Computer Engineering and ASRI, Seoul National University, Seoul, 151-744, South Korea","Lee, K., Department of Electrical and Computer Engineering and ASRI, Seoul National University, Seoul, 151-744, South Korea; Choi, S., Department of Electrical and Computer Engineering and ASRI, Seoul National University, Seoul, 151-744, South Korea; Oh, S., Department of Electrical and Computer Engineering and ASRI, Seoul National University, Seoul, 151-744, South Korea",,"Demonstrations; Gaussian distribution; Gaussian noise (electronic); Intelligent robots; Inverse problems; Learning algorithms; Gaussian Processes; Generative model; Inverse reinforcement learning; Nonlinear functions; Reward function; Reinforcement learning",Conference Paper,"Final",,Scopus,2-s2.0-85006379795
"Ramirez O.A.I., Khambhaita H., Chatila R., Chetouani M., Alami R.","57206141923;48661192400;7003451196;8964513700;7003803131;","Robots learning how and where to approach people",2016,"25th IEEE International Symposium on Robot and Human Interactive Communication, RO-MAN 2016",,, 7745154,"347","353",,16,"10.1109/ROMAN.2016.7745154","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85002878691&doi=10.1109%2fROMAN.2016.7745154&partnerID=40&md5=926b0e87cb8f1c98d22ff2a577d28ad2","ISIR, CNRS, Université Pierre et Marie Curie, Paris, France; LAAS, CNRS, Laboratory for Analysis and Architecture of Systems, Toulouse, France","Ramirez, O.A.I., ISIR, CNRS, Université Pierre et Marie Curie, Paris, France; Khambhaita, H., LAAS, CNRS, Laboratory for Analysis and Architecture of Systems, Toulouse, France; Chatila, R., ISIR, CNRS, Université Pierre et Marie Curie, Paris, France; Chetouani, M., ISIR, CNRS, Université Pierre et Marie Curie, Paris, France; Alami, R., LAAS, CNRS, Laboratory for Analysis and Architecture of Systems, Toulouse, France","Approaching People; Human Aware Navigation; Inverse Reinforcement Learning","Navigation; Reinforcement learning; Approaching People; Continuous functions; Dijkstra's algorithms; Human environment; Human-aware; Inverse reinforcement learning; Linear combinations; Static obstacles; Robots",Conference Paper,"Final",,Scopus,2-s2.0-85002878691
"Srinivasan A.R., Chakraborty S.","56254010900;14619036900;","Path planning with user route preference-A reward surface approximation approach using orthogonal Legendre polynomials",2016,"IEEE International Conference on Automation Science and Engineering","2016-November",, 7743527,"1100","1105",,1,"10.1109/COASE.2016.7743527","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85000983771&doi=10.1109%2fCOASE.2016.7743527&partnerID=40&md5=645b81682cd8447f905ebd9b7d45d417","Department of Mechanical Aerospace and Biomedical Engineering, University of Tennessee, Knoxville, TN  37996, United States","Srinivasan, A.R., Department of Mechanical Aerospace and Biomedical Engineering, University of Tennessee, Knoxville, TN  37996, United States; Chakraborty, S., Department of Mechanical Aerospace and Biomedical Engineering, University of Tennessee, Knoxville, TN  37996, United States",,"Automobile drivers; Autonomous agents; Demonstrations; Functions; Inverse problems; Markov processes; Motion planning; Orthogonal functions; Reinforcement learning; Inverse reinforcement learning; Legendre polynomials; Markov Decision Processes; Origin and destinations; Orthogonal polynomial; Route preferences; State-space explosion; Surface approximation; Learning algorithms",Conference Paper,"Final",,Scopus,2-s2.0-85000983771
"Hwang K.-S., Chiang H.-Y., Jiang W.-C.","7402426737;57192553650;48361586500;","Adaboost-like method for inverse reinforcement learning",2016,"2016 IEEE International Conference on Fuzzy Systems, FUZZ-IEEE 2016",,, 7737926,"1922","1925",,2,"10.1109/FUZZ-IEEE.2016.7737926","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85006782990&doi=10.1109%2fFUZZ-IEEE.2016.7737926&partnerID=40&md5=8eb4f4357f872f0b296f65f1fd916279","Department of Electrical Engineering, National Sun Yat-sen University, Kaohsiung, Taiwan","Hwang, K.-S., Department of Electrical Engineering, National Sun Yat-sen University, Kaohsiung, Taiwan; Chiang, H.-Y., Department of Electrical Engineering, National Sun Yat-sen University, Kaohsiung, Taiwan; Jiang, W.-C., Department of Electrical Engineering, National Sun Yat-sen University, Kaohsiung, Taiwan","Adaboost classifier; Inverse reinforcement learning; Reinforcement learning; Upper confidence bounds(UCB)","Adaptive boosting; Behavioral research; Fuzzy systems; Inverse problems; Ada boost classifiers; Computation time; Intelligent behavior; Inverse reinforcement learning; Learning tasks; Reward function; Trial-and-error method; Upper confidence bound; Reinforcement learning",Conference Paper,"Final",,Scopus,2-s2.0-85006782990
"Xia C., El Kamel A.","57191588472;7003630805;","Neural inverse reinforcement learning in autonomous navigation",2016,"Robotics and Autonomous Systems","84",,,"1","14",,14,"10.1016/j.robot.2016.06.003","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84991678526&doi=10.1016%2fj.robot.2016.06.003&partnerID=40&md5=e2060b765006061c8588137dc6ff5d06","Research Center CRIStAL, UMR CNRS 9189, École Centrale de Lille, Villeneuve d'Ascq, 59651, France","Xia, C., Research Center CRIStAL, UMR CNRS 9189, École Centrale de Lille, Villeneuve d'Ascq, 59651, France; El Kamel, A., Research Center CRIStAL, UMR CNRS 9189, École Centrale de Lille, Villeneuve d'Ascq, 59651, France","Autonomous navigation; Dynamic environments; Inverse reinforcement learning; Learning from demonstration; Markov decision processes; Neural network","Air navigation; Demonstrations; Intelligent robots; Markov processes; Mobile robots; Navigation; Navigation systems; Neural networks; Robotics; Robots; Stochastic systems; Autonomous navigation; Dynamic environments; Inverse reinforcement learning; Learning from demonstration; Markov Decision Processes; Reinforcement learning",Article,"Final",,Scopus,2-s2.0-84991678526
"Gu T., Dolan J.M., Lee J.-W.","56001409400;56045990600;56237914700;","Human-like planning of swerve maneuvers for autonomous vehicles",2016,"IEEE Intelligent Vehicles Symposium, Proceedings","2016-August",, 7535466,"716","721",,6,"10.1109/IVS.2016.7535466","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84983242921&doi=10.1109%2fIVS.2016.7535466&partnerID=40&md5=cefe5da7b3f916d65477906bac576367","Department of Electrical and Computer Engineering, Carnegie Mellon University, Pittsburgh, PA, United States; Robotics Institute, Carnegie Mellon University, Pittsburgh, PA, United States; Research and Development, General Motors, Warren, MI, United States","Gu, T., Department of Electrical and Computer Engineering, Carnegie Mellon University, Pittsburgh, PA, United States; Dolan, J.M., Robotics Institute, Carnegie Mellon University, Pittsburgh, PA, United States; Lee, J.-W., Research and Development, General Motors, Warren, MI, United States",,"Behavioral research; Intelligent vehicle highway systems; Reinforcement learning; Stochastic models; Stochastic systems; Autonomous Vehicles; Driving styles; Inverse reinforcement learning; Motion planners; Parameter-tuning; Sampling-based; Smooth trajectories; Stochastic nature; Vehicles",Conference Paper,"Final",,Scopus,2-s2.0-84983242921
"Arora S., Tanner H.G.","57213211071;7007108972;","Programming by demonstration for Locally k-Testable tasks",2016,"24th Mediterranean Conference on Control and Automation, MED 2016",,, 7535995,"1146","1151",,,"10.1109/MED.2016.7535995","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84986183429&doi=10.1109%2fMED.2016.7535995&partnerID=40&md5=1d7b0dc34e9530f06879646585d3e99c","Department of Mechanical Engineering, University of Delaware, Newark, DE, United States","Arora, S., Department of Mechanical Engineering, University of Delaware, Newark, DE, United States; Tanner, H.G., Department of Mechanical Engineering, University of Delaware, Newark, DE, United States","grammatical inference; imitation learning; Learning by demonstration","Inverse problems; Markov processes; Natural language processing systems; Reinforcement learning; Grammatical inferences; Imitation learning; Inverse reinforcement learning; Language identification; Learning by demonstration; Markov Decision Processes; Predicate abstractions; Programming by demon-stration; Demonstrations",Conference Paper,"Final",,Scopus,2-s2.0-84986183429
"El-Hussieny H., Assal S.F.M., Abouelsoud A.A., Megahed S.M., Ogasawara T.","20433398600;8571391800;6601958127;6701420832;7201579979;","Incremental learning of reach-to-grasp behavior: A PSO-based Inverse optimal control approach",2016,"Proceedings of the 2015 7th International Conference of Soft Computing and Pattern Recognition, SoCPaR 2015",,, 7492796,"129","135",,2,"10.1109/SOCPAR.2015.7492796","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84979300847&doi=10.1109%2fSOCPAR.2015.7492796&partnerID=40&md5=81892d7cc42bf2b05c4128040dda53ae","Mechatronics and Robotics Engineering Department, School of Innovative Design Engineering, Egypt-Japan University of Science and Technology (E-JUST), Egypt; Nara Institute of Science and Technology, 8916-5, Takayama, Ikoma, Nara, 630-0192, Japan; Department of Production Engineering and Mechanical Design, Faculty of Engineering, Tanta University, Egypt; Electronics and Communications Eng. Dept., Faculty of Engineering, Cairo University, Egypt; Mechanical Design and Production Engineering Department, Faculty of Engineering, Cairo University, Egypt","El-Hussieny, H., Mechatronics and Robotics Engineering Department, School of Innovative Design Engineering, Egypt-Japan University of Science and Technology (E-JUST), Egypt, Nara Institute of Science and Technology, 8916-5, Takayama, Ikoma, Nara, 630-0192, Japan; Assal, S.F.M., Mechatronics and Robotics Engineering Department, School of Innovative Design Engineering, Egypt-Japan University of Science and Technology (E-JUST), Egypt, Department of Production Engineering and Mechanical Design, Faculty of Engineering, Tanta University, Egypt; Abouelsoud, A.A., Mechatronics and Robotics Engineering Department, School of Innovative Design Engineering, Egypt-Japan University of Science and Technology (E-JUST), Egypt, Electronics and Communications Eng. Dept., Faculty of Engineering, Cairo University, Egypt; Megahed, S.M., Mechatronics and Robotics Engineering Department, School of Innovative Design Engineering, Egypt-Japan University of Science and Technology (E-JUST), Egypt, Mechanical Design and Production Engineering Department, Faculty of Engineering, Cairo University, Egypt; Ogasawara, T., Nara Institute of Science and Technology, 8916-5, Takayama, Ikoma, Nara, 630-0192, Japan","Incremental learning; Inverse reinforcement learning; Linear Quadratic Regulator; Particle Swarm Optimization; Reach-to-grasp modeling","Computation theory; Cost functions; Costs; Evolutionary algorithms; Optimization; Particle swarm optimization (PSO); Pattern recognition; Reinforcement learning; Soft computing; Evolutionary Optimization Techniques; Incremental learning; Inverse reinforcement learning; Linear dynamic equations; Linear quadratic regulator; Neuroscience literature; Reach to grasp; Reach-to-grasp movements; Inverse problems",Conference Paper,"Final",,Scopus,2-s2.0-84979300847
"Hwang K.-S., Jiang W.-C., Tseng Y.-C.","7402426737;48361586500;57189684083;","An unified approach to inverse reinforcement learning by oppositive demonstrations",2016,"Proceedings of the IEEE International Conference on Industrial Technology","2016-May",, 7475012,"1664","1668",,1,"10.1109/ICIT.2016.7475012","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84974533403&doi=10.1109%2fICIT.2016.7475012&partnerID=40&md5=9218f16f3605e8abbc35fd8a591b8b9d","Department of Electrical Engineering, National Sun Yat-sen University, Kaohsiung, Taiwan","Hwang, K.-S., Department of Electrical Engineering, National Sun Yat-sen University, Kaohsiung, Taiwan; Jiang, W.-C., Department of Electrical Engineering, National Sun Yat-sen University, Kaohsiung, Taiwan; Tseng, Y.-C., Department of Electrical Engineering, National Sun Yat-sen University, Kaohsiung, Taiwan","Inverse Reinforcement learning; Reinforcement learning; Reward functionv Feature weight","Decision making; Demonstrations; Inverse problems; Complex problems; Dynamic environments; Feature weight; Inverse reinforcement learning; Performance of algorithm; Reinforcement learning techniques; Sequential decision making; Unified approach; Reinforcement learning",Conference Paper,"Final",,Scopus,2-s2.0-84974533403
"Banovic N., Buzali T., Chevalier F., Mankoff J., Dey A.K.","54794750000;57193576344;18834382100;57203175552;7101701731;","Modeling and understanding human routine behavior",2016,"Conference on Human Factors in Computing Systems - Proceedings",,,,"248","260",,28,"10.1145/2858036.2858557","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85014990471&doi=10.1145%2f2858036.2858557&partnerID=40&md5=4a652384d9fbc5cdeaf1d7aa2ae4ed18","Human-Computer Interaction Institute, CMU, Pittsburgh, PA  15213, United States; INRIA, Lille, France","Banovic, N., Human-Computer Interaction Institute, CMU, Pittsburgh, PA  15213, United States; Buzali, T., Human-Computer Interaction Institute, CMU, Pittsburgh, PA  15213, United States; Chevalier, F., INRIA, Lille, France; Mankoff, J., Human-Computer Interaction Institute, CMU, Pittsburgh, PA  15213, United States; Dey, A.K., Human-Computer Interaction Institute, CMU, Pittsburgh, PA  15213, United States","Inverse reinforcement learning; Markov decision process","Byproducts; Human computer interaction; Human engineering; Markov processes; Reinforcement learning; Activity predictions; Bad habits; Causal relationships; Inverse reinforcement learning; Markov Decision Processes; Repetitive task; Behavioral research",Conference Paper,"Final",Open Access,Scopus,2-s2.0-85014990471
"Tegelund B., Son H., Lee D.","57188668395;55428430500;55645739400;","A task-oriented service personalization scheme for smart environments using reinforcement learning",2016,"2016 IEEE International Conference on Pervasive Computing and Communication Workshops, PerCom Workshops 2016",,, 7457110,"","",,4,"10.1109/PERCOMW.2016.7457110","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84966666615&doi=10.1109%2fPERCOMW.2016.7457110&partnerID=40&md5=baf5e68163fa278f820db16929ce294f","School of Computer Science, KAIST, Daejeon, South Korea","Tegelund, B., School of Computer Science, KAIST, Daejeon, South Korea; Son, H., School of Computer Science, KAIST, Daejeon, South Korea; Lee, D., School of Computer Science, KAIST, Daejeon, South Korea",,"Inverse problems; Optimization; Ubiquitous computing; Inverse reinforcement learning; Optimization problems; Service personalization; Smart environment; Task-oriented; User's preferences; Reinforcement learning",Conference Paper,"Final",,Scopus,2-s2.0-84966666615
"Herman M., Gindele T., Wagner J., Schmitt F., Burgard W.","56742755300;24476503400;57191835760;57190805464;7003610380;","Simultaneous estimation of rewards and dynamics from noisy expert demonstrations",2016,"ESANN 2016 - 24th European Symposium on Artificial Neural Networks",,,,"677","682",,,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84994180781&partnerID=40&md5=38353fa4ecbdb0d2c533896ca6509b29","Robert Bosch GmbH, Stuttgart, 70442, Germany; University of Freiburg, Department of Computer Science, Freiburg, 79110, Germany","Herman, M., Robert Bosch GmbH, Stuttgart, 70442, Germany, University of Freiburg, Department of Computer Science, Freiburg, 79110, Germany; Gindele, T., Robert Bosch GmbH, Stuttgart, 70442, Germany; Wagner, J., Robert Bosch GmbH, Stuttgart, 70442, Germany; Schmitt, F., Robert Bosch GmbH, Stuttgart, 70442, Germany; Burgard, W., University of Freiburg, Department of Computer Science, Freiburg, 79110, Germany",,"Artificial intelligence; Demonstrations; Learning algorithms; Learning systems; Markov processes; Neural networks; Reinforcement learning; Stochastic systems; System theory; Gradient based; Inverse reinforcement learning; Markov Decision Processes; Reward function; Simultaneous estimation; State transitions; Stochastic policy; System Dynamics; Inverse problems",Conference Paper,"Final",,Scopus,2-s2.0-84994180781
"Sexton T., Ren M.Y.","57192815712;57194968344;","Learning human search strategies from a crowdsourcing game",2016,"Proceedings of the ASME Design Engineering Technical Conference","2A-2016",,,"","",,2,"10.1115/DETC2016-59775","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85008234786&doi=10.1115%2fDETC2016-59775&partnerID=40&md5=c030376dfde97fc88765512bdc42c5a5","Mechanical Engineering, Arizona State University, Tempe, AZ  85287, United States","Sexton, T., Mechanical Engineering, Arizona State University, Tempe, AZ  85287, United States; Ren, M.Y., Mechanical Engineering, Arizona State University, Tempe, AZ  85287, United States",,"Computer aided design; Design; Optimal systems; Optimization; Reinforcement learning; Algorithmic parameters; Bayesian optimization; Human demonstrations; Inverse reinforcement learning; Learning capabilities; Near-optimal solutions; Optimal control solution; Simulation studies; Inverse problems",Conference Paper,"Final",,Scopus,2-s2.0-85008234786
"Odom P., Natarajan S.","56050615500;57203254125;","Active advice seeking for inverse reinforcement learning",2016,"Proceedings of the International Joint Conference on Autonomous Agents and Multiagent Systems, AAMAS",,,,"503","511",,8,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85014229219&partnerID=40&md5=c8c00a55bba7aa2336e4fee588e9494f","Indiana University, Bloomington, IN, United States","Odom, P., Indiana University, Bloomington, IN, United States; Natarajan, S., Indiana University, Bloomington, IN, United States","Active advice seeking; Advice-based learning; Inverse reinforcement learning","Artificial intelligence; Autonomous agents; Decision making; Demonstrations; Intelligent systems; Inverse problems; Learning systems; Multi agent systems; Reinforcement learning; Active advice seeking; Active Learning; Active learning systems; Advice-based learning; Human expert; Inverse reinforcement learning; Optimal decision making; Traditional systems; Learning algorithms",Conference Paper,"Final",,Scopus,2-s2.0-85014229219
"Kim D., Kim D.-H., Moon I.-C.","57188761105;57198637806;56038773000;","Inverse modeling of combat behavior with virtual-constructive simulation training",2016,"Communications in Computer and Information Science","644",,,"597","606",,1,"10.1007/978-981-10-2666-9_60","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988805130&doi=10.1007%2f978-981-10-2666-9_60&partnerID=40&md5=54451e46706bf69be9352cdff810a42a","Department of Industrial and Systems Engineering, KAIST, Daejeon, 305701, South Korea","Kim, D., Department of Industrial and Systems Engineering, KAIST, Daejeon, 305701, South Korea; Kim, D.-H., Department of Industrial and Systems Engineering, KAIST, Daejeon, 305701, South Korea; Moon, I.-C., Department of Industrial and Systems Engineering, KAIST, Daejeon, 305701, South Korea",,"E-learning; Inverse problems; Reinforcement learning; Combat environments; Descriptive statistics; Inverse modeling; Inverse reinforcement learning; Simple modeling; Simulation applications; Simulation training; Synthetic environments; Virtual reality",Conference Paper,"Final",,Scopus,2-s2.0-84988805130
"Herman M., Gindele T., Wagner J., Schmitt F., Burgard W.","56742755300;24476503400;57191835760;57190805464;7003610380;","Inverse reinforcement learning with simultaneous estimation of rewards and dynamics",2016,"Proceedings of the 19th International Conference on Artificial Intelligence and Statistics, AISTATS 2016",,,,"102","110",,11,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052981199&partnerID=40&md5=4e1525d964d621ec5b67c295a6b86bb0","Robert Bosch GmbH, Stuttgart, D-70442, Germany; University of Freiburg, Freiburg, D-79110, Germany","Herman, M., Robert Bosch GmbH, Stuttgart, D-70442, Germany, University of Freiburg, Freiburg, D-79110, Germany; Gindele, T., Robert Bosch GmbH, Stuttgart, D-70442, Germany; Wagner, J., Robert Bosch GmbH, Stuttgart, D-70442, Germany; Schmitt, F., Robert Bosch GmbH, Stuttgart, D-70442, Germany; Burgard, W., University of Freiburg, Freiburg, D-79110, Germany",,"Machine learning; Markov processes; Petroleum reservoir evaluation; Problem solving; Reinforcement learning; Stochastic systems; Additional samples; Combined optimization problem; Inverse reinforcement learning; Markov Decision Processes; Reward function; Simultaneous estimation; Transfer learning; Transition model; Inverse problems",Conference Paper,"Final",,Scopus,2-s2.0-85052981199
"El Asri L., Piot B., Geist M., Laroche R., Pietquin O.","55767861100;55697434100;25929145100;35179791100;16040586900;","Score-based inverse reinforcement learning",2016,"Proceedings of the International Joint Conference on Autonomous Agents and Multiagent Systems, AAMAS",,,,"457","465",,6,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85014225758&partnerID=40&md5=063b9b21bb00add08f128d9a5ad7b000","Orange Labs and Maluuba, Montréal, Canada; Univ. Lille, CNRS, Centrale Lille, Inria UMR 9189, CRIStAL, Lille, France; UMI 2958, Georgia Tech-CNRS, Centrale Supélec, Université Paris-Saclay, Metz, France; Orange Labs, Issy Les Moulineaux, France","El Asri, L., Orange Labs and Maluuba, Montréal, Canada; Piot, B., Univ. Lille, CNRS, Centrale Lille, Inria UMR 9189, CRIStAL, Lille, France; Geist, M., UMI 2958, Georgia Tech-CNRS, Centrale Supélec, Université Paris-Saclay, Metz, France; Laroche, R., Orange Labs, Issy Les Moulineaux, France; Pietquin, O., Univ. Lille, CNRS, Centrale Lille, Inria UMR 9189, CRIStAL, Lille, France","Inverse reinforcement learning; Learning from demonstration; Markov decision processes; Reinforcement learning; Spoken dialogue systems","Autonomous agents; Markov processes; Multi agent systems; Optimization; Speech processing; Trajectories; Inverse reinforcement learning; Learning from demonstration; Least square regression; Markov Decision Processes; Near-optimal policies; Spoken dialogue system; Standard setting; Theoretical guarantees; Reinforcement learning",Conference Paper,"Final",,Scopus,2-s2.0-85014225758
"Bogert K., Lin J.F.-S., Doshi P., Kulic D.","36095803300;55891993000;23008336000;10244652900;","Expectation-maximization for inverse reinforcement learning with hidden data",2016,"Proceedings of the International Joint Conference on Autonomous Agents and Multiagent Systems, AAMAS",,,,"1034","1042",,4,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85014148697&partnerID=40&md5=6492a3e2b9a3c155bd95db4381b4158a","THINC Lab., Dept. of Computer Science, University of Georgia, Athens, GA  30602, United States; Dept. of Electrical and Computer Engg., University of Waterloo, Waterloo, ON  N2L 3G1, Canada","Bogert, K., THINC Lab., Dept. of Computer Science, University of Georgia, Athens, GA  30602, United States; Lin, J.F.-S., Dept. of Electrical and Computer Engg., University of Waterloo, Waterloo, ON  N2L 3G1, Canada; Doshi, P., THINC Lab., Dept. of Computer Science, University of Georgia, Athens, GA  30602, United States; Kulic, D., Dept. of Electrical and Computer Engg., University of Waterloo, Waterloo, ON  N2L 3G1, Canada",,"Autonomous agents; Inverse problems; Maximum principle; Multi agent systems; Apprenticeship learning; Expectation - maximizations; Hidden variable; Information missing; Inverse reinforcement learning; Missing information; Missing values; Nonconvex problem; Reinforcement learning",Conference Paper,"Final",,Scopus,2-s2.0-85014148697
"Loftin R., MacGlashan J., Peng B., Taylor M.E., Littman M.L., Roberts D.L.","55669790200;21934008200;56393762000;10339318000;7006510438;55338221500;","Towards behavior-aware model learning from human-generated trajectories",2016,"AAAI Fall Symposium - Technical Report","FS-16-01 - FS-16-05",,,"67","70",,,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85025802345&partnerID=40&md5=346c0ecb0615a8337f1c610bc6440fa0","North Carolina State University, United States; Brown University, United States; Washington State University, United States","Loftin, R., North Carolina State University, United States; MacGlashan, J., Brown University, United States; Peng, B., Washington State University, United States; Taylor, M.E., North Carolina State University, United States; Littman, M.L., Brown University, United States; Roberts, D.L., North Carolina State University, United States",,"Behavioral research; Cognitive systems; Education; Human robot interaction; Intelligent robots; Inverse problems; Markov processes; Problem solving; Reinforcement learning; Complementary problems; Continuous State Space; Generated trajectories; Inverse reinforcement learning; Markov Decision Processes; Model representation; Policy gradient; Transition dynamics; Learning algorithms",Conference Paper,"Final",,Scopus,2-s2.0-85025802345
"Hadfield-Menell D., Dragan A., Abbeel P., Russell S.","55921863400;55193779100;8269962600;7401538237;","Cooperative inverse reinforcement learning",2016,"Advances in Neural Information Processing Systems",,,,"3916","3924",,57,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018873749&partnerID=40&md5=60d0969695bd71e2eeaa97a256706d97","Electrical Engineering and Computer Science, University of California at Berkeley, Berkeley, CA  94709, United States","Hadfield-Menell, D., Electrical Engineering and Computer Science, University of California at Berkeley, Berkeley, CA  94709, United States; Dragan, A., Electrical Engineering and Computer Science, University of California at Berkeley, Berkeley, CA  94709, United States; Abbeel, P., Electrical Engineering and Computer Science, University of California at Berkeley, Berkeley, CA  94709, United States; Russell, S., Electrical Engineering and Computer Science, University of California at Berkeley, Berkeley, CA  94709, United States",,"Computer games; Reinforcement learning; Active teachings; Alignment Problems; Autonomous systems; Communicative actions; Formal definition; Inverse reinforcement learning; Partial information; Reward function; Inverse problems",Conference Paper,"Final",,Scopus,2-s2.0-85018873749
"Okal B., Arras K.O.","56132081300;6701603183;","Practical bayesian inverse reinforcement learning for robot navigation",2016,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","9853 LNCS",,,"271","274",,,"10.1007/978-3-319-46131-1_33","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988646125&doi=10.1007%2f978-3-319-46131-1_33&partnerID=40&md5=e16ec10110f86e270ec8f08b523c03e0","Social Robotics Lab, University of Freiburg, Freiburg, Germany; Bosch Corporate Research, Robert-Bosch GmbH, Renningen, Germany","Okal, B., Social Robotics Lab, University of Freiburg, Freiburg, Germany; Arras, K.O., Social Robotics Lab, University of Freiburg, Freiburg, Germany, Bosch Corporate Research, Robert-Bosch GmbH, Renningen, Germany","Inverse reinforcement learning; Representation; Robot navigation","Artificial intelligence; Behavioral research; Graphic methods; Learning systems; Navigation; Robots; Experimental evaluation; Human demonstrations; Inverse reinforcement learning; Learning behavior; Real-world task; Representation; Robot navigation; Task constraints; Reinforcement learning",Conference Paper,"Final",,Scopus,2-s2.0-84988646125
"Shiarlis K., Messias J., Whiteson S.","57193494529;55208968600;10240257400;","Inverse reinforcement learning from failure",2016,"Proceedings of the International Joint Conference on Autonomous Agents and Multiagent Systems, AAMAS",,,,"1060","1068",,11,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85014246612&partnerID=40&md5=37d2f2a842952f6d75f19704e22eb23d","Informatics Institute, University of Amsterdam, Science Park 904, Amsterdam, Netherlands; Dept. of Computer Science, University of Oxford, Wolfson Building, Parks Rd, Oxford, United Kingdom","Shiarlis, K., Informatics Institute, University of Amsterdam, Science Park 904, Amsterdam, Netherlands; Messias, J., Informatics Institute, University of Amsterdam, Science Park 904, Amsterdam, Netherlands; Whiteson, S., Dept. of Computer Science, University of Oxford, Wolfson Building, Parks Rd, Oxford, United Kingdom","Inverse reinforcement learning; Learning from demonstration; Machine learning; Robotics; Social navigation","Autonomous agents; Constrained optimization; Demonstrations; Entropy; Inverse problems; Learning systems; Multi agent systems; Robotics; Robots; Complex task; Inverse reinforcement learning; Learning from demonstration; Optimisations; Reward function; Social navigation; State of the art; Trial and error; Reinforcement learning",Conference Paper,"Final",,Scopus,2-s2.0-85014246612
"Suay H.B., Brys T., Taylor M.E., Chernova S.","35106246500;55420448800;10339318000;56084330100;","Learning from demonstration for shaping through inverse reinforcement learning",2016,"Proceedings of the International Joint Conference on Autonomous Agents and Multiagent Systems, AAMAS",,,,"429","437",,18,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85014262693&partnerID=40&md5=037cfbf97668f842417be3e12bdcbcb7","Worcester Polytechnic Institute, United States; Vrije Universiteit, Brussel, Netherlands; Washington State University, United States; Georgia Institute of Technology, United States","Suay, H.B., Worcester Polytechnic Institute, United States; Brys, T., Vrije Universiteit, Brussel, Netherlands; Taylor, M.E., Washington State University, United States; Chernova, S., Georgia Institute of Technology, United States","Communication; Learning agent capabilities (agent models; Learning and adaptation; Observation); Reward structures for learning","Autonomous agents; Communication; Demonstrations; Intelligent agents; Inverse problems; Multi agent systems; Agent model; Asymptotic performance; Inverse reinforcement learning; Learning and adaptation; Learning from demonstration; Observation); Reinforcement learning agent; Reward structures for learning; Reinforcement learning",Conference Paper,"Final",,Scopus,2-s2.0-85014262693
"Ho J., Ermon S.","55921701300;35791579200;","Generative adversarial imitation learning",2016,"Advances in Neural Information Processing Systems",,,,"4572","4580",,194,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018872345&partnerID=40&md5=00a24f1e6d46cb44f74266997a7e1123","OpenAI, United States; Stanford University, United States","Ho, J., OpenAI, United States; Ermon, S., Stanford University, United States",,"Cost functions; Inverse problems; Learning algorithms; Adversarial networks; High dimensional environment; Imitation learning; Inverse reinforcement learning; Model free; Model-free method; Performance Gain; Reinforcement signal; Reinforcement learning",Conference Paper,"Final",,Scopus,2-s2.0-85018872345
"Pérez-Higueras N., Caballero F., Merino L.","56414879200;8956620100;7003946345;","Learning robot navigation behaviors by demonstration using a RRT* planner",2016,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","9979 LNAI",,,"1","10",,5,"10.1007/978-3-319-47437-3_1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84992489167&doi=10.1007%2f978-3-319-47437-3_1&partnerID=40&md5=fbd4d56b156a3c5478aeb396397696b0","School of Engineering, Universidad Pablo de Olavide, Crta. Utrera km 1, Seville, Spain; Department of System Engineering and Automation, University of Seville, Camino de los Descubrimientos, s/n, Seville, Spain","Pérez-Higueras, N., School of Engineering, Universidad Pablo de Olavide, Crta. Utrera km 1, Seville, Spain; Caballero, F., Department of System Engineering and Automation, University of Seville, Camino de los Descubrimientos, s/n, Seville, Spain; Merino, L., School of Engineering, Universidad Pablo de Olavide, Crta. Utrera km 1, Seville, Spain",,"Air navigation; Cost functions; Demonstrations; Inverse problems; Learning algorithms; Reinforcement learning; Robot programming; Robotics; Inverse reinforcement learning; Rapidly-exploring random trees; Robot navigation; Robot planning; Robots",Conference Paper,"Final",,Scopus,2-s2.0-84992489167
"Junges S., Katoen J.-P., Jansen N., Topcu U.","55321319200;7003679176;36646082100;23570155900;","Probabilistic verification for cognitive models: Controller synthesis and model evaluation",2016,"AAAI Fall Symposium - Technical Report","FS-16-01 - FS-16-05",,,"185","188",,,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85025834417&partnerID=40&md5=76d1506991ee9735282b1a2802fa4cde","RWTH Aachen University, Germany; University of Texas, Austin, United States","Junges, S., RWTH Aachen University, Germany; Katoen, J.-P., RWTH Aachen University, Germany; Jansen, N., University of Texas, Austin, United States; Topcu, U., RWTH Aachen University, Germany",,"Behavioral research; Cognitive systems; Formal verification; Human robot interaction; Intelligent robots; Model checking; Reinforcement learning; Robots; Stochastic systems; Controller synthesis; Flexible framework; Inverse reinforcement learning; Parallel composition; Probabilistic model checking; Probabilistic verification; Robotics applications; Stochastic behavior; Stochastic models",Conference Paper,"Final",,Scopus,2-s2.0-85025834417
"Ho M.K., Littman M.L., MacGlashan J., Cushman F., Austerweil J.L.","57193692515;7006510438;21934008200;8286891500;36010482500;","Showing versus doing: Teaching by demonstration",2016,"Advances in Neural Information Processing Systems",,,,"3035","3043",,13,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018924601&partnerID=40&md5=10b6e1f90afc25256d2d556747b2a7ef","Department of Cognitive, Linguistic, and Psychological Sciences, Brown University, Providence, RI  02912, United States; Department of Computer Science, Brown University, Providence, RI  02912, United States; Department of Psychology, Harvard University, Cambridge, MA  02138, United States; Department of Psychology, University of Wisconsin-Madison, Madison, WI  53706, United States","Ho, M.K., Department of Cognitive, Linguistic, and Psychological Sciences, Brown University, Providence, RI  02912, United States; Littman, M.L., Department of Computer Science, Brown University, Providence, RI  02912, United States; MacGlashan, J., Department of Computer Science, Brown University, Providence, RI  02912, United States; Cushman, F., Department of Psychology, Harvard University, Cambridge, MA  02138, United States; Austerweil, J.L., Department of Psychology, University of Wisconsin-Madison, Madison, WI  53706, United States",,"Bayesian networks; Reinforcement learning; Bayesian model; Inverse reinforcement learning; Teaching by demonstration; Demonstrations",Conference Paper,"Final",,Scopus,2-s2.0-85018924601
"Herman M., Gindele T., Wagner J., Schmitt F., Quignon C., Burgard W.","56742755300;24476503400;57191835760;57190805464;57192673998;7003610380;","Learning high-level navigation strategies via inverse reinforcement learning: A comparative analysis",2016,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","9992 LNAI",,,"525","534",,2,"10.1007/978-3-319-50127-7_45","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85007173587&doi=10.1007%2f978-3-319-50127-7_45&partnerID=40&md5=e929d36945195b34835d3a2aff3d81ad","Robert Bosch GmbH, Stuttgart, 70442, Germany; University of Freiburg, Freiburg, 79110, Germany","Herman, M., Robert Bosch GmbH, Stuttgart, 70442, Germany, University of Freiburg, Freiburg, 79110, Germany; Gindele, T., Robert Bosch GmbH, Stuttgart, 70442, Germany; Wagner, J., Robert Bosch GmbH, Stuttgart, 70442, Germany; Schmitt, F., Robert Bosch GmbH, Stuttgart, 70442, Germany; Quignon, C., Robert Bosch GmbH, Stuttgart, 70442, Germany; Burgard, W., University of Freiburg, Freiburg, 79110, Germany","Inverse Reinforcement Learning; Learning from demonstration; Reinforcement Learning; Simultaneous Estimation of Rewards and Dynamics","Artificial intelligence; Demonstrations; Dynamics; Markov processes; Navigation; Robot programming; Robots; Comparative analysis; Inverse reinforcement learning; Learning from demonstration; Markov Decision Processes; Navigation strategies; Programming technique; Reward function; Simultaneous estimation; Reinforcement learning",Conference Paper,"Final",,Scopus,2-s2.0-85007173587
"Pirotta M., Restelli M.","49362257000;6603404086;","Inverse reinforcement learning through policy gradient minimization",2016,"30th AAAI Conference on Artificial Intelligence, AAAI 2016",,,,"1993","1999",,6,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85007188327&partnerID=40&md5=79d1e0384f7b49e1ea8c97eb87a0d253","Dipartimento di ElettronicaInformazione e Bioingegneria, Politecnico di Milano, Piazza Leonardo da Vinci, 32, Milan, I-20133, Italy","Pirotta, M., Dipartimento di ElettronicaInformazione e Bioingegneria, Politecnico di Milano, Piazza Leonardo da Vinci, 32, Milan, I-20133, Italy; Restelli, M., Dipartimento di ElettronicaInformazione e Bioingegneria, Politecnico di Milano, Piazza Leonardo da Vinci, 32, Milan, I-20133, Italy",,"Artificial intelligence; Optimization; Reinforcement learning; Empirical evaluations; Inverse reinforcement learning; Linear combinations; Linear quadratic regulator; Optimal policies; Optimization problems; Policy gradient; State of the art; Inverse problems",Conference Paper,"Final",,Scopus,2-s2.0-85007188327
"Kim B., Pineau J.","56123347300;13404973100;","Socially Adaptive Path Planning in Human Environments Using Inverse Reinforcement Learning",2016,"International Journal of Social Robotics","8","1",,"51","66",,47,"10.1007/s12369-015-0310-2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84957605937&doi=10.1007%2fs12369-015-0310-2&partnerID=40&md5=a22e71d3d9cf72bc78001d0e0625f50c","School of Computer Science, McGill University, 3480 University, Montreal, Canada","Kim, B., School of Computer Science, McGill University, 3480 University, Montreal, Canada; Pineau, J., School of Computer Science, McGill University, 3480 University, Montreal, Canada","Inverse reinforcement learning; Learning from demonstration; Navigation; Obstacle avoidance; RGB-D optical flow","Adaptive optics; Collision avoidance; Cost functions; Extraction; Feature extraction; Graph theory; Motion planning; Navigation; Robots; Trajectories; Adaptive path planning; Dynamic environments; Inverse reinforcement learning; Learning from demonstration; Performance criterion; Robotic wheelchairs; Surrounding obstacles; Threelayer architecture; Reinforcement learning",Article,"Final",,Scopus,2-s2.0-84957605937
"Sadigh D., Sastry S., Seshia S.A., Dragan A.D.","55052871500;56111601700;56434045800;55193779100;","Planning for autonomous cars that leverage effects on human actions",2016,"Robotics: Science and Systems","12",,,"","",,71,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041945463&partnerID=40&md5=b96312420b8f99d534323c7ac22e0bb0","University of California, Berkeley, United States","Sadigh, D., University of California, Berkeley, United States; Sastry, S., University of California, Berkeley, United States; Seshia, S.A., University of California, Berkeley, United States; Dragan, A.D., University of California, Berkeley, United States",,"Dynamical systems; Inverse problems; Reinforcement learning; Robot programming; Robotics; Autonomous car; Human actions; Human drivers; Inverse reinforcement learning; Leverage effects; Reward function; Robot plan; User study; Human robot interaction",Conference Paper,"Final",,Scopus,2-s2.0-85041945463
[No author name available],[No author id available],"29th Australasian Joint Conference on Artificial Intelligence, AI 2016",2016,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","9992 LNAI",,,"1","728",,,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85007197675&partnerID=40&md5=d7c4d395b993804f5ab821d97b0edc8a",,"",,,Conference Review,"Final",,Scopus,2-s2.0-85007197675
[No author name available],[No author id available],"23rd International Conference on Neural Information Processing, ICONIP 2016",2016,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","9947 LNCS",,,"1","638",,,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84992623347&partnerID=40&md5=e229a32c0bd42694699b6bbda1ffeea1",,"",,,Conference Review,"Final",,Scopus,2-s2.0-84992623347
"Ranchod P., Rosman B., Konidaris G.","56407112400;36464253300;14822243500;","Nonparametric Bayesian reward segmentation for skill discovery using inverse reinforcement learning",2015,"IEEE International Conference on Intelligent Robots and Systems","2015-December",, 7353414,"471","477",,14,"10.1109/IROS.2015.7353414","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84958154263&doi=10.1109%2fIROS.2015.7353414&partnerID=40&md5=9ac49604f7077d87e9a643a54f6e8768","School of Computer Science and Applied Mathematics, University of the Witwatersrand, Johannesburg, South Africa; Departments of Computer Science and Electrical and Computer Engineering, Duke UniversityNC, United States; Council for Scientific and Industrial Research, Pretoria, South Africa","Ranchod, P., School of Computer Science and Applied Mathematics, University of the Witwatersrand, Johannesburg, South Africa; Rosman, B., School of Computer Science and Applied Mathematics, University of the Witwatersrand, Johannesburg, South Africa, Council for Scientific and Industrial Research, Pretoria, South Africa; Konidaris, G., Departments of Computer Science and Electrical and Computer Engineering, Duke UniversityNC, United States","Bayes methods; Context; Heuristic algorithms; Hidden Markov models; Learning (artificial intelligence); Markov processes; Trajectory","Artificial intelligence; Heuristic algorithms; Heuristic methods; Hidden Markov models; Intelligent robots; Inverse problems; Markov processes; Trajectories; Bayes method; Context; Input trajectory; Inverse reinforcement learning; Learning (artificial intelligence); Markov Decision Processes; Non-parametric Bayesian; Nonparametric approaches; Reinforcement learning",Conference Paper,"Final",,Scopus,2-s2.0-84958154263
"Masuyama G., Umeda K.","55211221400;7102144600;","Apprenticeship learning based on inconsistent demonstrations",2015,"IEEE International Conference on Intelligent Robots and Systems","2015-December",, 7354121,"5273","5278",,,"10.1109/IROS.2015.7354121","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84958231206&doi=10.1109%2fIROS.2015.7354121&partnerID=40&md5=30fff1d701bbde99af7a27f1ab8a5987","Department of Precision Mechanics, Faculty of Science and Engineering, Chuo University, 1-13-27 Kasuga, Bunkyo-ku, Tokyo, 113-8551, Japan","Masuyama, G., Department of Precision Mechanics, Faculty of Science and Engineering, Chuo University, 1-13-27 Kasuga, Bunkyo-ku, Tokyo, 113-8551, Japan; Umeda, K., Department of Precision Mechanics, Faculty of Science and Engineering, Chuo University, 1-13-27 Kasuga, Bunkyo-ku, Tokyo, 113-8551, Japan","Context; Estimation; Learning (artificial intelligence); Robot control; Training data; Trajectory","Apprentices; Artificial intelligence; Demonstrations; Estimation; Intelligent robots; Robots; Trajectories; Affine transformations; Apprenticeship learning; Context; Inverse reinforcement learning; Learning (artificial intelligence); Non-stationarities; Robot controls; Training data; Reinforcement learning",Conference Paper,"Final",,Scopus,2-s2.0-84958231206
"Almingol J., Montesano L.","56096608600;6602849539;","Learning multiple behaviours using hierarchical clustering of rewards",2015,"IEEE International Conference on Intelligent Robots and Systems","2015-December",, 7354033,"4608","4613",,,"10.1109/IROS.2015.7354033","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84958205829&doi=10.1109%2fIROS.2015.7354033&partnerID=40&md5=7ab7cee65d8be18e898e6de5048c046e","Universidad de Zaragoza, Instituto de Investigación en Ingeniería de Aragón (I3A), Spain","Almingol, J., Universidad de Zaragoza, Instituto de Investigación en Ingeniería de Aragón (I3A), Spain; Montesano, L., Universidad de Zaragoza, Instituto de Investigación en Ingeniería de Aragón (I3A), Spain","Aerospace electronics; Clustering algorithms; Cost function; Entropy; Learning (artificial intelligence); Space exploration; Trajectory","Artificial intelligence; Cost functions; Entropy; Intelligent robots; Inverse problems; Reinforcement learning; Robotics; Robots; Space research; Trajectories; Aerospace electronics; Hier-archical clustering; Hierarchical clustering approach; Inverse reinforcement learning; Learning (artificial intelligence); Learning from demonstration; Similarity metrics; Space explorations; Clustering algorithms",Conference Paper,"Final",,Scopus,2-s2.0-84958205829
"Tsunekawa H., Suzuki T., Hamagami T.","57140196900;56808086500;8317852200;","Examination of skill-based learning by inverse reinforcement learning using evolutionary process",2015,"Proceedings of 2015 IEEE 9th International Conference on Intelligent Systems and Control, ISCO 2015",,, 7282296,"","",,3,"10.1109/ISCO.2015.7282296","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84959162902&doi=10.1109%2fISCO.2015.7282296&partnerID=40&md5=96f23e00cf57e49d013c86be0dd94775","Yokohama National University, Yokohama, Japan","Tsunekawa, H., Yokohama National University, Yokohama, Japan; Suzuki, T., Yokohama National University, Yokohama, Japan; Hamagami, T., Yokohama National University, Yokohama, Japan","Evolutionary Process; Inverse Reinforcement Learning; Reinforcement Learning; Skill-based Learning","Intelligent systems; Inverse problems; Behavior rules; Driving tasks; Evolutionary process; Inverse reinforcement learning; Learning convergence; Process of learning; Reward function; Skill-based Learning; Reinforcement learning",Conference Paper,"Final",,Scopus,2-s2.0-84959162902
"Mohammed R.A.A., Staadt O.","55536402900;6602657927;","Learning eye movements strategies on tiled Large High-Resolution Displays using inverse reinforcement learning",2015,"Proceedings of the International Joint Conference on Neural Networks","2015-September",, 7280675,"","",,2,"10.1109/IJCNN.2015.7280675","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84950969117&doi=10.1109%2fIJCNN.2015.7280675&partnerID=40&md5=55c573c2345f921ba65fa45690ebb671","Institute of Computer Science, University of Rostock, Germany","Mohammed, R.A.A., Institute of Computer Science, University of Rostock, Germany; Staadt, O., Institute of Computer Science, University of Rostock, Germany","Learning (artificial intelligence); Sensors; Visualization","Artificial intelligence; Behavioral research; Flow visualization; Interface states; Learning algorithms; Markov processes; Reinforcement learning; Sensors; High resolution display; Intention recognition; Inverse reinforcement learning; Learning (artificial intelligence); Markov Decision Processes; Movement behavior; Reward function; Visual Interface; Eye movements",Conference Paper,"Final",,Scopus,2-s2.0-84950969117
"Shimosaka M., Nishi K., Sato J., Kataoka H.","7003853628;56723495100;57212913027;36720714800;","Predicting driving behavior using inverse reinforcement learning with multiple reward functions towards environmental diversity",2015,"IEEE Intelligent Vehicles Symposium, Proceedings","2015-August",, 7225745,"567","572",,12,"10.1109/IVS.2015.7225745","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84951162976&doi=10.1109%2fIVS.2015.7225745&partnerID=40&md5=d5ce3dfdb0b08ee5582ee780f2efb21d","Department of Mechano-Informatics, University of Tokyo, Tokyo, Japan","Shimosaka, M., Department of Mechano-Informatics, University of Tokyo, Tokyo, Japan; Nishi, K., Department of Mechano-Informatics, University of Tokyo, Tokyo, Japan; Sato, J., Department of Mechano-Informatics, University of Tokyo, Tokyo, Japan; Kataoka, H., Department of Mechano-Informatics, University of Tokyo, Tokyo, Japan",,"Automobile drivers; Bayesian networks; Behavioral research; Forecasting; Intelligent vehicle highway systems; Inverse problems; Reinforcement learning; Vehicles; Dirichlet process mixture; Driving behavior; Driving environment; Environmental diversities; Inverse reinforcement learning; Long-term prediction; Poor performance; Professional drivers; Advanced driver assistance systems",Conference Paper,"Final",,Scopus,2-s2.0-84951162976
"Odom P., Natarajan S.","56050615500;57203254125;","Active advice seeking for inverse reinforcement learning",2015,"Proceedings of the National Conference on Artificial Intelligence","6",,,"4186","4187",,4,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84961233902&partnerID=40&md5=ec840c66e309c4b074a69463467208e8","School of Informatics and Computing, Indiana University Bloomington, United States","Odom, P., School of Informatics and Computing, Indiana University Bloomington, United States; Natarajan, S., School of Informatics and Computing, Indiana University Bloomington, United States",,"Artificial intelligence; Decision making; Demonstrations; Intelligent systems; Inverse problems; Active Learning; Human expert; Inverse reinforcement learning; Optimal decision making; Reinforcement learning",Conference Paper,"Final",,Scopus,2-s2.0-84961233902
"Ermon S., Xue Y., Toth R., Dilkina B., Bernstein R., Damoulas T., Clark P., DeGloria S., Mude A., Barrett C., Gomes C.P.","35791579200;55216614000;55940320300;23008031500;36805567300;24175970800;56745331400;6603727016;16203313100;7201943246;7101707114;","Learning large-scale dynamic discrete choice models of spatio-temporal preferences with application to migratory pastoralism in east Africa",2015,"Proceedings of the National Conference on Artificial Intelligence","1",,,"644","650",,9,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84959537621&partnerID=40&md5=f9bbe4f3af1c39b07e8368091aa4ca16","Stanford University, United States; Cornell University, United States; University of Sydney, Australia; Georgia Tech, United States; NYU CUSP, United States; USDA Research Service, United States; International Livestock Research Institute, United States","Ermon, S., Stanford University, United States; Xue, Y., Cornell University, United States; Toth, R., University of Sydney, Australia; Dilkina, B., Georgia Tech, United States; Bernstein, R., Cornell University, United States; Damoulas, T., NYU CUSP, United States; Clark, P., USDA Research Service, United States; DeGloria, S., Cornell University, United States; Mude, A., International Livestock Research Institute, United States; Barrett, C., Cornell University, United States; Gomes, C.P., Cornell University, United States",,"Artificial intelligence; Climate change; Decision making; Economics; Inverse problems; Learning algorithms; Learning systems; Reinforcement learning; Statistics; Arid and semi-arid regions; Discrete choice models; Inverse reinforcement learning; Large-scale dynamics; Machine learning literature; Movement trajectories; Probabilistic approaches; Resource availability; Climate models",Conference Paper,"Final",,Scopus,2-s2.0-84959537621
"Michini B., Walsh T.J., Agha-Mohammadi A.-A., How J.P.","36603096200;56388213200;54792649700;7006512768;","Bayesian Nonparametric Reward Learning From Demonstration",2015,"IEEE Transactions on Robotics","31","2", 7076638,"369","386",,19,"10.1109/TRO.2015.2405593","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027958292&doi=10.1109%2fTRO.2015.2405593&partnerID=40&md5=336016010dbf9ea167a354ded3a05581","Aerospace Controls Lab., Massachusetts Institute of Technology, CambridgeMA  02139, United States; Airware, San Francisco, CA  94103, United States; Kronos Incorporated, Chelmsford, MA  01824, United States; Laboratory for Information and Decision Systems, Massachusetts Institute of Technology, Cambridge, MA  02139, United States","Michini, B., Aerospace Controls Lab., Massachusetts Institute of Technology, CambridgeMA  02139, United States, Airware, San Francisco, CA  94103, United States; Walsh, T.J., Aerospace Controls Lab., Massachusetts Institute of Technology, CambridgeMA  02139, United States, Kronos Incorporated, Chelmsford, MA  01824, United States; Agha-Mohammadi, A.-A., Laboratory for Information and Decision Systems, Massachusetts Institute of Technology, Cambridge, MA  02139, United States; How, J.P., Aerospace Controls Lab., Massachusetts Institute of Technology, CambridgeMA  02139, United States","Demonstration; inverse reinforcement learning (IRL); reward learning","Demonstrations; Learning algorithms; Remote control; Attractive solutions; Computational advantages; Discrete state space; Gaussian Processes; Inverse reinforcement learning; Learning frameworks; Learning from demonstration; reward learning; Reinforcement learning",Article,"Final",,Scopus,2-s2.0-85027958292
"Choi J., Kim K.-E.","36835669400;15053383400;","Hierarchical Bayesian Inverse Reinforcement Learning",2015,"IEEE Transactions on Cybernetics","45","4", 6914557,"793","805",,8,"10.1109/TCYB.2014.2336867","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027940635&doi=10.1109%2fTCYB.2014.2336867&partnerID=40&md5=0b2c0e9c7ca0ab4ceaf28923618e95d2","Department of Computer Science, Korea Advanced Institute of Science and Technology, Daejeon, 305-701, South Korea","Choi, J., Department of Computer Science, Korea Advanced Institute of Science and Technology, Daejeon, 305-701, South Korea; Kim, K.-E., Department of Computer Science, Korea Advanced Institute of Science and Technology, Daejeon, 305-701, South Korea","Decision theory; inverse problems; maximum a posteriori estimation","Decision theory; Inverse problems; Taxicabs; Driving behavior; Hierarchical bayesian; Infinite numbers; Inverse reinforcement learning; Maximum a posteriori estimation; Noisy behavior; Reward function; Synthetic problem; Reinforcement learning",Article,"Final",,Scopus,2-s2.0-85027940635
"Vasquez D., Yu Y., Kumar S., Laugier C.","7004245418;56308172600;55567173000;7007031683;","An open framework for human-like autonomous driving using inverse reinforcement learning",2015,"2014 IEEE Vehicle Power and Propulsion Conference, VPPC 2014",,, 7007013,"","",,2,"10.1109/VPPC.2014.7007013","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84934312155&doi=10.1109%2fVPPC.2014.7007013&partnerID=40&md5=54fde521cbbd5f0d1de346da42faee30","Inria Rhne-Alpes, France; Beijing University, China; IIIT Hyderabad, India","Vasquez, D., Inria Rhne-Alpes, France; Yu, Y., Beijing University, China; Kumar, S., IIIT Hyderabad, India; Laugier, C., Inria Rhne-Alpes, France","Autonomous Driving; Experimental Frameworks; Inverse Reinforcement Learning","Reinforcement learning; Robotics; Autonomous driving; Autonomous Vehicles; Driving assistance systems; Driving simulator; Evaluation metrics; Experimental Frameworks; Inverse reinforcement learning; Reinforce learning; Advanced driver assistance systems",Conference Paper,"Final",,Scopus,2-s2.0-84934312155
"Bogert K., Doshi P.","36095803300;23008336000;","Multi-robot inverse reinforcement learning under occlusion with state transition estimation",2015,"Proceedings of the International Joint Conference on Autonomous Agents and Multiagent Systems, AAMAS","3",,,"1837","1838",,1,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84944679224&partnerID=40&md5=ad8ddedf7e8b772caf511e8b6827a9a0","THINC Lab, University of Georgia, Athens, GA  30602, United States","Bogert, K., THINC Lab, University of Georgia, Athens, GA  30602, United States; Doshi, P., THINC Lab, University of Georgia, Athens, GA  30602, United States","Inverse reinforcement; Machine learning; Multi-robot systems","Artificial intelligence; Autonomous agents; Industrial robots; Learning systems; Multi agent systems; Multipurpose robots; Probability; Robots; Stochastic systems; Inverse reinforcement learning; Multi-robot systems; Multiple robot; Robotic applications; State transitions; Stochastic transitions; Transition errors; Transition functions; Reinforcement learning",Conference Paper,"Final",,Scopus,2-s2.0-84944679224
"Nguyen Q.P., Low K.H., Jaillet P.","57189094420;7102180182;6701591623;","Inverse reinforcement learning with locally consistent reward functions",2015,"Advances in Neural Information Processing Systems","2015-January",,,"1747","1755",,8,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84965111532&partnerID=40&md5=cc7e0c181b86f6dbf7255fd4fb13c420","Dept. of Computer Science, National University of Singapore, Singapore, Singapore; Dept. of Electrical Engineering and Computer Science, Massachusetts Institute of Technology, United States","Nguyen, Q.P., Dept. of Computer Science, National University of Singapore, Singapore, Singapore; Low, K.H., Dept. of Computer Science, National University of Singapore, Singapore, Singapore; Jaillet, P., Dept. of Electrical Engineering and Computer Science, Massachusetts Institute of Technology, United States",,"Algorithms; Clustering algorithms; Information science; Inverse problems; Iterative methods; Maximum likelihood; Maximum principle; Stochastic models; Stochastic systems; Trajectories; Empirical evaluations; Expectation-maximization algorithms; Inverse reinforcement learning; Probabilistic graphical models; Real-world datasets; Reward function; State of the art; Stochastic transitions; Reinforcement learning",Conference Paper,"Final",,Scopus,2-s2.0-84965111532
"Herman M., Fischer V., Gindele T., Burgard W.","56742755300;56742874600;24476503400;7003610380;","Inverse reinforcement learning of behavioral models for online-adapting navigation strategies",2015,"Proceedings - IEEE International Conference on Robotics and Automation","2015-June","June", 7139642,"3215","3222",,6,"10.1109/ICRA.2015.7139642","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84938262677&doi=10.1109%2fICRA.2015.7139642&partnerID=40&md5=3a92ffad8fa4945aa14ca0071ba9fb3c","Robert Bosch GmbH, Corporate Sector Research and Advance Engineering, Stuttgart, D-70442, Germany; University of Freiburg, Department of Computer Science, Freiburg, D-79110, Germany","Herman, M., Robert Bosch GmbH, Corporate Sector Research and Advance Engineering, Stuttgart, D-70442, Germany; Fischer, V., Robert Bosch GmbH, Corporate Sector Research and Advance Engineering, Stuttgart, D-70442, Germany; Gindele, T., Robert Bosch GmbH, Corporate Sector Research and Advance Engineering, Stuttgart, D-70442, Germany; Burgard, W., University of Freiburg, Department of Computer Science, Freiburg, D-79110, Germany",,"Economic and social effects; Inverse problems; Reinforcement learning; Robot programming; Robotics; Robots; Stochastic models; Stochastic systems; Adaptive navigation; Autonomous systems; Human demonstrations; Inverse reinforcement learning; Mobile autonomous; Navigation strategies; Social acceptability; Stochastic behavior; Behavioral research",Conference Paper,"Final",,Scopus,2-s2.0-84938262677
"Bogert K., Doshi P.","36095803300;23008336000;","Toward estimating others' transition models under occlusion for multi-robot IRL",2015,"IJCAI International Joint Conference on Artificial Intelligence","2015-January",,,"1867","1873",,3,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84949803311&partnerID=40&md5=28f198a24905ddd4a1cba71d05fb776e","THINC Lab., Department of Computer Science, University of Georgia, Athens, GA  30602, United States","Bogert, K., THINC Lab., Department of Computer Science, University of Georgia, Athens, GA  30602, United States; Doshi, P., THINC Lab., Department of Computer Science, University of Georgia, Athens, GA  30602, United States",,"Artificial intelligence; Industrial robots; Inverse problems; Multipurpose robots; Nonlinear programming; Probability; Reinforcement learning; Stochastic systems; Inverse reinforcement learning; Non-linear optimization; Robotic applications; Stochastic transitions; Transition errors; Transition functions; Transition model; Under-constrained; Robots",Conference Paper,"Final",,Scopus,2-s2.0-84949803311
"Xie N., Zhao T., Tian F., Zhang X., Sugiyama M.","57203385327;53664758000;55186571600;57207320672;7402826969;","Stroke-based stylization learning and rendering with inverse reinforcement learning",2015,"IJCAI International Joint Conference on Artificial Intelligence","2015-January",,,"2531","2539",,3,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84949755630&partnerID=40&md5=e07d7bf827677784e159fc07839916fc","Tongji University, China; Tianjin University of Science and Technology, China; Bournemouth University, United Kingdom; Hiroshima Institute of Technology, Japan; University of Tokyo, Japan","Xie, N., Tongji University, China; Zhao, T., Tianjin University of Science and Technology, China; Tian, F., Bournemouth University, United Kingdom; Zhang, X., Hiroshima Institute of Technology, Japan; Sugiyama, M., University of Tokyo, Japan",,"Artificial intelligence; Computer graphics; Rendering (computer graphics); Brush stroke; Drawing styles; Inverse reinforcement learning; Non-Photorealistic Rendering; PhotoShop; Reinforcement learning",Conference Paper,"Final",,Scopus,2-s2.0-84949755630
"Audiffren J., Valko M., Lazaric A., Ghavamzadeh M.","55633243900;25642222500;14834304600;15845811100;","Maximum entropy semi-supervised inverse reinforcement learning",2015,"IJCAI International Joint Conference on Artificial Intelligence","2015-January",,,"3315","3321",,7,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84949769854&partnerID=40&md5=f92e813b5942b2aa21199588ba85dd8c","CMLA UMR 8536 ENS, Cachan, France; SequeL Team, INRIA, Lille, France; Adobe Research, INRIA, Lille, France","Audiffren, J., CMLA UMR 8536 ENS, Cachan, France; Valko, M., SequeL Team, INRIA, Lille, France; Lazaric, A., SequeL Team, INRIA, Lille, France; Ghavamzadeh, M., Adobe Research, INRIA, Lille, France",,"Algorithms; Artificial intelligence; Entropy; Inverse problems; Supervised learning; Trajectories; Apprenticeship learning; Inverse reinforcement learning; Maximum entropy principle; Novel algorithm; Semi- supervised learning; Semi-supervised; Unsupervised data; Reinforcement learning",Conference Paper,"Final",,Scopus,2-s2.0-84949769854
"Munzer T., Piot B., Geist M., Pietquin O., Lopes M.","57003152600;55697434100;25929145100;16040586900;8607501600;","Inverse reinforcement learning in relational domains",2015,"IJCAI International Joint Conference on Artificial Intelligence","2015-January",,,"3735","3741",,3,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84949786420&partnerID=40&md5=448741e80fba0f1657f2e44940ead6f7","INRIA, Bordeaux, France; University Lille 1, Lille, France; Supelec, Metz, France","Munzer, T., INRIA, Bordeaux, France; Piot, B., University Lille 1, Lille, France; Geist, M., Supelec, Metz, France; Pietquin, O., University Lille 1, Lille, France; Lopes, M., INRIA, Bordeaux, France",,"Artificial intelligence; Reinforcement learning; Changing dynamics; Compact representation; Experimental conditions; Generalization performance; Inverse reinforcement learning; Relational learning; Transfer properties; Inverse problems",Conference Paper,"Final",,Scopus,2-s2.0-84949786420
"Hahn J., Zoubir A.M.","36628372800;35584414100;","Inverse Reinforcement Learning using Expectation Maximization in mixture models",2015,"ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings","2015-August",, 7178666,"3721","3725",,2,"10.1109/ICASSP.2015.7178666","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84946058885&doi=10.1109%2fICASSP.2015.7178666&partnerID=40&md5=1d10ab999cab517309a1818e58baab59","Signal Processing Group, Technische Universität Darmstadt, Merckstraße 25, Darmstadt, 64283, Germany","Hahn, J., Signal Processing Group, Technische Universität Darmstadt, Merckstraße 25, Darmstadt, 64283, Germany; Zoubir, A.M., Signal Processing Group, Technische Universität Darmstadt, Merckstraße 25, Darmstadt, 64283, Germany","Expectation Maximization; Inverse Reinforcement Learning; Markov Decision Process",,Conference Paper,"Final",,Scopus,2-s2.0-84946058885
"Bloem M., Bambos N.","23570762400;7004840524;","Ground delay program analytics with behavioral cloning and inverse reinforcement learning",2015,"Journal of Aerospace Information Systems","12","3",,"299","313",,15,"10.2514/1.I010304","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84929144713&doi=10.2514%2f1.I010304&partnerID=40&md5=5fd387c1b7fda91b77b4c086e6f148c4","Systems Modeling and Optimization Branch, MS 210-15, NASA Ames Research Center, Moffett Field, CA  94035, United States; Departments of Management Science and Engineering and Electrical Engineering, Stanford University, Stanford, CA  94305, United States","Bloem, M., Systems Modeling and Optimization Branch, MS 210-15, NASA Ames Research Center, Moffett Field, CA  94035, United States; Bambos, N., Departments of Management Science and Engineering and Electrical Engineering, Stanford University, Stanford, CA  94305, United States",,"Air traffic control; Cloning; Decision making; Decision trees; Forecasting; Genetic engineering; Air traffics; Behavioral cloning; Ground delay programs; Historical data; Inverse reinforcement learning; Random forests; San Francisco International Airport; Reinforcement learning",Article,"Final",,Scopus,2-s2.0-84929144713
"Nikolaidis S., Ramakrishnan R., Gu K., Shah J.","55635433000;56895701000;56895699700;19640414800;","Efficient Model Learning from Joint-Action Demonstrations for Human-Robot Collaborative Tasks",2015,"ACM/IEEE International Conference on Human-Robot Interaction","2015-March",,,"189","196",,57,"10.1145/2696454.2696455","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84943534743&doi=10.1145%2f2696454.2696455&partnerID=40&md5=d9fc83d3488fe6c3abdb4b7bd0d3377b","Computer Science and Artificial Intelligence Laboratory, Massachusetts Institute of Technology, Cambridge, MA  02139, United States","Nikolaidis, S., Computer Science and Artificial Intelligence Laboratory, Massachusetts Institute of Technology, Cambridge, MA  02139, United States; Ramakrishnan, R., Computer Science and Artificial Intelligence Laboratory, Massachusetts Institute of Technology, Cambridge, MA  02139, United States; Gu, K., Computer Science and Artificial Intelligence Laboratory, Massachusetts Institute of Technology, Cambridge, MA  02139, United States; Shah, J., Computer Science and Artificial Intelligence Laboratory, Massachusetts Institute of Technology, Cambridge, MA  02139, United States","human-robot collaboration; mixed observability markov decision process; model learning","Human computer interaction; Learning algorithms; Man machine systems; Markov processes; Observability; Reinforcement learning; Robots; Collaborative tasks; Human subject experiments; Human-robot collaboration; Inverse reinforcement learning; Markov Decision Processes; Mixed observabilities; Model learning; Observable variables; Human robot interaction",Conference Paper,"Final",,Scopus,2-s2.0-84943534743
"Bloem M., Bambos N.","23570762400;7004840524;","Ground delay program analytics with behavioral cloning and inverse reinforcement learning",2015,"AGIFORS 55th Annual Symposium: Analytics for Efficiency and Customer Centric Optimization",,,,"","",,,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84979088384&partnerID=40&md5=6608dd2266190fa16fb0833a8abf9ad3","Department of Management Science and Engineering, Stanford University, Stanford, CA  94305, United States; Department of Management Science and Engineering and Electrical Engineering, Stanford University, Stanford, CA  94305, United States","Bloem, M., Department of Management Science and Engineering, Stanford University, Stanford, CA  94305, United States; Bambos, N., Department of Management Science and Engineering and Electrical Engineering, Stanford University, Stanford, CA  94305, United States",,"Air traffic control; Cloning; Decision making; Decision trees; Efficiency; Forecasting; Genetic engineering; Air traffics; Behavioral cloning; Gain insight; Ground delay programs; Historical data; Inverse reinforcement learning; Random forests; San Francisco International Airport; Reinforcement learning",Conference Paper,"Final",,Scopus,2-s2.0-84979088384
"MacGlashan J., Littman M.L.","21934008200;7006510438;","Between imitation and intention learning",2015,"IJCAI International Joint Conference on Artificial Intelligence","2015-January",,,"3692","3698",,8,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84949895077&partnerID=40&md5=e17c6fd043afd75f0cfb4e85908eabc5","Brown University, United States","MacGlashan, J., Brown University, United States; Littman, M.L., Brown University, United States",,"Artificial intelligence; Costs; Reinforcement learning; Supervised learning; Computational costs; Generalization performance; Imitation learning; Inverse reinforcement learning; Learning from demonstration; Learning paradigms; Planning algorithms; Planning horizons; Learning algorithms",Conference Paper,"Final",,Scopus,2-s2.0-84949895077
"Sezener C.E.","56728606400;","Inferring human values for safe AGI design",2015,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","9205",,,"152","155",,3,"10.1007/978-3-319-21365-1_16","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84952760405&doi=10.1007%2f978-3-319-21365-1_16&partnerID=40&md5=e6afb198afb83c70b196a149178212a0","Department of Computer Science, Ozyegin University, Istanbul, Turkey","Sezener, C.E., Department of Computer Science, Ozyegin University, Istanbul, Turkey","Friendly AI; Inverse reinforcement learning; Safe AGI; Value learning","Reinforcement learning; Human values; Inverse reinforcement learning; Safe AGI; Value learning; Behavioral research",Conference Paper,"Final",,Scopus,2-s2.0-84952760405
[No author name available],[No author id available],"AGIFORS 55th Annual Symposium: Analytics for Efficiency and Customer Centric Optimization",2015,"AGIFORS 55th Annual Symposium: Analytics for Efficiency and Customer Centric Optimization",,,,"","",715,,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84979085483&partnerID=40&md5=cfcd9832f101fa8d1b5c22870ff75aad",,"",,,Conference Review,"Final",,Scopus,2-s2.0-84979085483
"Yang S.Y., Qiao Q., Beling P.A., Scherer W.T., Kirilenko A.A.","24802841600;35848809400;6603732790;7102162666;7005937861;","Gaussian process-based algorithmic trading strategy identification",2015,"Quantitative Finance","15","10",,"1683","1703",,11,"10.1080/14697688.2015.1011684","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84941803646&doi=10.1080%2f14697688.2015.1011684&partnerID=40&md5=5508d8be850793319ecd7524238c4336","Financial Engineering Program, Stevens Institute of Technology, 1 Castle Point on Hudson, Hoboken, NJ  03070, United States; Department of Systems and Information Engineering, The University of Virginia, Charlottesville, VA  22904, United States; MIT Sloan School of Management, 50 Memorial Drive, Cambridge, MA  02142, United States","Yang, S.Y., Financial Engineering Program, Stevens Institute of Technology, 1 Castle Point on Hudson, Hoboken, NJ  03070, United States; Qiao, Q., Department of Systems and Information Engineering, The University of Virginia, Charlottesville, VA  22904, United States; Beling, P.A., Department of Systems and Information Engineering, The University of Virginia, Charlottesville, VA  22904, United States; Scherer, W.T., Department of Systems and Information Engineering, The University of Virginia, Charlottesville, VA  22904, United States; Kirilenko, A.A., Department of Systems and Information Engineering, The University of Virginia, Charlottesville, VA  22904, United States, MIT Sloan School of Management, 50 Memorial Drive, Cambridge, MA  02142, United States","Algorithmic trading; Behavioural finance; Gaussian process; High-frequency trading; Inverse reinforcement learning; Markov decision process; Support vector machine",,Article,"Final",Open Access,Scopus,2-s2.0-84941803646
"Li D.C., He Y.Q., Fu F.","56650286700;23060003200;56650564700;","Nonlinear inverse reinforcement learning with mutual information and Gaussian process",2014,"2014 IEEE International Conference on Robotics and Biomimetics, IEEE ROBIO 2014",,, 7090537,"1445","1450",,,"10.1109/ROBIO.2014.7090537","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84949928482&doi=10.1109%2fROBIO.2014.7090537&partnerID=40&md5=2e92cbadd0125b09ea9dda575eaf9d96","Shenyang Institute of Automation, Chinese Academy of Sciences, Shenyang, China","Li, D.C., Shenyang Institute of Automation, Chinese Academy of Sciences, Shenyang, China; He, Y.Q., Shenyang Institute of Automation, Chinese Academy of Sciences, Shenyang, China; Fu, F., Shenyang Institute of Automation, Chinese Academy of Sciences, Shenyang, China",,"Algorithms; Biomimetics; Gaussian distribution; Gaussian noise (electronic); Knowledge acquisition; Learning algorithms; Learning systems; Adaptive modeling; Automatic relevance determination; Extreme learning machine; Gaussian Processes; Generalization capability; Inverse reinforcement learning; Mutual informations; Optimal subsets; Reinforcement learning",Conference Paper,"Final",,Scopus,2-s2.0-84949928482
"Surana A., Srivastava K.","6602087409;57213897847;","Bayesian nonparametric inverse reinforcement learning for switched markov decision processes",2014,"Proceedings - 2014 13th International Conference on Machine Learning and Applications, ICMLA 2014",,, 7033090,"47","54",,9,"10.1109/ICMLA.2014.105","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84946685279&doi=10.1109%2fICMLA.2014.105&partnerID=40&md5=d0d708d4e61f8253d2f6ad5efaebc9c2","United Technologies Research Center, East Hartford, CT  06118, United States","Surana, A., United Technologies Research Center, East Hartford, CT  06118, United States; Srivastava, K., United Technologies Research Center, East Hartford, CT  06118, United States","Bayesian Nonparametrics; Inverse Reinforcement Learning; Markov Decision Processes","Artificial intelligence; Dynamical systems; Learning algorithms; Learning systems; Linear control systems; Markov processes; Monte Carlo methods; Bayesian nonparametrics; Complex agent; Inverse reinforcement learning; Linear dynamical systems; Markov chain Monte Carlo method; Markov Decision Processes; Non-parametric; Sticky hierarchical dirichlet process; Reinforcement learning",Conference Paper,"Final",,Scopus,2-s2.0-84946685279
"Kim D., Breslin C., Tsiakoulis P., Gašić M., Henderson M., Young S.","56304109400;18041542300;14018573600;26631804000;56360767500;7404515229;","Inverse reinforcement learning for micro-turn management",2014,"Proceedings of the Annual Conference of the International Speech Communication Association, INTERSPEECH",,,,"328","332",,9,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84910078105&partnerID=40&md5=1c54ea57e3b7cc9fb30094cd269b7781","Department of Engineering, University of Cambridge, Cambridge, United Kingdom","Kim, D., Department of Engineering, University of Cambridge, Cambridge, United Kingdom; Breslin, C., Department of Engineering, University of Cambridge, Cambridge, United Kingdom; Tsiakoulis, P., Department of Engineering, University of Cambridge, Cambridge, United Kingdom; Gašić, M., Department of Engineering, University of Cambridge, Cambridge, United Kingdom; Henderson, M., Department of Engineering, University of Cambridge, Cambridge, United Kingdom; Young, S., Department of Engineering, University of Cambridge, Cambridge, United Kingdom","Dialogue management; Inverse reinforcement learning; Markov decision processes; Spoken dialogue systems","Behavioral research; Markov processes; Speech communication; Speech processing; Decision-theoretic; Dialogue management; Human-human interactions; Inverse reinforcement learning; Markov Decision Processes; Natural interactions; Reward function; Spoken dialogue system; Reinforcement learning",Conference Paper,"Final",,Scopus,2-s2.0-84910078105
"Rojas-Barahona L.M., Cerisara C.","26653813700;6506781029;","Bayesian inverse reinforcement learning for modeling conversational agents in a virtual environment",2014,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","8403 LNCS","PART 1",,"503","514",,1,"10.1007/978-3-642-54906-9_41","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84958522870&doi=10.1007%2f978-3-642-54906-9_41&partnerID=40&md5=c7a815b00d10fae40ceb1ae2acfb9721","Université de Lorraine/LORIA, Nancy, France; CNRS/LORIA, Nancy, France","Rojas-Barahona, L.M., Université de Lorraine/LORIA, Nancy, France; Cerisara, C., CNRS/LORIA, Nancy, France",,"Bayesian networks; Behavioral research; Computational linguistics; Text processing; Virtual reality; Baseline systems; Bayesian; Bayesian approaches; Conversational agents; Dialogue manager; Inverse reinforcement learning; Optimal decisions; Serious games; Reinforcement learning",Conference Paper,"Final",,Scopus,2-s2.0-84958522870
"Vasquez D., Okal B., Arras K.O.","7004245418;56132081300;6701603183;","Inverse Reinforcement Learning algorithms and features for robot navigation in crowds: An experimental comparison",2014,"IEEE International Conference on Intelligent Robots and Systems",,, 6942731,"1341","1346",,49,"10.1109/IROS.2014.6942731","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84911478644&doi=10.1109%2fIROS.2014.6942731&partnerID=40&md5=3ace6f55116d230adaca305484d9b973","Social Robotics Lab, INRIA Grenoble, France; Social Robotics Lab, University of Freiburg, Germany","Vasquez, D., Social Robotics Lab, INRIA Grenoble, France; Okal, B., Social Robotics Lab, University of Freiburg, Germany; Arras, K.O., Social Robotics Lab, University of Freiburg, Germany",,"Algorithms; Computer programming; Human robot interaction; Intelligent robots; Learning algorithms; Reinforcement learning; Experimental comparison; Inverse reinforcement learning; Large scale simulations; Robot navigation; Social interactions; Software frameworks; Subjective performance; Systematic evaluation; Robots",Conference Paper,"Final",,Scopus,2-s2.0-84911478644
"Pérez-Higueras N., Ramón-Vigo R., Caballero F., Merino L.","56414879200;56414842900;8956620100;7003946345;","Robot local navigation with learned social cost functions",2014,"ICINCO 2014 - Proceedings of the 11th International Conference on Informatics in Control, Automation and Robotics","2",,,"618","625",,6,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84910024548&partnerID=40&md5=20f5ae5cc8e1d4a21934265b84306782","School of Engineering, Pablo de Olavide University, Crta. Utrera km 1, Seville, Spain; Dpt. of System Engineering and Automation, University of Seville, Camino de los Descubrimientos s/n, Seville, Spain","Pérez-Higueras, N., School of Engineering, Pablo de Olavide University, Crta. Utrera km 1, Seville, Spain; Ramón-Vigo, R., School of Engineering, Pablo de Olavide University, Crta. Utrera km 1, Seville, Spain; Caballero, F., Dpt. of System Engineering and Automation, University of Seville, Camino de los Descubrimientos s/n, Seville, Spain; Merino, L., School of Engineering, Pablo de Olavide University, Crta. Utrera km 1, Seville, Spain","Inverse Reinforcement Learning; Learning from Demonstrations; Robot Navigation; Social Robots","Cost functions; Costs; Human robot interaction; Navigation systems; Reinforcement learning; Robotics; Telecommunication networks; Human interactions; Human social interactions; Inverse reinforcement learning; Learning from demonstration; Robot acceptances; Robot navigation; Robot navigation system; Social robots; Robots",Conference Paper,"Final",,Scopus,2-s2.0-84910024548
"Bloem M., Bambos N.","23570762400;7004840524;","Infinite time horizon maximum causal entropy inverse reinforcement learning",2014,"Proceedings of the IEEE Conference on Decision and Control","2015-February","February", 7040156,"4911","4916",,17,"10.1109/CDC.2014.7040156","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988268663&doi=10.1109%2fCDC.2014.7040156&partnerID=40&md5=b1d4cd7ceff3278c960c86b7c0da39f5","Aviation Systems Division, NASA Ames Research Center, Moffett Field, CA  94035, United States; Departments of Electrical Engineering and Management Science and Engineering, Stanford University, Stanford, CA  94305, United States","Bloem, M., Aviation Systems Division, NASA Ames Research Center, Moffett Field, CA  94035, United States; Bambos, N., Departments of Electrical Engineering and Management Science and Engineering, Stanford University, Stanford, CA  94305, United States",,,Conference Paper,"Final",,Scopus,2-s2.0-84988268663
"Surana A.","6602087409;","Unsupervised inverse reinforcement learning with noisy data",2014,"Proceedings of the IEEE Conference on Decision and Control","2015-February","February", 7040160,"4938","4945",,2,"10.1109/CDC.2014.7040160","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988293079&doi=10.1109%2fCDC.2014.7040160&partnerID=40&md5=56a8361833b3406d40fff74afbe63e29","United Technologies Research Center, 411 Silver Lane, East Hartford, CT  06108, United States","Surana, A., United Technologies Research Center, 411 Silver Lane, East Hartford, CT  06108, United States",,,Conference Paper,"Final",,Scopus,2-s2.0-84988293079
"Shimosaka M., Kaneko T., Nishi K.","7003853628;55421217800;56723495100;","Modeling risk anticipation and defensive driving on residential roads with inverse reinforcement learning",2014,"2014 17th IEEE International Conference on Intelligent Transportation Systems, ITSC 2014",,, 6957937,"1694","1700",,13,"10.1109/ITSC.2014.6957937","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84937150950&doi=10.1109%2fITSC.2014.6957937&partnerID=40&md5=8a4c58b7b574c3618864ffdadec91a53","Department of Mechano-Informatics, Graduate School of Information Science and Technology, University of Tokyo, Tokyo, 113-8656, Japan","Shimosaka, M., Department of Mechano-Informatics, Graduate School of Information Science and Technology, University of Tokyo, Tokyo, 113-8656, Japan; Kaneko, T., Department of Mechano-Informatics, Graduate School of Information Science and Technology, University of Tokyo, Tokyo, 113-8656, Japan; Nishi, K., Department of Mechano-Informatics, Graduate School of Information Science and Technology, University of Tokyo, Tokyo, 113-8656, Japan",,"Accidents; Active safety systems; Behavioral research; Hidden Markov models; Housing; Intelligent systems; Inverse problems; Learning algorithms; Markov processes; Motion planning; Safety engineering; Transportation; Feature descriptors; Inverse reinforcement learning; Long-term prediction; Markov Decision Processes; Maximum entropy Markov model; Pedestrian detection; Physical limitations; Reducing traffic accidents; Reinforcement learning",Conference Paper,"Final",,Scopus,2-s2.0-84937150950
"Chinaei H., Chaib-draa B.","23388191000;7004238886;","Dialogue POMDP components (Part II): learning the reward function",2014,"International Journal of Speech Technology","17","4",,"325","340",,1,"10.1007/s10772-014-9224-x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84911971999&doi=10.1007%2fs10772-014-9224-x&partnerID=40&md5=66a204a6a06477a304ca8bdc1ba89095","Department of Computer Science, Laval University, Québec, Canada","Chinaei, H., Department of Computer Science, Laval University, Québec, Canada; Chaib-draa, B., Department of Computer Science, Laval University, Québec, Canada","Healthcare dialogue management; Inverse reinforcement learning; Partially observable Markov decision processes (POMDP)","Approximation algorithms; Behavioral research; Health care; Learning algorithms; Markov processes; Speech processing; Dialogue management; Dialogue systems; Intelligent wheelchair; Inverse reinforcement learning; Linear approximations; Markov Decision Processes; Partially observable Markov decision process; Transition model; Reinforcement learning",Article,"Final",,Scopus,2-s2.0-84911971999
"Pynadath D.V., Rosenbloom P.S., Marsella S.C.","6602713215;6603896745;6603739353;","Reinforcement learning for adaptive theory of mind in the sigma cognitive architecture",2014,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","8598 LNAI",,,"143","154",,6,"10.1007/978-3-319-09274-4_14","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84905844071&doi=10.1007%2f978-3-319-09274-4_14&partnerID=40&md5=f18f885d0e0a4a9568a8dbe910f00cc7","Institute for Creative Technologies, University of Southern California, Los Angeles, CA, United States; Department of Computer Science, University of Southern California, Los Angeles, CA, United States; Northeastern University, Boston, MA, United States","Pynadath, D.V., Institute for Creative Technologies, University of Southern California, Los Angeles, CA, United States; Rosenbloom, P.S., Institute for Creative Technologies, University of Southern California, Los Angeles, CA, United States, Department of Computer Science, University of Southern California, Los Angeles, CA, United States; Marsella, S.C., Northeastern University, Boston, MA, United States",,"Behavioral research; Cognitive systems; Knowledge management; Multi agent systems; Cognitive architectures; Explicit modeling; Gradient descent algorithms; Human intelligence; Inverse reinforcement learning; Multi-agent learning; Multi-agent reinforcement learning; Social interactions; Reinforcement learning",Conference Paper,"Final",,Scopus,2-s2.0-84905844071
"Bloem M., Bambos N.","23570762400;7004840524;","Ground delay program analytics with behavioral cloning and inverse reinforcement learning",2014,"AIAA AVIATION 2014 -14th AIAA Aviation Technology, Integration, and Operations Conference",,,,"","",,8,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84907388882&partnerID=40&md5=a2a848590094da70dea62ea56ce227dc","NASA Ames Research Center, Systems Modeling and Optimization Branch, MS 210-15, Moffett Field, CA, 94035, United States; Stanford University, Stanford, CA, 94305, United States","Bloem, M., NASA Ames Research Center, Systems Modeling and Optimization Branch, MS 210-15, Moffett Field, CA, 94035, United States; Bambos, N., Stanford University, Stanford, CA, 94305, United States",,"Air traffic control; Aviation; Cloning; Decision trees; Forecasting; Air traffics; Behavioral cloning; Gain insight; Ground delay programs; Historical data; Inverse reinforcement learning; Random forests; San Francisco International Airport; Reinforcement learning",Conference Paper,"Final",,Scopus,2-s2.0-84907388882
"Jetchev N., Toussaint M.","14034161200;7006246144;","Discovering relevant task spaces using inverse feedback control",2014,"Autonomous Robots","37","2",,"169","189",,4,"10.1007/s10514-014-9384-1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84902287582&doi=10.1007%2fs10514-014-9384-1&partnerID=40&md5=acaa3d6a9248ed2d398b72262d330c52","FU Berlin, Berlin, Germany; University of Stuttgart, Stuttgart, Germany","Jetchev, N., FU Berlin, Berlin, Germany; Toussaint, M., University of Stuttgart, Stuttgart, Germany","Imitation learning; Inverse reinforcement learning; Machine learning and robotics; Motion rate control; Robot motion generation; Task spaces for motion","Artificial intelligence; Reinforcement learning; Robotics; Imitation learning; Inverse reinforcement learning; Motion rate control; Robot motion generations; Task space; Feedback control",Article,"Final",,Scopus,2-s2.0-84902287582
"Calinon S., Bruno D., Malekzadeh M.S., Nanayakkara T., Caldwell D.G.","6507248533;56018693500;56019474800;6508336274;7202685497;","Human-robot skills transfer interfaces for a flexible surgical robot",2014,"Computer Methods and Programs in Biomedicine","116","2",,"81","96",,25,"10.1016/j.cmpb.2013.12.015","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84903192398&doi=10.1016%2fj.cmpb.2013.12.015&partnerID=40&md5=928d826f81dc853819cf4a14e1c201b9","Department of Advanced Robotics, Istituto Italiano di Tecnologia (IIT), Via Morego 30, 16163 Genova, Italy; Department of Informatics, King's College London, Strand, London, WC2R 2LS, United Kingdom","Calinon, S., Department of Advanced Robotics, Istituto Italiano di Tecnologia (IIT), Via Morego 30, 16163 Genova, Italy; Bruno, D., Department of Advanced Robotics, Istituto Italiano di Tecnologia (IIT), Via Morego 30, 16163 Genova, Italy; Malekzadeh, M.S., Department of Advanced Robotics, Istituto Italiano di Tecnologia (IIT), Via Morego 30, 16163 Genova, Italy; Nanayakkara, T., Department of Informatics, King's College London, Strand, London, WC2R 2LS, United Kingdom; Caldwell, D.G., Department of Advanced Robotics, Istituto Italiano di Tecnologia (IIT), Via Morego 30, 16163 Genova, Italy","Inverse reinforcement learning; Learning from demonstration; Robot-assisted surgery; Skills transfer; Soft robotics; Stochastic optimization","Demonstrations; Optimization; Reinforcement learning; Robots; Surgery; Inverse reinforcement learning; Learning from demonstration; Robot-assisted surgery; Skills transfer; Soft robotics; Stochastic optimizations; Surgical equipment; article; human; kinematics; laparoscopic surgery; reinforcement; reward; robot assisted surgery; robotics; simulation; skill; surgical equipment; surgical technique; algorithm; artificial intelligence; biomechanics; computer interface; computer simulation; devices; image quality; motor performance; robotic surgical procedure; robotics; task performance; Algorithms; Artificial Intelligence; Biomechanical Phenomena; Computer Simulation; Humans; Motor Skills; Phantoms, Imaging; Robotic Surgical Procedures; Robotics; Task Performance and Analysis; User-Computer Interface",Article,"Final",,Scopus,2-s2.0-84903192398
"Zheng J., Liu S., Ni L.M.","55389084300;35339658000;7102681502;","Robust bayesian inverse reinforcement learning with sparse behavior noise",2014,"Proceedings of the National Conference on Artificial Intelligence","3",,,"2198","2205",,17,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84908200589&partnerID=40&md5=35a4ca76506f93fb74f3b4190b68e5b7","Department of Computer Science and Engineering, Hong Kong University of Science and Technology, Hong Kong, Hong Kong; Guangzhou HKUST Fok Ying Tung Research Institute, Hong Kong University of Science and Technology, Hong Kong, Hong Kong; Heinz College, Carnegie Mellon University, Pittsburgh, United States","Zheng, J., Department of Computer Science and Engineering, Hong Kong University of Science and Technology, Hong Kong, Hong Kong; Liu, S., Heinz College, Carnegie Mellon University, Pittsburgh, United States; Ni, L.M., Department of Computer Science and Engineering, Hong Kong University of Science and Technology, Hong Kong, Hong Kong, Guangzhou HKUST Fok Ying Tung Research Institute, Hong Kong University of Science and Technology, Hong Kong, Hong Kong",,"Bayesian; Inverse reinforcement learning",Conference Paper,"Final",,Scopus,2-s2.0-84908200589
"Oswald S., Kretzschmar H., Burgard W., Stachniss C.","56640890200;34168016900;7003610380;6507517732;","Learning to give route directions from human demonstrations",2014,"Proceedings - IEEE International Conference on Robotics and Automation",,, 6907334,"3303","3308",,6,"10.1109/ICRA.2014.6907334","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84929223719&doi=10.1109%2fICRA.2014.6907334&partnerID=40&md5=57f88279f8012446c263a065049955e6","Department of Computer Science, University of Freiburg, Freiburg, 79110, Germany; University of Bonn, Institute of Geodesy and Geoinformation, Bonn, 53115, Germany","Oswald, S., Department of Computer Science, University of Freiburg, Freiburg, 79110, Germany; Kretzschmar, H., Department of Computer Science, University of Freiburg, Freiburg, 79110, Germany; Burgard, W., Department of Computer Science, University of Freiburg, Freiburg, 79110, Germany; Stachniss, C., Department of Computer Science, University of Freiburg, Freiburg, 79110, Germany, University of Bonn, Institute of Geodesy and Geoinformation, Bonn, 53115, Germany",,,Conference Paper,"Final",,Scopus,2-s2.0-84929223719
"Das S., Lavoie A.","55476999400;55295620500;","The effects of feedback on human behavior in social media: An inverse reinforcement learning model",2014,"13th International Conference on Autonomous Agents and Multiagent Systems, AAMAS 2014","1",,,"653","660",,9,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84911426890&partnerID=40&md5=9410899641a02cca87887f0e3b436adc","Washington University, St. Louis, United States","Das, S., Washington University, St. Louis, United States; Lavoie, A., Washington University, St. Louis, United States","Multi-agent learning; Social media; Social simulation",,Conference Paper,"Final",,Scopus,2-s2.0-84911426890
"Bogert K., Doshi P.","36095803300;23008336000;","Multi-robot inverse reinforcement learning under occlusion with interactions",2014,"13th International Conference on Autonomous Agents and Multiagent Systems, AAMAS 2014","1",,,"173","180",,8,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84911432424&partnerID=40&md5=c339192e4d608b2bd420cedecf1eb5ca","Computer Science Department, University of Georgia, United States","Bogert, K., Computer Science Department, University of Georgia, United States; Doshi, P., Computer Science Department, University of Georgia, United States","Inverse reinforcement; Machine learning; Multi-robot systems; Patrolling",,Conference Paper,"Final",,Scopus,2-s2.0-84911432424
"Ondrúška P., Posner I.","56308174300;23010022000;","The route not taken: Driver-centric estimation of electric vehicle range",2014,"Proceedings International Conference on Automated Planning and Scheduling, ICAPS","2014-January","January",,"413","420",,17,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84933050809&partnerID=40&md5=46b4fa6e96f048605ef4708bb7d219f6","Mobile Robotics Group, University of Oxford, United Kingdom","Ondrúška, P., Mobile Robotics Group, University of Oxford, United Kingdom; Posner, I., Mobile Robotics Group, University of Oxford, United Kingdom",,"Behavioral research; Decision making; Electric vehicles; Energy utilization; Forecasting; Markov processes; Reinforcement learning; Scheduling; Energy prediction; Inverse reinforcement learning; Markov Decision Processes; Policy evaluation; Real trajectories; Relative errors; Route preferences; Sequential decision making; Energy policy",Conference Paper,"Final",,Scopus,2-s2.0-84933050809
"Yang S.Y., Qiao Q., Beling P.A., Scherer W.T.","24802841600;35848809400;6603732790;7102162666;","Algorithmic trading behavior identification using reward learning method",2014,"Proceedings of the International Joint Conference on Neural Networks",,, 6889878,"3807","3814",,,"10.1109/IJCNN.2014.6889878","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84908472703&doi=10.1109%2fIJCNN.2014.6889878&partnerID=40&md5=ddc139e54cc186fd1884a40ed5b9c9f1","Financial Engineering Program, School of Systems and Enterprises, Stevens Institute of Technology, United States; Department of Systems and Information Engineering, University of Virginia, United States","Yang, S.Y., Financial Engineering Program, School of Systems and Enterprises, Stevens Institute of Technology, United States; Qiao, Q., Department of Systems and Information Engineering, University of Virginia, United States; Beling, P.A., Department of Systems and Information Engineering, University of Virginia, United States; Scherer, W.T., Department of Systems and Information Engineering, University of Virginia, United States","Algorithmic Trading; Behavioral Finance; Gaussian Process; High Frequency Trading; Inverse Reinforcement Learning; Markov Decision Process; Support Vector Machine","Algorithms; Correlation theory; Electronic data interchange; Financial markets; Gaussian distribution; Gaussian noise (electronic); Markov processes; Reinforcement learning; Support vector machines; Virtual reality; Algorithmic trading; Behavioral finance; Gaussian Processes; High-frequency trading; Inverse reinforcement learning; Markov Decision Processes; Commerce",Conference Paper,"Final",,Scopus,2-s2.0-84908472703
"Piot B., Geist M., Pietquin O.","55697434100;25929145100;16040586900;","Boosted and reward-regularized classification for apprenticeship learning",2014,"13th International Conference on Autonomous Agents and Multiagent Systems, AAMAS 2014","2",,,"1249","1256",,6,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84911437588&partnerID=40&md5=8afbd7ea74fd484a395c5ef67f6268b7","Supelec, MaLIS Research Group, France; Georgia Tech., CNRS UMI 2958, France; University Lille 1, France; LIFL (UMR 8022 CNRS/Lille 1), SequeL Team, France","Piot, B., Supelec, MaLIS Research Group, France, Georgia Tech., CNRS UMI 2958, France; Geist, M., Supelec, MaLIS Research Group, France, Georgia Tech., CNRS UMI 2958, France; Pietquin, O., University Lille 1, France, LIFL (UMR 8022 CNRS/Lille 1), SequeL Team, France","Boosting; Inverse reinforcement learning; Large margin methods; Learning from demonstrations",,Conference Paper,"Final",,Scopus,2-s2.0-84911437588
"Grizou J., Lopes M., Oudeyer P.-Y.","55608034900;8607501600;6507418132;","Robot learning simultaneously a task and how to interpret human instructions",2013,"2013 IEEE 3rd Joint International Conference on Development and Learning and Epigenetic Robotics, ICDL 2013 - Electronic Conference Proceedings",,, 6652523,"","",,17,"10.1109/DevLrn.2013.6652523","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84891137725&doi=10.1109%2fDevLrn.2013.6652523&partnerID=40&md5=58e3f595978091d74ba40fcb3e2ec264","Flowers Team, INRIA / ENSTA-Paristech, France","Grizou, J., Flowers Team, INRIA / ENSTA-Paristech, France; Lopes, M., Flowers Team, INRIA / ENSTA-Paristech, France; Oudeyer, P.-Y., Flowers Team, INRIA / ENSTA-Paristech, France",,"Computational results; Human teachers; Inverse reinforcement learning; Learning efficiency; Shared understanding; Spoken words; Reinforcement learning; Robotics; Teaching; Robots",Conference Paper,"Final",,Scopus,2-s2.0-84891137725
"Osogami T., Raymond R.","10042517800;23393816800;","Map matching with inverse reinforcement learning",2013,"IJCAI International Joint Conference on Artificial Intelligence",,,,"2547","2553",,21,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84896061924&partnerID=40&md5=530c6fc9da49dc759f450d2de9582ebb","IBM Research-Tokyo, 5-6-52 Toyosu, Koto-ku, Tokyo, Japan","Osogami, T., IBM Research-Tokyo, 5-6-52 Toyosu, Koto-ku, Tokyo, Japan; Raymond, R., IBM Research-Tokyo, 5-6-52 Toyosu, Koto-ku, Tokyo, Japan",,"Inverse reinforcement learning; Map matching; Numerical experiments; Road segments; State-of-the-art approach; Transition probabilities; Travel distance; Artificial intelligence; Hidden Markov models; Reinforcement learning; Roads and streets; Probability",Conference Paper,"Final",,Scopus,2-s2.0-84896061924
"Choi J., Kim K.-E.","36835669400;15053383400;","Bayesian nonparametric feature construction for inverse reinforcement learning",2013,"IJCAI International Joint Conference on Artificial Intelligence",,,,"1287","1293",,12,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84896063989&partnerID=40&md5=ae70c28bd80d490190627052bcf9f409","Department of Computer Science, Korea Advanced Institute of Science and Technology, Daejeon 305-701, South Korea","Choi, J., Department of Computer Science, Korea Advanced Institute of Science and Technology, Daejeon 305-701, South Korea; Kim, K.-E., Department of Computer Science, Korea Advanced Institute of Science and Technology, Daejeon 305-701, South Korea",,"Composite features; Feature construction; Inverse reinforcement learning; Linear functions; Non-parametric; Nonparametric approaches; Reward function; Taxi drivers; Artificial intelligence; Reinforcement learning",Conference Paper,"Final",,Scopus,2-s2.0-84896063989
"Tossou A.C.Y., Dimitrakakis C.","55936518300;55886345900;","Probabilistic inverse reinforcement learning in unknown environments",2013,"Uncertainty in Artificial Intelligence - Proceedings of the 29th Conference, UAI 2013",,,,"635","643",,2,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84888166253&partnerID=40&md5=3e86bcb775de98bd76f2ca7ca5f1fecd","EPAC, Abomey-Calavi, Benin; EPFL, Lausanne, Switzerland","Tossou, A.C.Y., EPAC, Abomey-Calavi, Benin; Dimitrakakis, C., EPFL, Lausanne, Switzerland",,"Agent preferences; Bayesian inference; Convex optimisation; Inverse reinforcement learning; Learning by demonstration; Maximum a posteriori estimation; Probabilistic approaches; Probabilistic models; Artificial intelligence; Bayesian networks; Inference engines; Reinforcement learning",Conference Paper,"Final",,Scopus,2-s2.0-84888166253
"Michini B., Cutler M., How J.P.","36603096200;55540436700;7006512768;","Scalable reward learning from demonstration",2013,"Proceedings - IEEE International Conference on Robotics and Automation",,, 6630592,"303","308",,11,"10.1109/ICRA.2013.6630592","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84887309045&doi=10.1109%2fICRA.2013.6630592&partnerID=40&md5=11485c155db550b1d94b7d3010f9593c","Aerospace Controls Laboratory, Massachusetts Institute of Technology, United States","Michini, B., Aerospace Controls Laboratory, Massachusetts Institute of Technology, United States; Cutler, M., Aerospace Controls Laboratory, Massachusetts Institute of Technology, United States; How, J.P., Aerospace Controls Laboratory, Massachusetts Institute of Technology, United States",,"Action spaces; Discretizations; Inverse reinforcement learning; Key modifications; Learning from demonstration; Markov Decision Processes; Non-parametric; Optimal value functions; Learning algorithms; Markov processes; Optimal systems; Reinforcement learning; Robotics",Conference Paper,"Final",,Scopus,2-s2.0-84887309045
"Kalakrishnan M., Pastor P., Righetti L., Schaal S.","24467757900;57197413733;22981433900;7007171558;","Learning objective functions for manipulation",2013,"Proceedings - IEEE International Conference on Robotics and Automation",,, 6630743,"1331","1336",,45,"10.1109/ICRA.2013.6630743","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84887294616&doi=10.1109%2fICRA.2013.6630743&partnerID=40&md5=576de078e46efcf4d167d1433d576c7f","CLMC Lab, University of Southern California, Los Angeles CA 90089, United States; Max Planck Institute for Intelligent Systems, Tübingen, 72076, Germany","Kalakrishnan, M., CLMC Lab, University of Southern California, Los Angeles CA 90089, United States; Pastor, P., CLMC Lab, University of Southern California, Los Angeles CA 90089, United States; Righetti, L., CLMC Lab, University of Southern California, Los Angeles CA 90089, United States, Max Planck Institute for Intelligent Systems, Tübingen, 72076, Germany; Schaal, S., CLMC Lab, University of Southern California, Los Angeles CA 90089, United States, Max Planck Institute for Intelligent Systems, Tübingen, 72076, Germany",,"Continuous state-action spaces; Convex objectives; Grasping and manipulation; High-dimensional; Inverse reinforcement learning; Learning objective functions; Redundancy resolution; Robotic manipulation; Algorithms; Cost functions; Inverse kinematics; Optimization; Reinforcement learning; Robotics",Conference Paper,"Final",,Scopus,2-s2.0-84887294616
"Englert P., Paraschos A., Peters J., Deisenroth M.P.","55855227200;55336427400;35248912800;24922891000;","Model-based imitation learning by probabilistic trajectory matching",2013,"Proceedings - IEEE International Conference on Robotics and Automation",,, 6630832,"1922","1927",,20,"10.1109/ICRA.2013.6630832","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84887311623&doi=10.1109%2fICRA.2013.6630832&partnerID=40&md5=02a762475414abb5e7f4eca24a590d4d","Dept. of Computer Science, Technische Universität Darmstadt, Germany; Max Planck Institute for Intelligent Systems, Tübingen, Germany","Englert, P., Dept. of Computer Science, Technische Universität Darmstadt, Germany; Paraschos, A., Dept. of Computer Science, Technische Universität Darmstadt, Germany; Peters, J., Dept. of Computer Science, Technische Universität Darmstadt, Germany, Max Planck Institute for Intelligent Systems, Tübingen, Germany; Deisenroth, M.P., Dept. of Computer Science, Technische Universität Darmstadt, Germany",,"Behavioral cloning; Function learning; Imitation learning; Inverse reinforcement learning; Kullback Leibler divergence; Model-based reinforcement learning; Non-trivial tasks; Trajectory matching; Cost functions; Probability distributions; Reinforcement learning; Robots; Trajectories; Robotics",Conference Paper,"Final",,Scopus,2-s2.0-84887311623
"Klein E., Piot B., Geist M., Pietquin O.","55234823100;55697434100;25929145100;16040586900;","A cascaded supervised learning approach to inverse reinforcement learning",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","8188 LNAI","PART 1",,"1","16",,17,"10.1007/978-3-642-40988-2_1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84886548664&doi=10.1007%2f978-3-642-40988-2_1&partnerID=40&md5=48491f0d52609e1d78c2f9971bd28de4","ABC Team, LORIA-CNRS, France; Supélec, IMS-MaLIS Research Group, France; UMI 2958 (GeorgiaTech-CNRS), France","Klein, E., ABC Team, LORIA-CNRS, France, Supélec, IMS-MaLIS Research Group, France; Piot, B., Supélec, IMS-MaLIS Research Group, France, UMI 2958 (GeorgiaTech-CNRS), France; Geist, M., Supélec, IMS-MaLIS Research Group, France; Pietquin, O., Supélec, IMS-MaLIS Research Group, France, UMI 2958 (GeorgiaTech-CNRS), France",,"Driving simulator; Inverse reinforcement learning; Markov Decision Processes; Near-optimal; Reward function; Score function; State-of-the-art approach; Supervised learning approaches; Learning algorithms; Markov processes; Optimization; Reinforcement learning; Supervised learning; Inverse problems",Conference Paper,"Final",,Scopus,2-s2.0-84886548664
"Piot B., Geist M., Pietquin O.","55697434100;25929145100;16040586900;","Learning from demonstrations: Is it worth estimating a reward function?",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","8188 LNAI","PART 1",,"17","32",,9,"10.1007/978-3-642-40988-2_2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84886520914&doi=10.1007%2f978-3-642-40988-2_2&partnerID=40&md5=41b5bdf6c3eaadf9d987bf489d5681f1","Supélec, IMS-MaLIS Research Group, France; GeorgiaTech-CNRS UMI 2958, France","Piot, B., Supélec, IMS-MaLIS Research Group, France, GeorgiaTech-CNRS UMI 2958, France; Geist, M., Supélec, IMS-MaLIS Research Group, France; Pietquin, O., Supélec, IMS-MaLIS Research Group, France, GeorgiaTech-CNRS UMI 2958, France",,"Apprenticeship learning; Comparative studies; Imitation learning; Inverse reinforcement learning; Learning from demonstration; Markov Decision Processes; Reward function; Learning algorithms; Markov processes; Reinforcement learning; Estimation",Conference Paper,"Final",Open Access,Scopus,2-s2.0-84886520914
[No author name available],[No author id available],"Machine Learning and Knowledge Discovery in Databases - European Conference, ECML PKDD 2013, Proceedings",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","8188 LNAI","PART 1",,"","",2063,,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84886531190&partnerID=40&md5=105f8e158f5aba4a9175359d407bb8f9",,"",,,Conference Review,"Final",,Scopus,2-s2.0-84886531190
[No author name available],[No author id available],"Machine Learning and Knowledge Discovery in Databases - European Conference, ECML PKDD 2013, Proceedings",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","8190 LNAI","PART 3",,"","",2063,,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84886474154&partnerID=40&md5=7605c5ec9d5a425c9506ad3bf511b50a",,"",,,Conference Review,"Final",,Scopus,2-s2.0-84886474154
[No author name available],[No author id available],"Machine Learning and Knowledge Discovery in Databases - European Conference, ECML PKDD 2013, Proceedings",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","8189 LNAI","PART 2",,"","",2063,,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84886451399&partnerID=40&md5=6c1f4b95ef2f206e6f61966047991f4a",,"",,,Conference Review,"Final",,Scopus,2-s2.0-84886451399
"Qiao Q., Beling P.A.","35848809400;6603732790;","Recognition of agents based on observation of their sequential behavior",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","8188 LNAI","PART 1",,"33","48",,2,"10.1007/978-3-642-40988-2_3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84886474556&doi=10.1007%2f978-3-642-40988-2_3&partnerID=40&md5=c5e9a03405d8e3590bd486e8bd0062d0","Department of Systems Engineering, University of Virginia, VA, United States","Qiao, Q., Department of Systems Engineering, University of Virginia, VA, United States; Beling, P.A., Department of Systems Engineering, University of Virginia, VA, United States",,"Classification models; Empirical - comparisons; Inverse reinforcement learning; Learning scenarios; Markov Decision Processes; Navigation problem; Optimal stopping problem; Sequential decisions; Learning algorithms; Markov processes; Optimization; Reinforcement learning",Conference Paper,"Final",Open Access,Scopus,2-s2.0-84886474556
"Klein E., Piot B., Geist M., Pietquin O.","55234823100;55697434100;25929145100;16040586900;","Structured classification for inverse reinforcement learning [Classification structurée pour l'apprentissage par renforcement inverse]",2013,"Revue d'Intelligence Artificielle","27","2",,"155","169",,,"10.3166/RIA.27.155-169","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84885946141&doi=10.3166%2fRIA.27.155-169&partnerID=40&md5=b30444dfc2d0b47907adcfb6c1b01ea9","LORIA Équipe ABC, Nancy, France; Supélec Groupe de Recherche IMS-MaLIS, Metz, France; UMI 2958 (GeorgiaTech-CNRS), Metz, France","Klein, E., LORIA Équipe ABC, Nancy, France, Supélec Groupe de Recherche IMS-MaLIS, Metz, France; Piot, B., Supélec Groupe de Recherche IMS-MaLIS, Metz, France, UMI 2958 (GeorgiaTech-CNRS), Metz, France; Geist, M., Supélec Groupe de Recherche IMS-MaLIS, Metz, France; Pietquin, O., Supélec Groupe de Recherche IMS-MaLIS, Metz, France, UMI 2958 (GeorgiaTech-CNRS), Metz, France","Inverse reinforcement learning; Reinforcement learning","Car driving; Classification structure; Inverse reinforcement learning; Near-optimal; Reward function; Score function; Algorithms; Optimization; Reinforcement learning; Inverse problems",Article,"Final",,Scopus,2-s2.0-84885946141
"Tao Z., Chen Z., Li Y.","55968518700;55967386500;55719284400;","Sensitivity-based inverse reinforcement learning",2013,"Chinese Control Conference, CCC",,, 6639909,"2856","2861",,1,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84890488073&partnerID=40&md5=9683f22b9ea73b1bd683b37b2dc6bbdc","Harbin Institute of Technology, Shenzhen Graduate School, Harbin 518055, China","Tao, Z., Harbin Institute of Technology, Shenzhen Graduate School, Harbin 518055, China; Chen, Z., Harbin Institute of Technology, Shenzhen Graduate School, Harbin 518055, China; Li, Y., Harbin Institute of Technology, Shenzhen Graduate School, Harbin 518055, China","Inverse Reinforcement Learning; Performance sensitivity; Reward function","Markov processes; Site selection; Inverse reinforcement learning; Markov Decision Processes; Optimal control policy; Optimization theory; Performance sensitivity; Potential rewards; Reward function; Unified approach; Reinforcement learning",Conference Paper,"Final",,Scopus,2-s2.0-84890488073
"Englert P., Paraschos A., Deisenroth M.P., Peters J.","55855227200;55336427400;24922891000;35248912800;","Probabilistic model-based imitation learning",2013,"Adaptive Behavior","21","5",,"388","403",,26,"10.1177/1059712313491614","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84884155630&doi=10.1177%2f1059712313491614&partnerID=40&md5=d6ac74c3cd95313e1b1aa5c432cf8a1f","Department of Computer Science, Technische Universität Darmstadt, Germany; Max Planck Institute for Intelligent Systems, Tübingen, Germany","Englert, P., Department of Computer Science, Technische Universität Darmstadt, Germany; Paraschos, A., Department of Computer Science, Technische Universität Darmstadt, Germany; Deisenroth, M.P., Department of Computer Science, Technische Universität Darmstadt, Germany; Peters, J., Department of Computer Science, Technische Universität Darmstadt, Germany, Max Planck Institute for Intelligent Systems, Tübingen, Germany",,,Article,"Final",,Scopus,2-s2.0-84884155630
"Liu S., Araujo M., Brunskill E., Rossetti R., Barros J., Krishnan R.","35339658000;36695850400;15762040900;8598263800;12544810000;7402451300;","Understanding sequential decisions via inverse reinforcement learning",2013,"Proceedings - IEEE International Conference on Mobile Data Management","1",, 6569134,"177","186",,11,"10.1109/MDM.2013.28","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84883549455&doi=10.1109%2fMDM.2013.28&partnerID=40&md5=53a9fb101f16604256f0e0621ac614af","Carnegie Mellon University, 5000 Forbes Ave, Pittsburgh, PA 15213, United States; LIACC, Departamento de Engenharia Informatica, Universidade Do Porto, Rua Dr. Roberto Frias, 4200-465. Porto, Portugal; Instituto de Telecomunicacoes, Faculdade de Engenharia, Universidade Do Porto, Rua Dr Roberto Frias, 4200-465. Porto, Portugal","Liu, S., Carnegie Mellon University, 5000 Forbes Ave, Pittsburgh, PA 15213, United States; Araujo, M., Carnegie Mellon University, 5000 Forbes Ave, Pittsburgh, PA 15213, United States; Brunskill, E., Carnegie Mellon University, 5000 Forbes Ave, Pittsburgh, PA 15213, United States; Rossetti, R., LIACC, Departamento de Engenharia Informatica, Universidade Do Porto, Rua Dr. Roberto Frias, 4200-465. Porto, Portugal; Barros, J., Instituto de Telecomunicacoes, Faculdade de Engenharia, Universidade Do Porto, Rua Dr Roberto Frias, 4200-465. Porto, Portugal; Krishnan, R., Carnegie Mellon University, 5000 Forbes Ave, Pittsburgh, PA 15213, United States",,"Complex activity; Decision makers; Improve performance; Individual preference; Inverse reinforcement learning; Novel algorithm; Public transportation networks; Sequential decisions; Algorithms; Information management; Reinforcement learning; Transportation routes; Motivation",Conference Paper,"Final",,Scopus,2-s2.0-84883549455
"Hayes R.L., Beling P.A., Scherer W.T.","35766253300;6603732790;7102162666;","Action-based feature representation for reverse engineering trading strategies",2013,"Environment Systems and Decisions","33","3",,"413","426",,5,"10.1007/s10669-013-9458-1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84885079056&doi=10.1007%2fs10669-013-9458-1&partnerID=40&md5=5c78d907e070ac0699619d77f53af6a6","Department of Systems and Information Engineering, University of Virginia, Charlottesville, VA, 22903, United States","Hayes, R.L., Department of Systems and Information Engineering, University of Virginia, Charlottesville, VA, 22903, United States; Beling, P.A., Department of Systems and Information Engineering, University of Virginia, Charlottesville, VA, 22903, United States; Scherer, W.T., Department of Systems and Information Engineering, University of Virginia, Charlottesville, VA, 22903, United States","Algorithm trading; Reverse engineering; Trading strategy","financial market; learning; research; student",Article,"Final",,Scopus,2-s2.0-84885079056
"Pietquin O.","16040586900;","Inverse reinforcement learning for interactive systems",2013,"ACM International Conference Proceeding Series",,,,"71","75",,2,"10.1145/2493525.2493529","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84883034035&doi=10.1145%2f2493525.2493529&partnerID=40&md5=00e70e1c2c9b2ac1d5885debdd12c24f","SUPELEC - UMI 2958 (GeorgiaTech-CNRS), 2 Rue Edouard Belin, 57070 Metz, France","Pietquin, O., SUPELEC - UMI 2958 (GeorgiaTech-CNRS), 2 Rue Edouard Belin, 57070 Metz, France","Human-machine interaction; Reinforcement learning","Human activity recognition; Human machine interaction; Interaction managers; Interactive system; Inverse reinforcement learning; Machine learning techniques; Natural language generation; Sequential decision making; Algorithms; Artificial intelligence; Communication; Human computer interaction; Reinforcement learning",Conference Paper,"Final",,Scopus,2-s2.0-84883034035
[No author name available],[No author id available],"Proceedings of the 2nd Workshop on Machine Learning for Interactive Systems: Bridging the Gap Between Perception, Action and Communication, MLIS 2013 - 23rd IJCAI 2013",2013,"ACM International Conference Proceeding Series",,,,"","",81,,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84883008795&partnerID=40&md5=7407a17bb4138e08cfae3b85c4445026",,"",,,Conference Review,"Final",,Scopus,2-s2.0-84883008795
"Iturrate I., Omedes J., Montesano L.","57194101260;55567220200;6602849539;","Shared control of a robot using EEG-based feedback Signals",2013,"ACM International Conference Proceeding Series",,,,"45","50",,4,"10.1145/2493525.2493533","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84882932193&doi=10.1145%2f2493525.2493533&partnerID=40&md5=fe837c5b62fab17f55d05e02a4a1a7ed","I3A, DIIS, EINA, University of Zaragoza, Zaragoza, Spain","Iturrate, I., I3A, DIIS, EINA, University of Zaragoza, Zaragoza, Spain; Omedes, J., I3A, DIIS, EINA, University of Zaragoza, Zaragoza, Spain; Montesano, L., I3A, DIIS, EINA, University of Zaragoza, Zaragoza, Spain",,"Binary information; Continuous state; Feedback signal; Graphical interface; Human observations; Inverse reinforcement learning; Multiple modalities; Robot operations; Artificial intelligence; Communication; Reinforcement learning; Robots",Conference Paper,"Final",,Scopus,2-s2.0-84882932193
"Rothkopf C.A., Ballard D.H.","15078701200;35809594800;","Modular inverse reinforcement learning for visuomotor behavior",2013,"Biological Cybernetics","107","4",,"477","490",,18,"10.1007/s00422-013-0562-6","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84883447302&doi=10.1007%2fs00422-013-0562-6&partnerID=40&md5=2db905fa5905a9a0ad9830ee89695d92","Frankfurt Institute for Advanced Studies, Goethe University, 60438 Frankfurt, Germany; Institute of Cognitive Science, University Osnabrück, 49076 Osnabrück, Germany; Technical University Darmstadt, 64283 Darmstadt, Germany; Department for Computer Science, University of Texas at Austin, Austin, TX 78712, United States","Rothkopf, C.A., Frankfurt Institute for Advanced Studies, Goethe University, 60438 Frankfurt, Germany, Institute of Cognitive Science, University Osnabrück, 49076 Osnabrück, Germany, Technical University Darmstadt, 64283 Darmstadt, Germany; Ballard, D.H., Department for Computer Science, University of Texas at Austin, Austin, TX 78712, United States","Inverse reinforcement learning; Spatial navigation; Task priorities; Visuomotor behavior","Avoiding obstacle; Cognitive architectures; Computational model; Costs and benefits; Inverse reinforcement learning; Spatial navigation; Task priorities; Visuomotors; Mathematical models; Reinforcement learning; Animalia; algorithm; article; computer simulation; human; learning; psychomotor performance; theoretical model; vision; Algorithms; Computer Simulation; Humans; Learning; Models, Theoretical; Psychomotor Performance; Vision, Ocular; Algorithms; Computer Simulation; Humans; Learning; Models, Theoretical; Psychomotor Performance; Vision, Ocular",Article,"Final",,Scopus,2-s2.0-84883447302
"Murray W.R., Harrison P., Singliar T.","57202909075;23094014700;15043013000;","Interpreting spatiotemporal expressions from english to fuzzy logic",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","8035",,,"226","233",,,"10.1007/978-3-642-39617-5_21","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880982771&doi=10.1007%2f978-3-642-39617-5_21&partnerID=40&md5=7836a29c16a2f842bc87f6d06d0b4bcd","Boeing Research and Technology, PO Box 3707, Seattle, WA 98124, United States","Murray, W.R., Boeing Research and Technology, PO Box 3707, Seattle, WA 98124, United States; Harrison, P., Boeing Research and Technology, PO Box 3707, Seattle, WA 98124, United States; Singliar, T., Boeing Research and Technology, PO Box 3707, Seattle, WA 98124, United States","anomaly detection; controlled natural language; CPL; fuzzy logic; GPSG parsing; inverse reinforcement learning; spatiotemporal reasoning","Computer circuits; Computer programming languages; Context free grammars; Learning algorithms; Learning systems; Reinforcement learning; Syntactics; Web crawler; Anomaly detection; Controlled natural language; GPSG parsing; Inverse reinforcement learning; Spatio-temporal reasoning; Fuzzy logic",Conference Paper,"Final",,Scopus,2-s2.0-84880982771
"Dragan A.D., Srinivasa S.S.","55193779100;6602084313;","A policy-blending formalism for shared control",2013,"International Journal of Robotics Research","32","7",,"790","805",,100,"10.1177/0278364913490324","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84879894559&doi=10.1177%2f0278364913490324&partnerID=40&md5=80d1d854125bbaade4cd3aff819dc319","Robotics Institute, Carnegie Mellon University, United States","Dragan, A.D., Robotics Institute, Carnegie Mellon University, United States; Srinivasa, S.S., Robotics Institute, Carnegie Mellon University, United States","arbitration; human-robot collaboration; intent prediction; shared control; sliding autonomy; teleoperation","arbitration; Human-robot collaboration; Inverse reinforcement learning; Prediction problem; Robotic manipulators; Shared control; Simplifying assumptions; Sliding autonomy; Blending; Forecasting; Inverse problems; Reinforcement learning; Remote control; Robots; Behavioral research",Conference Paper,"Final",,Scopus,2-s2.0-84879894559
"Boularias A., Chaib-draa B.","23090275300;7004238886;","Apprenticeship learning with few examples",2013,"Neurocomputing","104",,,"83","96",,3,"10.1016/j.neucom.2012.11.002","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873703578&doi=10.1016%2fj.neucom.2012.11.002&partnerID=40&md5=8b2c44c096ea60e6286e597a604461d4","Max Planck Institute for Intelligent Systems, Spemannstraße 41, 72076 Tuebingen, Germany; Computer Science and Software Engineering Department, Laval University, Quebec G1V 0A6, Canada","Boularias, A., Max Planck Institute for Intelligent Systems, Spemannstraße 41, 72076 Tuebingen, Germany; Chaib-draa, B., Computer Science and Software Engineering Department, Laval University, Quebec G1V 0A6, Canada","Bootstrapping; Imitation learning; Inverse reinforcement learning; Transfer learning","Apprenticeship learning; AS graph; Bootstrapping; Empirical averages; Imitation learning; Inverse reinforcement learning; Linear combinations; Near-optimal; New approaches; Optimal policies; Reward function; Simulated robot; State space; Transfer learning; Transfer technique; Value functions; Dynamics; Learning algorithms; Optimization; Reinforcement learning; Apprentices; algorithm; analytical error; apprenticeship learning; article; bootstrapping; dynamics; imitation; learning; priority journal; reinforcement; reward; robotics; simulation; statistical analysis",Article,"Final",,Scopus,2-s2.0-84873703578
"Ziebart B.D., Bagnell J.A., Dey A.K.","13608719300;6506127486;7101701731;","The principle of maximum causal entropy for estimating interacting processes",2013,"IEEE Transactions on Information Theory","59","4", 6479340,"1966","1980",,20,"10.1109/TIT.2012.2234824","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84896690184&doi=10.1109%2fTIT.2012.2234824&partnerID=40&md5=319b27f0a8c54dd29611d5c16d18a24b","Department of Computer Science, University of Illinois at Chicago, Chicago, IL 60607, United States; School of Computer Science, Carnegie Mellon University, Pittsburgh, PA 15213, United States","Ziebart, B.D., Department of Computer Science, University of Illinois at Chicago, Chicago, IL 60607, United States; Bagnell, J.A., School of Computer Science, Carnegie Mellon University, Pittsburgh, PA 15213, United States; Dey, A.K., School of Computer Science, Carnegie Mellon University, Pittsburgh, PA 15213, United States","Causal entropy; Correlated equilibrium (CE); Directed information; Inverse optimal control; Inverse reinforcement learning; Maximum entropy; Statistical estimation","Computer games; Entropy; Maximum entropy methods; Maximum principle; Probability distributions; Reinforcement learning; Correlated equilibria; Directed information; Inverse reinforcement learning; Inverse-optimal control; Statistical estimation; Estimation",Article,"Final",,Scopus,2-s2.0-84896690184
"Luo D., Wang Y., Wu X.","7202658172;57192184073;7407061832;","Discriminative apprenticeship learning with both preference and non-preference behavior",2013,"Proceedings - 2013 12th International Conference on Machine Learning and Applications, ICMLA 2013","1",, 6784634,"315","320",,1,"10.1109/ICMLA.2013.64","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84899447237&doi=10.1109%2fICMLA.2013.64&partnerID=40&md5=906efa861a9154c166bd3d2a106b0da0","Key Lab of Machine Perception (Ministry of Education), Peking University, Beijing, 100871, China; Speech and Hearing Research Center, Peking University, Beijing, 100871, China; College of Computer Science and Technology, Jilin University, Changchun, 130012, China","Luo, D., Key Lab of Machine Perception (Ministry of Education), Peking University, Beijing, 100871, China, Speech and Hearing Research Center, Peking University, Beijing, 100871, China; Wang, Y., Key Lab of Machine Perception (Ministry of Education), Peking University, Beijing, 100871, China, Speech and Hearing Research Center, Peking University, Beijing, 100871, China; Wu, X., Key Lab of Machine Perception (Ministry of Education), Peking University, Beijing, 100871, China, Speech and Hearing Research Center, Peking University, Beijing, 100871, China, College of Computer Science and Technology, Jilin University, Changchun, 130012, China","Discriminative apprenticeship learning; Inverse reinforcement learning; Preference and non-preference","Learning algorithms; Reinforcement learning; Apprenticeship learning; Changing environment; Generalization ability; Generalization Error; Inverse reinforcement learning; Preference and non-preference; Reward function; Theoretical guarantees; Apprentices",Conference Paper,"Final",,Scopus,2-s2.0-84899447237
"Singh S.S., Chopin N., Whiteley N.","37062609900;35572803700;55336867600;","Bayesian learning of noisy markov decision processes",2013,"ACM Transactions on Modeling and Computer Simulation","23","1", 4,"","",,2,"10.1145/2414416.2414420","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873697423&doi=10.1145%2f2414416.2414420&partnerID=40&md5=963d9ac0971a83a74919a0df94ea096b","Department of Engineering, University of Cambridge, 325 Baker Building, Trumpington Street, Cambridge, CB2 1PZ, United Kingdom; CREST-ENSAE, France; University of Bristol, School of Mathematics, University Walk, Bristol BS8 1TW, United Kingdom","Singh, S.S., Department of Engineering, University of Cambridge, 325 Baker Building, Trumpington Street, Cambridge, CB2 1PZ, United Kingdom; Chopin, N., CREST-ENSAE, France; Whiteley, N., University of Bristol, School of Mathematics, University Walk, Bristol BS8 1TW, United Kingdom","Bayesian inference; Data augmentation; Markov Chain Monte Carlo; Markov decision process; Parameter expansion","Bayesian inference; Data augmentation; Markov Chain Monte-Carlo; Markov Decision Processes; Parameter expansion; Bayesian networks; Inference engines; Inverse problems; Markov processes; Reinforcement learning; Learning algorithms",Article,"Final",,Scopus,2-s2.0-84873697423
"Sugiyama H., Meguro T., Minami Y.","56965179800;36632749100;24428386300;","Preference-learning based inverse reinforcement learning for dialog control",2012,"13th Annual Conference of the International Speech Communication Association 2012, INTERSPEECH 2012","1",,,"222","225",,9,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878412864&partnerID=40&md5=387fa005a872733b7eaabfec1556ca53","NTT Communication Science Laboratories, Kyoto, Japan","Sugiyama, H., NTT Communication Science Laboratories, Kyoto, Japan; Meguro, T., NTT Communication Science Laboratories, Kyoto, Japan; Minami, Y., NTT Communication Science Laboratories, Kyoto, Japan",,"Competitive algorithms; Dialog controls; Dialog systems; Inverse reinforcement learning; Reward function; Algorithms; Reinforcement learning",Conference Paper,"Final",,Scopus,2-s2.0-84878412864
"Reddy T.S., Gopikrishna V., Zaruba G., Huber M.","55557204500;35322408300;6602515225;7202671760;","Inverse reinforcement learning for decentralized non-cooperative multiagent systems",2012,"Conference Proceedings - IEEE International Conference on Systems, Man and Cybernetics",,, 6378020,"1930","1935",,5,"10.1109/ICSMC.2012.6378020","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84872422561&doi=10.1109%2fICSMC.2012.6378020&partnerID=40&md5=dc588a31fe74bea836cbc64a063d757c","Computer Science Department, University of Texas at Arlington, Arlington, TX, United States","Reddy, T.S., Computer Science Department, University of Texas at Arlington, Arlington, TX, United States; Gopikrishna, V., Computer Science Department, University of Texas at Arlington, Arlington, TX, United States; Zaruba, G., Computer Science Department, University of Texas at Arlington, Arlington, TX, United States; Huber, M., Computer Science Department, University of Texas at Arlington, Arlington, TX, United States","Game Theory; General-Sum Stochastic Games; Inverse Reinfocement Learning; Multiagent Systems; Nash equilibrium","Distributed solutions; Inverse Reinfocement Learning; Inverse reinforcement learning; Multi-agent problems; Multi-agent setting; Nash equilibria; Non-cooperative; Optimal policies; Reward function; Single-agent; Stochastic game; Cybernetics; Game theory; Reinforcement learning; Multi agent systems",Conference Paper,"Final",,Scopus,2-s2.0-84872422561
"Singliar T., Margineantu D.D.","15043013000;9738431900;","Scalable inverse reinforcement learning via instructed feature construction",2012,"AAAI Workshop - Technical Report","WS-12-11",,,"33","35",,,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84875583424&partnerID=40&md5=061580a7cddf361b2384a6a52270e7ae","Boeing Research and Technology, M/C 7L-44, P.O. Box 3707, Seattle, WA 98124-2207, United States","Singliar, T., Boeing Research and Technology, M/C 7L-44, P.O. Box 3707, Seattle, WA 98124-2207, United States; Margineantu, D.D., Boeing Research and Technology, M/C 7L-44, P.O. Box 3707, Seattle, WA 98124-2207, United States",,"Basis functions; Feature construction; Inverse reinforcement learning; Limited rationality; Reward function; Traditional models; Value function approximation; Value functions; Stochastic models; Reinforcement learning",Conference Paper,"Final",,Scopus,2-s2.0-84875583424
"Choi J., Kim K.-E.","36835669400;15053383400;","Nonparametric Bayesian inverse reinforcement learning for multiple reward functions",2012,"Advances in Neural Information Processing Systems","1",,,"305","313",,31,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84877772241&partnerID=40&md5=fc0735bfddc6092e23d5cbea486adaf9","Department of Computer Science, Korea Advanced Institute of Science and Technology, Daejeon 305-701, South Korea","Choi, J., Department of Computer Science, Korea Advanced Institute of Science and Technology, Daejeon 305-701, South Korea; Kim, K.-E., Department of Computer Science, Korea Advanced Institute of Science and Technology, Daejeon 305-701, South Korea",,"Bayesian; Dirichlet process mixture model; Inverse reinforcement learning; Metropolis-Hastings samplings; Multiple rewards; Non-parametric Bayesian; Problem domain; Reward function; Algorithms; Bayesian networks; Reinforcement learning",Conference Paper,"Final",,Scopus,2-s2.0-84877772241
"Klein E., Geist M., Piot B., Pietquin O.","55234823100;25929145100;55697434100;16040586900;","Inverse reinforcement learning through structured classification",2012,"Advances in Neural Information Processing Systems","2",,,"1007","1015",,29,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84877756695&partnerID=40&md5=27f1bed54aa7e7a1fd17e16183d6398c","LORIA, Team ABC, Nancy, France; Supélec, IMS-MaLIS Research Group, Metz, France; UMI 2958 (GeorgiaTech-CNRS), Metz, France","Klein, E., LORIA, Team ABC, Nancy, France, Supélec, IMS-MaLIS Research Group, Metz, France; Geist, M., Supélec, IMS-MaLIS Research Group, Metz, France; Piot, B., Supélec, IMS-MaLIS Research Group, Metz, France, UMI 2958 (GeorgiaTech-CNRS), Metz, France; Pietquin, O., Supélec, IMS-MaLIS Research Group, Metz, France, UMI 2958 (GeorgiaTech-CNRS), Metz, France",,"Car driving; Inverse reinforcement learning; Multi-class classifier; Near-optimal; Reward function; Score function; Algorithms; Optimization; Reinforcement learning; Inverse problems",Conference Paper,"Final",,Scopus,2-s2.0-84877756695
"Chinaei H.R., Chaib-Draa B.","23388191000;7004238886;","An inverse reinforcement learning algorithm for partially observable domains with application on healthcare dialogue management",2012,"Proceedings - 2012 11th International Conference on Machine Learning and Applications, ICMLA 2012","1",, 6406603,"144","149",,3,"10.1109/ICMLA.2012.31","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873590016&doi=10.1109%2fICMLA.2012.31&partnerID=40&md5=3b637cb68ed4835579cdd5b4f298274a","Computer Science Department, Laval University, Québec, QC, Canada","Chinaei, H.R., Computer Science Department, Laval University, Québec, QC, Canada; Chaib-Draa, B., Computer Science Department, Laval University, Québec, QC, Canada","dialogue management; Inverse reinforcement learning; partially observable Markov decision processes","Dialogue management; Inverse reinforcement learning; Markov Decision Processes; Model components; Partially observable Markov decision process; Health care; Inverse problems; Markov processes; Reinforcement learning; Learning algorithms",Conference Paper,"Final",,Scopus,2-s2.0-84873590016
"Boularias A., Krömer O., Peters J.","23090275300;36463880000;35248912800;","Algorithms for learning Markov field policies",2012,"Advances in Neural Information Processing Systems","3",,,"2177","2185",,,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84877750880&partnerID=40&md5=f9e7cd783d262b1f739f04485e780a8c","Max Planck Institute for Intelligent Systems, Germany; Technische Universität Darmstadt, Germany","Boularias, A., Max Planck Institute for Intelligent Systems, Germany; Krömer, O., Technische Universität Darmstadt, Germany; Peters, J., Technische Universität Darmstadt, Germany",,"Domain knowledge; GraphicaL model; High probability; Inverse reinforcement learning; Markov Decision Processes; Markov Random Fields; Optimal actions; Policy distribution; Learning algorithms; Markov processes; Reinforcement learning; Probability distributions",Conference Paper,"Final",,Scopus,2-s2.0-84877750880
"MacGlashan J., Babeş-Vroman M., Winner K., Gao R., Adjogah R., DesJardins M., Littman M., Muresan S.","21934008200;52263115900;57204815305;57211271340;55635986700;10239052700;7006510438;57206143958;","Learning to interpret natural language instructions",2012,"AAAI Workshop - Technical Report","WS-12-07",,,"18","24",,1,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84875609319&partnerID=40&md5=b5eb86a0d028a936a679982f702f51fb","Department of Computer Science and Electrical Engineering, University of Maryland, Baltimore County, MD, United States; Computer Science Department, Rutgers University, United States; School of Communication and Information, Rutgers University, United States","MacGlashan, J., Department of Computer Science and Electrical Engineering, University of Maryland, Baltimore County, MD, United States; Babeş-Vroman, M., Computer Science Department, Rutgers University, United States; Winner, K., Department of Computer Science and Electrical Engineering, University of Maryland, Baltimore County, MD, United States; Gao, R., Computer Science Department, Rutgers University, United States; Adjogah, R., Department of Computer Science and Electrical Engineering, University of Maryland, Baltimore County, MD, United States; DesJardins, M., Department of Computer Science and Electrical Engineering, University of Maryland, Baltimore County, MD, United States; Littman, M., Computer Science Department, Rutgers University, United States; Muresan, S., School of Communication and Information, Rutgers University, United States",,"Artificial agents; Domain specific; Goal definitions; Inverse reinforcement learning; Natural languages; Reward function; Semantic parsing; Three component; Reinforcement learning; Semantics; Natural language processing systems",Conference Paper,"Final",,Scopus,2-s2.0-84875609319
"Tastan B., Chang Y., Sukthankar G.","6505775414;55546271300;6507708407;","Learning to intercept opponents in first person shooter games",2012,"2012 IEEE Conference on Computational Intelligence and Games, CIG 2012",,, 6374144,"100","107",,11,"10.1109/CIG.2012.6374144","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84871941831&doi=10.1109%2fCIG.2012.6374144&partnerID=40&md5=dffa55757bde2621a1d2ae753acabe32","Department of EECS, University of Central Florida, Orlando, FL, United States","Tastan, B., Department of EECS, University of Central Florida, Orlando, FL, United States; Chang, Y., Department of EECS, University of Central Florida, Orlando, FL, United States; Sukthankar, G., Department of EECS, University of Central Florida, Orlando, FL, United States",,"First person shooter games; Human players; Inverse reinforcement learning; Motion models; Motion prediction; Particle filter; Prediction methods; Time horizons; Tracking accuracy; Unreal tournament; Motion planning; Reinforcement learning; Forecasting",Conference Paper,"Final",,Scopus,2-s2.0-84871941831
"Yang S., Paddrik M., Hayes R., Todd A., Kirilenko A., Beling P., Scherer W.","24802841600;24802447300;35766253300;55496521200;7005937861;6603732790;7102162666;","Behavior based learning in identifying High Frequency Trading strategies",2012,"2012 IEEE Conference on Computational Intelligence for Financial Engineering and Economics, CIFEr 2012 - Proceedings",,, 6327783,"133","140",,12,"10.1109/CIFEr.2012.6327783","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84869779430&doi=10.1109%2fCIFEr.2012.6327783&partnerID=40&md5=da4d0674b126f12ca1896e902a3b58b3","Department of Systems and Information Engineering, University of Virginia, United States; Commodity Futures Trading Commission, United States","Yang, S., Department of Systems and Information Engineering, University of Virginia, United States; Paddrik, M., Department of Systems and Information Engineering, University of Virginia, United States; Hayes, R., Department of Systems and Information Engineering, University of Virginia, United States; Todd, A., Department of Systems and Information Engineering, University of Virginia, United States; Kirilenko, A., Commodity Futures Trading Commission, United States; Beling, P., Department of Systems and Information Engineering, University of Virginia, United States; Scherer, W., Department of Systems and Information Engineering, University of Virginia, United States","Algorithmic trading; High Frequency Trading; Inverse Reinforcement Learning; Limit order book; Markov Decision Process; Price impact; Spoofing","Algorithmic trading; High frequency; Inverse reinforcement learning; Limit order book; Markov Decision Processes; Price impacts; Spoofing; Algorithms; Artificial intelligence; Correlation theory; Finance; Inverse problems; Markov processes; Reinforcement learning; Commerce",Conference Paper,"Final",,Scopus,2-s2.0-84869779430
"Vogel A., Ramachandran D., Gupta R., Raux A.","36945583200;56247655000;56537374400;14018300800;","Improving hybrid vehicle fuel efficiency using inverse reinforcement learning",2012,"Proceedings of the National Conference on Artificial Intelligence","1",,,"384","390",,15,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84868272761&partnerID=40&md5=de0776a2574cfbc32a41ebf907744570","Stanford University, United States; Honda Research Institute (USA) Inc., United States","Vogel, A., Stanford University, United States; Ramachandran, D., Honda Research Institute (USA) Inc., United States; Gupta, R., Honda Research Institute (USA) Inc., United States; Raux, A., Honda Research Institute (USA) Inc., United States",,"Battery power; Driver behavior; Fuel efficiency; Fuel savings; Hardware modifications; Hybrid controls; Inverse reinforcement learning; Optimal mixes; Power efficiency; Power requirement; Prediction systems; Artificial intelligence; Fuel economy; Fuels; Hybrid vehicles; Land vehicle propulsion; Optimization; Reinforcement learning; Efficiency",Conference Paper,"Final",,Scopus,2-s2.0-84868272761
"Levine S., Koltun V.","35731728100;7004337914;","Continuous inverse optimal control with locally optimal examples",2012,"Proceedings of the 29th International Conference on Machine Learning, ICML 2012","1",,,"41","48",,65,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867123291&partnerID=40&md5=737595bfb1fe409ba7278928432d0b82","Computer Science Department, Stanford University, Stanford, CA 94305, United States","Levine, S., Computer Science Department, Stanford University, Stanford, CA 94305, United States; Koltun, V., Computer Science Department, Stanford University, Stanford, CA 94305, United States",,"Continuous domain; Inverse reinforcement learning; Inverse-optimal control; Local approximation; Local optimality; Markov Decision Processes; Optimal policies; Reward function; Inverse problems; Learning algorithms; Markov processes; Reinforcement learning; Optimization",Conference Paper,"Final",,Scopus,2-s2.0-84867123291
"Shao Z., Er M.J.","55339553100;7102842925;","A review of inverse reinforcement learning theory and recent advances",2012,"2012 IEEE Congress on Evolutionary Computation, CEC 2012",,, 6256507,"","",,17,"10.1109/CEC.2012.6256507","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84866865478&doi=10.1109%2fCEC.2012.6256507&partnerID=40&md5=29ecb2c129c5bb0532396273b9694c95","School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore, Singapore","Shao, Z., School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore, Singapore; Er, M.J., School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore, Singapore","expert demonstration; inverse reinforcement learning; Reinforcement learning; reward function","Complex problems; Decision-making problem; Dynamic environments; Generalization ability; Inverse reinforcement learning; Learning policy; Machine learning communities; Reinforcement learning techniques; Reward function; Succinct representation; Algorithms; Reinforcement learning; Inverse problems",Conference Paper,"Final",,Scopus,2-s2.0-84866865478
"Michini B., How J.P.","36603096200;7006512768;","Bayesian nonparametric inverse reinforcement learning",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","7524 LNAI","PART 2",,"148","163",,13,"10.1007/978-3-642-33486-3_10","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84866860874&doi=10.1007%2f978-3-642-33486-3_10&partnerID=40&md5=ddfcb7477e310778845d8f3f1c88b1e6","Massachusetts Institute of Technology, Cambridge, MA, United States","Michini, B., Massachusetts Institute of Technology, Cambridge, MA, United States; How, J.P., Massachusetts Institute of Technology, Cambridge, MA, United States",,"Computationally efficient; Different structure; Inverse reinforcement learning; Markov Decision Processes; Mixture model; Non-parametric; Real-world learning; Reward function; State space; Subgoals; Transition functions; Learning algorithms; Markov processes; Reinforcement learning",Conference Paper,"Final",Open Access,Scopus,2-s2.0-84866860874
"Murray W.R., Singliar T.","57202909075;15043013000;","Spatiotemporal extensions to a controlled natural language",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","7427 LNAI",,,"61","78",,3,"10.1007/978-3-642-32612-7_5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84865677079&doi=10.1007%2f978-3-642-32612-7_5&partnerID=40&md5=e3cbc415a8fecebd3cfa97fb24ce8dcc","Boeing Research and Technology, P.O. Box 3707, Seattle, WA 98124, United States","Murray, W.R., Boeing Research and Technology, P.O. Box 3707, Seattle, WA 98124, United States; Singliar, T., Boeing Research and Technology, P.O. Box 3707, Seattle, WA 98124, United States","Allen's Interval Algebra; controlled natural language; CPL; inverse reinforcement learning; RCC-8; spatiotemporal reasoning","Allen's interval algebra; Controlled natural language; CPL; Inverse reinforcement learning; RCC-8; Spatio-temporal reasoning; Motivation; Reinforcement learning; Learning algorithms",Conference Paper,"Final",,Scopus,2-s2.0-84865677079
"Zhifei S., Joo E.M.","55339553100;7102842925;","A survey of inverse reinforcement learning techniques",2012,"International Journal of Intelligent Computing and Cybernetics","5","3",,"293","311",,29,"10.1108/17563781211255862","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84865146660&doi=10.1108%2f17563781211255862&partnerID=40&md5=5a8d14d716804fc981f2949c17a5db3b","School of Electrical and Electronics Engineering, Nanyang Technological University, Singapore, Singapore","Zhifei, S., School of Electrical and Electronics Engineering, Nanyang Technological University, Singapore, Singapore; Joo, E.M., School of Electrical and Electronics Engineering, Nanyang Technological University, Singapore, Singapore","Artificial intelligence; Inverse reinforcement learning; Learning methods; Reinforcement learning; Reward function","Complex problems; Design/methodology/approach; Dynamic environments; Fundamental theory; Inverse reinforcement learning; Latest development; Learning methods; Reinforcement learning techniques; Reward function; Sequential decision making; Succinct representation; Algorithms; Artificial intelligence; Surveys; Reinforcement learning",Article,"Final",,Scopus,2-s2.0-84865146660
"Klein E., Geist M., Pietquin O.","55234823100;25929145100;16040586900;","Batch, off-policy and model-free apprenticeship learning",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","7188 LNAI",,,"285","296",,4,"10.1007/978-3-642-29946-9_28","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84861679549&doi=10.1007%2f978-3-642-29946-9_28&partnerID=40&md5=ceaa3029aa9e912551823633d4e99cfb","Supélec, IMS Research Group, France; UMI 2958, GeorgiaTech-CNRS, France; Equipe ABC, LORIA-CNRS, France","Klein, E., Supélec, IMS Research Group, France, Equipe ABC, LORIA-CNRS, France; Geist, M., Supélec, IMS Research Group, France; Pietquin, O., Supélec, IMS Research Group, France, UMI 2958, GeorgiaTech-CNRS, France",,"Apprenticeship learning; Associated feature; Generative model; Inverse reinforcement learning; Learning control; Model free; Monte Carlo Simulation; Parameterized; Reward function; Temporal differences; Utility functions; Monte Carlo methods; Reinforcement learning; Apprentices",Conference Paper,"Final",,Scopus,2-s2.0-84861679549
"Michini B., How J.P.","36603096200;7006512768;","Improving the efficiency of Bayesian inverse reinforcement learning",2012,"Proceedings - IEEE International Conference on Robotics and Automation",,, 6225241,"3651","3656",,14,"10.1109/ICRA.2012.6225241","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84864464460&doi=10.1109%2fICRA.2012.6225241&partnerID=40&md5=0142288bc1788ff0c277618ee2c889f2","Aerospace Controls Laboratory, MIT, United States; Laboratory for Information and Decision Systems, MIT, United States","Michini, B., Aerospace Controls Laboratory, MIT, United States; How, J.P., Aerospace Controls Laboratory, MIT, United States, Laboratory for Information and Decision Systems, MIT, United States",,"Bayesian networks; Demonstrations; Efficiency; Inference engines; Inverse problems; Markov processes; Bayesian inference; Faster convergence; Inverse reinforcement learning; Its efficiencies; Markov Decision Processes; Modified algorithms; Solution quality; Transition functions; Reinforcement learning",Conference Paper,"Final",,Scopus,2-s2.0-84864464460
"Chandramohan S., Geist M., Lefèvre F., Pietquin O.","55817552546;25929145100;56273617700;16040586900;","Behavior specific user simulation in spoken dialogue systems",2012,"Proceedings of 10th ITG Symposium on Speech Communication",,, 6309603,"","",,3,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84883047874&partnerID=40&md5=66394fa6e9a17b0af1a0d768a3194d9a","Supelec, IMS - MaLIS Research Group, France; UMI 2958 (CNRS - GeorgiaTech), France; Université d'Avignon et des Pays de Vaucluse, LIA-CERI, France","Chandramohan, S., Supelec, IMS - MaLIS Research Group, France, Université d'Avignon et des Pays de Vaucluse, LIA-CERI, France; Geist, M., Supelec, IMS - MaLIS Research Group, France; Lefèvre, F., Université d'Avignon et des Pays de Vaucluse, LIA-CERI, France; Pietquin, O., Supelec, IMS - MaLIS Research Group, France, UMI 2958 (CNRS - GeorgiaTech), France",,"Inverse problems; Linguistics; Reinforcement learning; Speech communication; Speech processing; Data requirements; Data-driven methods; Inverse reinforcement learning; Policy optimization; Spoken dialogue system; Spoken languages; State of the art; User simulation; Behavioral research",Conference Paper,"Final",,Scopus,2-s2.0-84883047874
"Aghasadeghi N., Bretl T.","52763227100;6506270751;","Maximum entropy inverse reinforcement learning in continuous state spaces with path integrals",2011,"IEEE International Conference on Intelligent Robots and Systems",,, 6048804,"1561","1566",,22,"10.1109/IROS.2011.6048804","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84455160661&doi=10.1109%2fIROS.2011.6048804&partnerID=40&md5=a5ef44890dac4e25b0e0a2390ef7b425","Department of Electrical and Computer Engineering, University of Illinois at Urbana-Champaign, Urbana, IL 61801, United States; Department of Aerospace Engineering, University of Illinois at Urbana-Champaign, Urbana, IL 61801, United States","Aghasadeghi, N., Department of Electrical and Computer Engineering, University of Illinois at Urbana-Champaign, Urbana, IL 61801, United States; Bretl, T., Department of Aerospace Engineering, University of Illinois at Urbana-Champaign, Urbana, IL 61801, United States",,"Action spaces; Basis functions; Continuous state; Continuous State Space; Continuous time; Current estimates; Empirical evidence; Finite set; Input costs; Input noise; Inverse reinforcement learning; Maximum entropy; Maximum entropy distribution; Optimal control policy; Path integral; Sample path; Continuous time systems; Cost functions; Costs; Entropy; Intelligent robots; Inverse problems; Maximum likelihood estimation; Optimization; Reinforcement; Reinforcement learning; Robotics; Cost benefit analysis",Conference Paper,"Final",,Scopus,2-s2.0-84455160661
"Levine S., Popović Z., Koltun V.","35731728100;35176314800;7004337914;","Nonlinear inverse reinforcement learning with Gaussian processes",2011,"Advances in Neural Information Processing Systems 24: 25th Annual Conference on Neural Information Processing Systems 2011, NIPS 2011",,,,"","",9,92,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84860621067&partnerID=40&md5=63ebcead252f16eebde04c4b67a565cd","Stanford University, United States; University of Washington, United States","Levine, S., Stanford University, United States; Popović, Z., University of Washington, United States; Koltun, V., Stanford University, United States",,"Complex behavior; Gaussian Processes; Inverse reinforcement learning; Linear combinations; Markov Decision Processes; Nonlinear functions; Probabilistic algorithm; Reward function; Gaussian distribution; Gaussian noise (electronic); Learning algorithms; Markov processes; Reinforcement learning",Conference Paper,"Final",,Scopus,2-s2.0-84860621067
"Boularias A., Kober J., Peters J.","23090275300;22940629000;35248912800;","Relative entropy Inverse Reinforcement Learning",2011,"Journal of Machine Learning Research","15",,,"182","189",,62,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84862293297&partnerID=40&md5=6bb3401274a67b9a5b08b1157570752a","Max-Planck Institute for Intelligent Systems, 72076 Tübingen, Germany","Boularias, A., Max-Planck Institute for Intelligent Systems, 72076 Tübingen, Germany; Kober, J., Max-Planck Institute for Intelligent Systems, 72076 Tübingen, Germany; Peters, J., Max-Planck Institute for Intelligent Systems, 72076 Tübingen, Germany",,"Empirical distributions; Imitation learning; Inverse reinforcement learning; Markov Decision Processes; Model free; Optimal policies; Relative entropy; Reward function; State space; Stochastic gradient descent; Algorithms; Artificial intelligence; Entropy; Markov processes; Reinforcement learning; Mathematical models",Conference Paper,"Final",,Scopus,2-s2.0-84862293297
"Chandramohan S., Geist M., Lefèvre F., Pietquin O.","55817552546;25929145100;56273617700;16040586900;","User simulation in dialogue systems using Inverse Reinforcement Learning",2011,"Proceedings of the Annual Conference of the International Speech Communication Association, INTERSPEECH",,,,"1025","1028",,41,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84865717612&partnerID=40&md5=72da518c50fcb34bb90d80939ccd3640","Supelec - Metz Campus, IMS Research Group, France; UMI 2958 (CNRS - GeorgiaTech), France; Université D'Avignon et des Pays de Vaucluse, LIA-CERI, France","Chandramohan, S., Supelec - Metz Campus, IMS Research Group, France, Université D'Avignon et des Pays de Vaucluse, LIA-CERI, France; Geist, M., Supelec - Metz Campus, IMS Research Group, France; Lefèvre, F., Université D'Avignon et des Pays de Vaucluse, LIA-CERI, France; Pietquin, O., Supelec - Metz Campus, IMS Research Group, France, UMI 2958 (CNRS - GeorgiaTech), France","Inverse Reinforcement Learning; Spoken Dialogue Systems; User simulation","Dialogue systems; Human users; Imitation learning; Inverse reinforcement learning; Man-machine interface; Natural languages; Spoken dialogue system; Synthetic data; User simulation; Reinforcement learning; Speech processing",Conference Paper,"Final",,Scopus,2-s2.0-84865717612
"Daume III H.","8911764800;","Beyond structured prediction: Inverse reinforcement learning",2011,"ACL HLT 2011 - 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, Proceedings of Student Session",,,,"","",26,,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84859064813&partnerID=40&md5=5b1eaa1f013bdd655ab5cf054f1317ed","Computer Science, University of Maryland, United States","Daume III, H., Computer Science, University of Maryland, United States",,"Complete solutions; Inverse reinforcement learning; Reward function; Structured prediction; Computational linguistics; Reinforcement learning",Conference Paper,"Final",,Scopus,2-s2.0-84859064813
"Choi J., Kim K.-E.","36835669400;15053383400;","MAP inference for Bayesian inverse reinforcement learning",2011,"Advances in Neural Information Processing Systems 24: 25th Annual Conference on Neural Information Processing Systems 2011, NIPS 2011",,,,"","",9,20,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84860629866&partnerID=40&md5=28422a8d252d0b3010801ab470dc188a","Department of Computer Science, Korea Advanced Institute of Science and Technology, Daejeon 305-701, South Korea","Choi, J., Department of Computer Science, Korea Advanced Institute of Science and Technology, Daejeon 305-701, South Korea; Kim, K.-E., Department of Computer Science, Korea Advanced Institute of Science and Technology, Daejeon 305-701, South Korea",,"Bayesian frameworks; Differentiability; Infinite numbers; Inverse reinforcement learning; MAP estimation; Maximum a posteriori estimation; Posterior distributions; Reward function; Algorithms; Gradient methods; Reinforcement learning",Conference Paper,"Final",,Scopus,2-s2.0-84860629866
[No author name available],[No author id available],"Advances in Neural Information Processing Systems 24: 25th Annual Conference on Neural Information Processing Systems 2011, NIPS 2011",2011,"Advances in Neural Information Processing Systems 24: 25th Annual Conference on Neural Information Processing Systems 2011, NIPS 2011",,,,"","",2718,,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84860637780&partnerID=40&md5=f5dd06d12eb758b76907887b36017998",,"",,,Conference Review,"Final",,Scopus,2-s2.0-84860637780
"Michini B., How J.P.","36603096200;7006512768;","A human-interactive course of action planner for aircraft carrier deck operations",2011,"AIAA Infotech at Aerospace Conference and Exhibit 2011",,,,"","",,17,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880848742&partnerID=40&md5=efd23a574d3a6b1116c61b1b59bcde64","Aerospace Controls Laboratory, Massachusetts Institute of Technology, MIT, Cambridge, MA 02139, United States; Aerospace Controls Laboratory, Laboratory for Information and Decision Systems, Massachusetts Institute of Technology, MIT, Cambridge, MA 02139, United States","Michini, B., Aerospace Controls Laboratory, Massachusetts Institute of Technology, MIT, Cambridge, MA 02139, United States; How, J.P., Aerospace Controls Laboratory, Laboratory for Information and Decision Systems, Massachusetts Institute of Technology, MIT, Cambridge, MA 02139, United States",,"Course of action; Emergency scenario; Inverse reinforcement learning; Normal operations; Optimal policies; Planning problem; Time-critical scheduling; Uncertain environments; Aircraft; Aircraft carriers; Artificial intelligence; Decision support systems; Optimization; Personnel training; Scheduling; Taxicabs; Curricula",Conference Paper,"Final",,Scopus,2-s2.0-84880848742
"Riordan B., Brimi S., Schurr N., Freeman J., Ganberg G., Cooke N.J., Rima N.","36469192100;55343995500;8511831000;7403529943;36600400000;7102096954;36026010500;","Inferring user intent with Bayesian inverse planning: Making sense of multi-UAS mission management",2011,"20th Annual Conference on Behavior Representation in Modeling and Simulation 2011, BRiMS 2011",,,,"49","56",,6,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84865361469&partnerID=40&md5=bacac8df2ed1fac6e12e209d4eb8ca01","Aptima, Inc., 12 Gill Street, Woburn, MA 01801, United States; Cognitive Engineering Research Institute, 5810 South Sossaman Road, Mesa, AZ 85212, United States","Riordan, B., Aptima, Inc., 12 Gill Street, Woburn, MA 01801, United States; Brimi, S., Aptima, Inc., 12 Gill Street, Woburn, MA 01801, United States; Schurr, N., Aptima, Inc., 12 Gill Street, Woburn, MA 01801, United States; Freeman, J., Aptima, Inc., 12 Gill Street, Woburn, MA 01801, United States; Ganberg, G., Aptima, Inc., 12 Gill Street, Woburn, MA 01801, United States; Cooke, N.J., Cognitive Engineering Research Institute, 5810 South Sossaman Road, Mesa, AZ 85212, United States; Rima, N., Cognitive Engineering Research Institute, 5810 South Sossaman Road, Mesa, AZ 85212, United States","Bayesian inference; Intent; Inverse reinforcement learning; Markov decision processes; Unmanned aerial systems","Bayesian inference; Intent; Inverse reinforcement learning; Markov Decision Processes; Unmanned aerial systems; Bayesian networks; Computer simulation; Markov processes; Reinforcement learning; User interfaces; Inference engines",Conference Paper,"Final",,Scopus,2-s2.0-84865361469
"Qiao Q., Beling P.A.","35848809400;6603732790;","Inverse reinforcement learning with Gaussian process",2011,"Proceedings of the American Control Conference",,, 5990948,"113","118",,14,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-80053156567&partnerID=40&md5=e9585553ce328e45a88d480881cadfd8","Department of Systems and Information Engineering, University of Virginia, Charlottesville, VA 22904, United States","Qiao, Q., Department of Systems and Information Engineering, University of Virginia, Charlottesville, VA 22904, United States; Beling, P.A., Department of Systems and Information Engineering, University of Virginia, Charlottesville, VA 22904, United States",,"Apprenticeship learning; Bayesian inference; Gaussian process models; Gaussian Processes; Infinite state space; Inverse reinforcement learning; Inverse-optimal control; Maximum a posteriori estimation; Numerical problems; Preference graph; Quadratic programs; Real-world application; Reward function; Bayesian networks; Convex optimization; Gaussian distribution; Gaussian noise (electronic); Inference engines; Numerical methods; Optimization; Reinforcement; Reinforcement learning; Learning algorithms",Conference Paper,"Final",,Scopus,2-s2.0-80053156567
"Choi J., Kim K.-E.","36835669400;15053383400;","Inverse reinforcement learning in partially observable environments",2011,"Journal of Machine Learning Research","12",,,"691","730",,31,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-79955875655&partnerID=40&md5=02bd7a0269869bebe7c1cff2c4b935a5","Department of Computer Science, Korea Advanced Institute of Science and Technology, Daejeon 305-701, South Korea","Choi, J., Department of Computer Science, Korea Advanced Institute of Science and Technology, Daejeon 305-701, South Korea; Kim, K.-E., Department of Computer Science, Korea Advanced Institute of Science and Technology, Daejeon 305-701, South Korea","Inverse optimization; Inverse reinforcement learning; Linear programming; Partially observable Markov decision process; Quadratically constrained programming","Constrained programming; Ill posed; Inverse optimization; Inverse reinforcement learning; Markov Decision Processes; Partially observable environments; Partially observable Markov decision process; Realistic scenario; Reward function; Algorithms; Computer programming; Markov processes; Reinforcement learning; Inverse problems",Article,"Final",,Scopus,2-s2.0-79955875655
"Jin Z.-J., Qian H., Chen S.-Y., Zhu M.-L.","25652807500;57203653311;14035228600;7402909050;","Convergence analysis of an incremental approach to online inverse reinforcement learning",2011,"Journal of Zhejiang University: Science C","12","1",,"17","24",,1,"10.1631/jzus.C1010010","https://www.scopus.com/inward/record.uri?eid=2-s2.0-79951719816&doi=10.1631%2fjzus.C1010010&partnerID=40&md5=cc39d3500916ff197567172b04581342","School of Computer Science and Technology, Zhejiang University, Hangzhou 310027, China","Jin, Z.-J., School of Computer Science and Technology, Zhejiang University, Hangzhou 310027, China; Qian, H., School of Computer Science and Technology, Zhejiang University, Hangzhou 310027, China; Chen, S.-Y., School of Computer Science and Technology, Zhejiang University, Hangzhou 310027, China; Zhu, M.-L., School of Computer Science and Technology, Zhejiang University, Hangzhou 310027, China","Incremental approach; Inverse reinforcement learning; Markov decision process; Online learning; Reward recovering","Incremental approach; Inverse reinforcement learning; Markov Decision Processes; Online learning; Reward recovering; Markov processes; Recovery; Reinforcement learning; E-learning",Article,"Final",,Scopus,2-s2.0-79951719816
"Levine S., Popović Z., Koltun V.","35731728100;35176314800;7004337914;","Feature construction for inverse reinforcement learning",2010,"Advances in Neural Information Processing Systems 23: 24th Annual Conference on Neural Information Processing Systems 2010, NIPS 2010",,,,"","",9,30,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84860607709&partnerID=40&md5=c8e6d46a83e28f2ef513f62974a9691d","Stanford University, United States; University of Washington, United States","Levine, S., Stanford University, United States; Popović, Z., University of Washington, United States; Koltun, V., Stanford University, United States",,"Feature construction; Inverse reinforcement learning; Markov Decision Processes; Optimal policies; Reward function; Stationary policy; Algorithms; Markov processes; Reinforcement learning",Conference Paper,"Final",,Scopus,2-s2.0-84860607709
"Boularias A., Chaib-Draa B.","23090275300;7004238886;","Bootstrapping apprenticeship learning",2010,"Advances in Neural Information Processing Systems 23: 24th Annual Conference on Neural Information Processing Systems 2010, NIPS 2010",,,,"","",9,4,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84860621667&partnerID=40&md5=741e6f4a48735d37a0683dbf1d919fd6","Department of Empirical Inference, Max-Planck Institute for Biological Cybernetics, 72076 Tübingen, Germany; Department of Computer Science, Laval University, QC G1V 0A6, Canada","Boularias, A., Department of Empirical Inference, Max-Planck Institute for Biological Cybernetics, 72076 Tübingen, Germany; Chaib-Draa, B., Department of Computer Science, Laval University, QC G1V 0A6, Canada",,"Apprenticeship learning; Highly sensitive; Inverse reinforcement learning; Linear combinations; Monte-Carlo estimation; State space; Utility functions; Apprentices; Reinforcement learning; Demonstrations",Conference Paper,"Final",,Scopus,2-s2.0-84860621667
"Jin Z.-J., Qian H., Zhu M.-L.","25652807500;57203653311;7402909050;","Gaussian processes in inverse reinforcement learning",2010,"2010 International Conference on Machine Learning and Cybernetics, ICMLC 2010","1",, 5581063,"225","230",,,"10.1109/ICMLC.2010.5581063","https://www.scopus.com/inward/record.uri?eid=2-s2.0-78149352202&doi=10.1109%2fICMLC.2010.5581063&partnerID=40&md5=3dc9f7b3ddc5177a48c39e2c2e9b1a9e","College of Computer Science, Zhejiang University, Hangzhou 310027, China","Jin, Z.-J., College of Computer Science, Zhejiang University, Hangzhou 310027, China; Qian, H., College of Computer Science, Zhejiang University, Hangzhou 310027, China; Zhu, M.-L., College of Computer Science, Zhejiang University, Hangzhou 310027, China","Gaussian process; Inverse reinforcement learning; Markov decision process; Reinforcement learning; Reward learning",,Conference Paper,"Final",,Scopus,2-s2.0-78149352202
"Melo F.S., Lopes M.","35569256100;8607501600;","Learning from demonstration using MDP induced metrics",2010,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","6322 LNAI","PART 2",,"385","401",,7,"10.1007/978-3-642-15883-4_25","https://www.scopus.com/inward/record.uri?eid=2-s2.0-78049399307&doi=10.1007%2f978-3-642-15883-4_25&partnerID=40&md5=a8bffc83a889f0783baee7a7bafb6ade","INESC-ID/Instituto Superior Técnico, TagusPark - Edifício IST, 2780-990 Porto Salvo, Portugal; University of Plymouth PL4 8AA, Plymouth, Devon, United Kingdom","Melo, F.S., INESC-ID/Instituto Superior Técnico, TagusPark - Edifício IST, 2780-990 Porto Salvo, Portugal; Lopes, M., University of Plymouth PL4 8AA, Plymouth, Devon, United Kingdom",,"Computational costs; Generalization performance; Inverse reinforcement learning; Kernel based approach; Learning from demonstration; Optimal policies; State-space; Supervised learning methods; Learning systems; Demonstrations",Conference Paper,"Final",,Scopus,2-s2.0-78049399307
"Silver D., Bagnell J.A., Stentz A.","7202151417;6506127486;7006771602;","Learning from demonstration for autonomous navigation in complex unstructured terrain",2010,"International Journal of Robotics Research","29","12",,"1565","1592",,50,"10.1177/0278364910369715","https://www.scopus.com/inward/record.uri?eid=2-s2.0-77957947591&doi=10.1177%2f0278364910369715&partnerID=40&md5=1731feb8d6fe182bda5d9a780b829754","Carnegie Mellon University, 10 Fourtieth Street, Pittsburgh, PA 15201, United States","Silver, D., Carnegie Mellon University, 10 Fourtieth Street, Pittsburgh, PA 15201, United States; Bagnell, J.A., Carnegie Mellon University, 10 Fourtieth Street, Pittsburgh, PA 15201, United States; Stentz, A., Carnegie Mellon University, 10 Fourtieth Street, Pittsburgh, PA 15201, United States","autonomous navigation; Field robotics; imitation learning; inverse reinforcement learning; learning from demonstration; mobile robotics","Autonomous navigation; Field robotics; Imitation learning; Inverse reinforcement learning; Learning from demonstration; Mobile robotic; Demonstrations; Landforms; Machine design; Navigation systems; Online systems; Reinforcement learning; Robot programming; Robotics; Robots; Navigation",Conference Paper,"Final",,Scopus,2-s2.0-77957947591
"Dvijotham K., Todorov E.","36470045000;6701358567;","Inverse optimal control with linearly-solvable MDPs",2010,"ICML 2010 - Proceedings, 27th International Conference on Machine Learning",,,,"335","342",,60,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-77956518472&partnerID=40&md5=1c32392f7bcdb30cb34020ef9ba57540","Computer Science and Engineering and Applied Mathematics, University of Washington, Seattle - 98105, United States","Dvijotham, K., Computer Science and Engineering and Applied Mathematics, University of Washington, Seattle - 98105, United States; Todorov, E., Computer Science and Engineering and Applied Mathematics, University of Washington, Seattle - 98105, United States",,"Continuous State Space; Control engineering; Control policy; Density estimation; Forward problem; Inverse algorithm; Inverse reinforcement learning; Inverse-optimal control; Log likelihood; Maximum entropy; Nonlinear problems; Orders of magnitude; Special properties; Unconstrained optimization; Value functions; Control; Learning algorithms; Learning systems; Linearization; Maximum likelihood estimation; Optimization; Data reduction",Conference Paper,"Final",,Scopus,2-s2.0-77956518472
"Lee S.J., Popović Z.","57192515790;35176314800;","Learning behavior styles with inverse reinforcement learning",2010,"ACM Transactions on Graphics","29","4", 122,"","",,22,"10.1145/1778765.1778859","https://www.scopus.com/inward/record.uri?eid=2-s2.0-77956417638&doi=10.1145%2f1778765.1778859&partnerID=40&md5=e0e912385ee21466ac8e162af9717c79","University of Washington, United States","Lee, S.J., University of Washington, United States; Popović, Z., University of Washington, United States","Apprenticeship learning; Data driven animation; Human animation; Inverse reinforcement learning; Optimal control","Apprenticeship learning; Data-driven animation; Human animation; Inverse reinforcement learning; Optimal controls; Apprentices; Control; Learning algorithms; Optimization; Reinforcement learning; Animation",Article,"Final",,Scopus,2-s2.0-77956417638
"Boularias A., Chaib-draa B.","23090275300;7004238886;","Apprenticeship learning via soft local homomorphisms",2010,"Proceedings - IEEE International Conference on Robotics and Automation",,, 5509717,"2971","2976",,2,"10.1109/ROBOT.2010.5509717","https://www.scopus.com/inward/record.uri?eid=2-s2.0-77955793370&doi=10.1109%2fROBOT.2010.5509717&partnerID=40&md5=cfb8a63da7fe72c20077dd34778cffd2","Computer Science and Software Engineering Department, Laval University, QC, G1V 0A6, Canada","Boularias, A., Computer Science and Software Engineering Department, Laval University, QC, G1V 0A6, Canada; Chaib-draa, B., Computer Science and Software Engineering Department, Laval University, QC, G1V 0A6, Canada",,"Apprenticeship learning; Empirical results; Inverse reinforcement learning; Markov Decision Processes; Problem-based; State space; Transfer method; Apprentices; Markov processes; Robots; Robotics",Conference Paper,"Final",,Scopus,2-s2.0-77955793370
"Lee S.J., Popović Z.","57192515790;35176314800;","Learning behavior styles with inverse reinforcement learning",2010,"ACM SIGGRAPH 2010 Papers, SIGGRAPH 2010",,, 122,"","",,2,"10.1145/1778765.1778859","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029591362&doi=10.1145%2f1778765.1778859&partnerID=40&md5=633985d4e53cd0131f25e3d2a752ab74","University of Washington, United States","Lee, S.J., University of Washington, United States; Popović, Z., University of Washington, United States","Apprenticeship learning; Data driven animation; Human animation; Inverse reinforcement learning; Optimal control","Animation; Apprentices; Computer graphics; Interactive computer graphics; Inverse problems; Learning algorithms; Apprenticeship learning; Data-driven animation; Human animation; Inverse reinforcement learning; Optimal controls; Reinforcement learning",Conference Paper,"Final",,Scopus,2-s2.0-85029591362
